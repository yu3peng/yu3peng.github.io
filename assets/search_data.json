

[
  
  
    {
      "title"    : "页面没有找到",
      "url"      : "https://y2p.cc/404.html",
      "keywords" : "404"
    } ,
  
  
  
    {
      "title"    : "About",
      "url"      : "https://y2p.cc/about/",
      "keywords" : ""
    } ,
  
  
  
    {
      "title"    : "归档",
      "url"      : "https://y2p.cc/archives/",
      "keywords" : "归档"
    } ,
  
  
  
    {
      "title"    : "Categories",
      "url"      : "https://y2p.cc/categories/",
      "keywords" : "分类"
    } ,
  
  
  
    {
      "title"    : "捐助 / Donate",
      "url"      : "https://y2p.cc/donate/",
      "keywords" : "Donate"
    } ,
  
  
  
  
  
    {
      "title"    : "Links",
      "url"      : "https://y2p.cc/links/",
      "keywords" : "友情链接"
    } ,
  
  
  
    {
      "title"    : "mindmap",
      "url"      : "https://y2p.cc/mindmap-viewer/",
      "keywords" : "mindmap"
    } ,
  
  
  
    {
      "title"    : "Notes",
      "url"      : "https://y2p.cc/notes/",
      "keywords" : ""
    } ,
  
  
  
    {
      "title"    : "Open Source Projects",
      "url"      : "https://y2p.cc/open-source/",
      "keywords" : "开源,open-source,GitHub,开源项目"
    } ,
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  

  
    {
      "title"    : "VISUAL STUDIO CODE &amp; GOLANG 开发环境搭建",
      "category" : "Tool",
      "content": "集成开发工具，墙裂推荐 环境要求 win7或以上 64位 Go安装  下载地址：https://golang.org/dl/ (墙内下载地址http://www.golangtc.com/download)   下载后直接双击msi文件安装，默认安装在c:  go   安装完成后默认会在环境变量 Path 后添加 Go 安装目录下的 bin 目录 C:  Go  bin  ，并添加环境变量 GOROOT，值为 Go 安装根目录 C:  Go     验证是否安装成功，在运行中输入 cmd 打开命令行工具，在提示符下输入 go   设置工作空间gopath目录(Go语言开发的项目路径) Windows 设置如下，新建一个环境变量名称叫做GOPATH，值为你的工作目录，例如笔者的设置GOPATH=e:  mygo  以上 %GOPATH% 目录约定有三个子目录： src 存放源代码（比如：.go .c .h .s等） pkg 编译后生成的文件（比如：.a） bin 编译后生成的可执行文件（为了方便，可以把此目录加入到 windows的PATH 变量中，在环境变量path后追加%GOPATH%  bin）   用go env命令查看环境变量设置 安装git版本控制  下载git 访问https://git-for-windows.github.io/下载安装   配置账号   访问githubhttps://github.com/注册账号 github主要作为我们的远程仓库，将本地代码提交到上面，注意注册时填写的email和用户名要和我们本地git配置中的email和用户名相同   从开始菜单找打git程序打开git bash  然后输入 $ git config --global user.name Your Name$ git config --global user.email email@example.com   将上面的your name 和 email@example.com换成你在github上注册的用户名和邮箱 安装vs code及golang插件  下载地址 https://code.visualstudio.com/   安装golang插件 vs code为开发者提供了很多插件，要想让vs code可以更好的编写go代码，需要安装golang插件    安装go插件所需的第三方包 要想让golang插件实现语法智能提示，debug等，必须安装一些第三方包，安装第三包有两种方式，一种是让软件自动安装，一种是手动安装    第一种方式  在E:  mygo  src目录下新建hello.go文件，代码如下  package main import fmt func main() { fmt.Println(Hello world!) }    然后用vs code打开此文件，然后我们会发现在vs code的右下角会出现Analysis Tools Missing，单击即可帮我们安装此插件所需的所有第三方包  在安装过程中需要下载gorename包(golang.org/x/tools/cmd/gorename)和guru包(golang.org/x/tools/cmd/guru), 需要翻墙，否则无法下载安装，不会翻墙的童鞋请前往它的github地址 https://godoc.org/golang.org/x/tools，将其git clone下来 将git clone的文件拷贝到E:  mygo  src  golang.org  x  tools目录     第二种方式  打开命令行提示符，然后粘贴输入如下命令  go get -u -v github.com/nsf/gocode go get -u -v github.com/rogpeppe/godef go get -u -v github.com/golang/lint/golint go get -u -v github.com/lukehoban/go-outline go get -u -v sourcegraph.com/sqs/goreturns go get -u -v golang.org/x/tools/cmd/gorename go get -u -v github.com/tpng/gopkgs go get -u -v github.com/newhook/go-symbols go get -u -v golang.org/x/tools/cmd/guru    同样在获取gorename和guru包的时候需要翻墙，解决方法同上     vs code配置 启动vs code后选择文件菜单-》首选项-》工作区设置 在打开的settings.json文件里粘贴如下代码 { go.buildOnSave: true, go.lintOnSave: true, go.vetOnSave: true, go.buildTags: , go.buildFlags: [], go.lintFlags: [], go.vetFlags: [], go.coverOnSave: false, go.useCodeSnippetsOnFunctionSuggest: false, go.formatOnSave: true, go.formatTool: goreturns, go.goroot: C:    Go,  go.gopath: e:    mygo, go.gocodeAutoBuild: false }   重启即可启用vs code飞一般的编写go代码了 安装icons插件 vscode-icons插件，可以为vscode里的不同文件类型提供相应的图标，如下图 安装方法与安装golang插件相同 安装debug插件delve 执行 go get -u -v github.com/derekparker/delve/cmd/dlv命令，安装调试插件，安装好之后，就可以调试go语言了 delve目前还不支持32位机器 ",
      "url"      : "https://y2p.cc/2017/01/10/vs-code-golang/",
      "keywords" : ""
    } ,
  
    {
      "title"    : "NFV 缩略语",
      "category" : "NFV",
      "content": "NFV必读 缩略语 0~9 （1）3GPP，3rd Generation Partnership Project，第三代合作伙伴计划 A~B （1）API，Application Program Interface，应用编程接口 （2）ATCA，Advanced Telecom Computing Architecture，高级电信计算架构 （3）BESS，BGP Enabled ServiceS，BGP使能业务 （4）BRAS，Broadband Remote Access Serve，宽带接入服务器 （5）BSS，Business Support Systems，业务支撑系统 （6）BYOD，Bring Your Own Device，自带设备 C （1）CCSA，China Communications Standards Association，中国通信标准化协会 （2）COTS，Commercial Off-The-Shelf，商用成品 （3）CPE，Customer Premise Equipment，用户端设备 （4）CPU，Central Processing Unit，中央处理器 （5）CT，Communication Technology，通信技术 （6）CP，Connection Point，连接点 （7）CPD，Connection Point Descriptor，连接点描述符 D~M （1）DC，Data Center，数据中心 （2）DF，Deployment Flavour，部署风格 （3）DoS，Denial of Service，拒绝服务 （4）DPDK，Data Plane Development Kit，数据平面开发包 （5）ECC，Error Checking and Correcting，错误检查和纠正 （6）EMS，Element Management System，网元管理系统 （7）EPC，Evolved Packet Core，移动演进分组核心网 （8）ETSI，European Telecommunications Standards Institute，欧洲电信标准协会 （9）FCAPS，Fault, Configuration, Account, Performance, Security，管理5大功能 （10）HSS，Home Subscriber Server，属地用户服务器 （11）I2NSF，Interface to Network Security Functions，网络安全功能接口 （12）I2RS，Interface to the Routing System，路由系统接口 （13）IETF，Internet Engineering Task Force，国际互联网工程任务 （14）IMS，IP Multimedia Subsystem，IP多媒体子系统 （15）IPPM，IP Performance Metrics，互联网性能指标 （16）IT，Information Technology，信息技术 （17）IWF，InterWorking Function，互通功能 （18）MANO，NFV Management and Orchestration，NFV管理和编排 （19）MME，Mobility Management Entity，移动管理实体 （20）MPLS，Multi-Protocol Label Switching，多协议标签交换 （21）MVNO，Mobile Virtual Network Operator，移动虚拟化网络运营商。 N （1）NAT，Network Address Translation，网络地址转换 （2）NB-IoT，Narrow Band Internet of Things，窄带物联网 （3）NCT，Network Connection Topology，网络连接拓扑 （4）N-PoP，Network Point of Presence，网络接入点/服务提供点 （5）NF，Network Function，网络功能（模块） （6）NFP，Network Forwarding Path，网络转发路径 （6）NFV，Network Functions Virtualisation，网络功能虚拟化 （7）NFV-Res，NFV Resource，网络功能虚拟化资源 （8）NFVI，NFV Infrastructure，网络功能虚拟化基础设施 （9）NFV-MANO，Network Functions Virtualisation Management and Orchestration，网络功能虚拟化管理及（业务）编排 （10）NFVI-Node，Network Functions Virtualisation Infrastructure Node，网络功能虚拟化基础设施节点 （11）NFVI-PoP，Network Function Virtualisation Infrastructure Point of Presence，网络功能虚拟化基础设施接入点/服务提供点 （12）NFVO，Network Functions Virtualisation Orchestrator，网络功能虚拟化编排器 （13）NIC，Network Interface Controller，网络接口控制器 （14）NS，Network Service，网络服务 （15）NSD，Network Service Descriptor，网络服务描述符 （16）NSR，Network Service Record，网络服务记录 （17）NUMA，Non Uniform Memory Access，非一致性内存访问 （18）NVO3，Network Virtualisation Overlays，网络虚拟化叠加 O （1）OSS，Operations support systems，运营支撑系统 （2）OPNFV，Open Platform for NFV，NFV开放平台 P （1）PCI，Peripheral Component Interconnect，外围组件互连 （2）PNF，Physical Network Function，物理/实体网络功能（模块） （3）PCRF，Policy Control and Charging Rules Function，策略控制与计费规则功能 （4）PGW，Packet data network Gateway，包数据网网关 （5）PoP，Point of Presence，接入点/服务提供点 Q~R （1）QoS，Quality of Service，服务质量 （2）RCS，Rich Comunication Services，富媒体通信业务 S （1）SAN，Storage Area Network，儲存区域网络 （2）SFC，Service Function Chaining，业务功能链 （3）SLA，Service Level Agreement，服务水平协议 （4）SR-IOV，Single root I/O virtualization，单根I/O虚拟化 T~U （1）TEAS，Traffic Engineering Architecture and Signaling，流量工程架构与信令 （2）TLB，translation lookaside buffer，翻译后备缓冲器 （3）TOSCA，Topology and Orchestration Specification for Cloud Applications，拓扑和编排规范云应用 V （1）VA，Virtual Application，虚拟应用 （2）vCPU，Virtualised CPU，虚拟化的中央处理器 （3）VDU，Virtualization Deployment Unit，虚拟化部署单元 （4）VIM，Virtualised Infrastructure Manager，虚拟化的基础设施管理器 （5）VM，Virtual Machine，虚拟机 （6）VNF FG，VNF Forwarding Graph，虚拟化的网络功能模块转发表 （7）VNF，Virtualised Network Function，虚拟化的网络功能模块 （8）VNFC，Virtualised Network Function Component，虚拟化的网络功能模块组件 （9）VNFD，Virtualised Network Function Descriptor，虚拟化的网络功能模块描述符 （10）VNFM，Virtualised Network Function Manager，虚拟化的网络功能模块管理器 （11）vNIC，Virtualised NIC，虚拟化的网络接口控制器 （12）vStorage，Virtualised Storage，虚拟化的存储 （13）vSwitch，Virtualised Switch，虚拟交换机 W到Z 无 ",
      "url"      : "https://y2p.cc/2017/01/11/nfv-abbreviation/",
      "keywords" : ""
    } ,
  
    {
      "title"    : "NFV 术语及定义",
      "category" : "NFV",
      "content": "NFV必读 NFV的术语及定义 0~9 无。 A~B 无。 C （1）CPU（Central Processing Unit，中央处理器）：一种NFV计算域设备，用以提供主要的容器接口。 （2）compute domain（计算域）：一种NFVI（笔者注：NFVI即NFV Infrastructure——NFV基础设施）内的域，其中包括各种服务器与存储（设备）。 （3）compute node（计算节点）：对于服务器的抽象定义。 （4）Connection Point（CP，链接点）：虚拟链接的相应连接点。 D~H 无。 I （1）infrastructure network domain（（NFV）基础设施（的）网络域）：一种NFVI内的域，其中包括计算基础设施与存储基础设施（笔者注：即NFVI的计算域）的各种内部数据互联网络。 注：（NFV）基础设施（的）网络域（的部署）先于VNFs（笔者注：VNFs即Virtualised Network Function——网络功能虚拟化）的实现。 J~K 无。 L （1）lifecycle management（生命周期管理）：对VNF（网络功能虚拟化）或NS（Network Service，网络服务）的实例化、维护以及终结进行管理所需的一组功能。 M （1）measurement（测量）：为得到测量数值或测量结果而执行的一组操作。对相关操作的实际执行可以得到测量数值（基于ISO/IEC 15939:2007: “Systems and software engineering – Measurement process”《ISO/IEC 15939:2007：系统与软件工程——测量过程》中对于“测量”的定义，NIST Special Publication 500-307: “Cloud Computing Service Metrics Description”《云计算服务度量描述》中有相关的引用）。 （2）metric（度量）：（对于）在性能评估以及/或者网络可靠性评估中所产生的数量的标准定义。其具有一种预期效用，并专门用于传递测量数值的准确含义。 注：此处的这一定义符合IETF RFC 2330: “Framework for IP Performance Metrics”《IP性能度量框架》以及IETF RFC 6390: “Guidelines for Considering New Performance Metric Development”《（关于）发展新的性能度量（方式）的指导意见》中对于“性能度量”的定义。例如：分组传输性能或者网络的可靠性。 N （1）network controller（网络控制器）：将一个（NFV）网络域中的一些或者全部控制与管理功能集中在一起的功能模块。其可通过已经明确定义好的接口为其他功能模块提供所在（NFV）网络域的（全局）视图。 （2）network forwarding path（网络转发路径）：由一系列NF（Network Function，网络功能（模块））所组成的连接点序列表，其中也包含相关的转发策略以及虚拟链路。 （3）Network Function（NF，网络功能（模块））：（NFV）网络基础设施中的功能模块。其具有明确定义的各个外部接口以及明确定义的功能性行为。 注：实际上，网络功能（模块）与目前的网络节点或者实体设备等同。 （4）Network Functions Virtualisation（NFV，网络功能虚拟化）：通过虚拟化的硬件（设备）抽象，将网络功能从其所运行的硬件（设备）中分离出来。 （5）Network Functions Virtualisation Infrastructure（NFVI，网络功能虚拟化基础设施）：具有部署网络功能虚拟化的能力的环境中，所有硬件设备与软件组件的统称。 注：网络功能虚拟化基础设施NFVI可以跨越若干个物理位置（例如：数据中心运营场所）进行扩展部署，此时，为这些物理站点提供数据连接的网络也被考虑成网络功能虚拟化基础设施NFVI的一部分。在网络功能虚拟化NFV的范畴内，NFVI与VNF均是顶级的概念型实体，而其他的NFV组成部分则是这两大主要实体的子实体。 （6）Network Functions Virtualisation Infrastructure（NFVI）components（NFVI组件）：不可以在（网络）现场更换的，但是在制造时可作为COTS（commercial off-the-shelf，商务现货供应）组件以示区分的NFVI硬件资源。 （7）Network Functions Virtualisation Infrastructure Node（NFVI-Node，NFVI节点）：以独立实体进行部署与管理的物理设备。NFVI节点为支持虚拟化网络功能VNFs执行环境提供所需的各项NFVI功能。 （8）Network Function Virtualisation Infrastructure Point of Presence（NFVI-PoP，NFVI接入点/服务提供点）：以虚拟网络功能模块VNF形式所部属的网络功能模块。 （9）Network Functions Virtualisation Management and Orchestration（NFV-MANO，NFV的管理与（业务）编排）：由NFVO（Network Functions Virtualisation Orchestrator，网络功能虚拟化编排器）、VNFM（Network Functions Virtualisation Manager，网络功能虚拟化管理器）以及VIM（Virtualised Infrastructure Manager，虚拟化基础设施管理器）共同完成/提供的功能。 （10）Network Functions Virtualisation Management and Orchestration Architectural Framework（NFV-MANO Architectural Framework，NFV-MANO体系结构框架）：所有功能模块（包括NFV-MANO中的功能模块以及与NFV-MANO互通的功能模块）、这些功能模块所使用的数据库、这些功能模块之间用以交换信息（以执行NFV管理与业务编排）的参考点以及接口的集合。 （11）Network Functions Virtualisation Orchestrator（NFVO，网络功能虚拟化编排器）：可用以管理NS（Network Service，网络服务）生命周期，并协调NS生命周期的管理、协调VNF（Virtualised Network Function，虚拟化的网络功能）生命周期的管理（需要得到VNF管理器VNFM的支持）、协调NFVI（NFV Infrastructure，网络功能虚拟化基础设施）各类资源的管理（需要得到虚拟化基础设施管理器VIM的支持），以此确保所需各类资源与连接的优化配置。 （12）Network Interface Controller（NIC，网络接口控制器）：在NFV计算节点内，提供与（外部）基础设施网络（连接）的物理接口的设备。 （13）network operator（网络运营商）：电子通信网络整体或其中一部分的运营商。由此类网络运营商所组成的协会或者组织也可被称为“网络运营商”（参见ETSI Directives: Annex 1: “Definitions in relation to the member categories of ETSI”《ETSI（欧洲电信标准协会）指令：附录1：关于ETSI成员类别的定义》）。 （14）Network Point of Presence（N-PoP，网络接入点）：NF（网络功能（模块））作为一个PNF（Physical Network Function，物理网络功能（模块））或者一个VNF（Virtual Network Function，虚拟网络功能（模块））进行部署的（网络）位置。 （15）Network Service（NS，网络服务）：一组VNF，VNF转发图，虚拟链接（VL）和连接点（CP）一起形成“网络服务”。 注：网络服务有助于高层服务的性能、可靠性以及安全性的提高。端到端网络服务行为的产生需要单个网络功能（模块）行为以及网络基础设施组合机制行为相互结合。 （16）network service descriptor（网络服务描述符）：描述网络服务的部署的模板。包括：业务拓扑（虚拟化网络功能的各个组成部分及其相互间的关系、虚拟链路Virtual Links、虚拟化网络功能的转发表）以及网络服务的各项特征（比如服务等级协议SLAs、网络服务实例的运行及生命周期管理所需具备的相关特征）。 （17）network service orchestration（网络服务编排）：NFV业务编排器功能的子集，用以进行网络服务的生命周期管理。 （18）network service provider（网络服务提供商）：部署网络服务的服务提供商类型。 （19）network stability（网络稳定性）：提供相关功能服务，并在非正常情况下（网络过载或者其他未超过设计极限的网络异常）恢复其指定的行为，与此同时，保持NFV（网络功能虚拟化）结构稳定性的能力。 （20）NF forwarding graph（网络功能（模块）转发表）：连接网络功能（模块）节点的各条逻辑链路的汇总，用以描述这些网络功能（模块）节点之间的数据流向。 （21）NF set（连接网络功能（模块）集）：具有未指定连接的所有网络功能（模块）的集合。 （22）NFVI component（网络功能虚拟化基础设施组件）：不可以在（网络）现场更换的，但是在制造时可作为COTS（commercial off-the-shelf，商务现货供应）组件以示区分的NFVI硬件资源。 （23）NFV framework（NFV架构/框架）：由ETSI ISG NFV发布的相关规范中所定义的所有实体、参考点、信息模型以及其他组成部分。 （24）NFV Infrastructure（NFVI，网络功能虚拟化基础设施）：具有部署网络功能虚拟化的能力的环境中，所有硬件设备与软件组件的统称。 注：网络功能虚拟化基础设施NFVI可以跨越若干个物理位置（例如：数据中心运营场所）进行扩展部署，此时，为这些物理站点提供数据连接的网络也被考虑成网络功能虚拟化基础设施NFVI的一部分。在网络功能虚拟化NFV的范畴内，NFVI与VNF均是顶级的概念型实体，而其他的NFV组成部分则是这两大主要实体的子实体。 （25）NFV-Resource（NFV-Res，网络功能虚拟化资源）：网络功能虚拟化资源存在于网络功能虚拟化基础设施之中，由VNF/VNSF正确地执行相关功能而使用。 （26）NS Catalog（网络服务目录）：所有可用网络服务描述符（NSD）的存储库。 O 无。 P （1）Physical Network Function（PNF，物理网络功能（模块））：专用硬件，提供特定的网络功能，例如防火墙。 Q 无。 R （1）resiliency（弹性）：规避网络服务中断，使其回归正常状态的网络功能虚拟化框架/结构的相关能力。或者在出现网络故障、宕机或甚至出现正常操作中断事件时，以最低的、可接受的质量进行服务传送的网络功能虚拟化框架/结构的相关能力。 S （1）scaling（伸缩）：根据需要，为虚拟化网络功能模块动态地分配或回收相关资源的能力。 注：“伸缩”包括横向扩展/纵向扩展以及向外扩展/向内扩展。 （2）scaling out/in（向外扩展/向内扩展）：通过增加/删除资源实例（例如：VM（虚拟机））来进行伸缩的能力。 （3）scaling up/down（横向扩展/纵向扩展）：通过改变所分配的资源（例如：增加/减少内存、CPU能力或存储规模）来进行伸缩的能力。 （4）service（业务）：由服务提供商面向用户提供的一系列服务选择组合中的组件，是提供给用户的一种功能。ETSI TR 121 905: “Digital cellular telecommunications system (Phase 2+); Universal Mobile Telecommunications System (UMTS); LTE; Vocabulary for 3GPP Specifications(3GPP TR 21.905)”《数字蜂窝移动通信系统（第二阶段）；通用移动通信系统（UMTS）；LTE；3GPP（移动通信国际标准组织）规范的词汇（3GPP TR 21.905）》中有详细定义。 注：“用户”有可能是终端客户，也可能是网络实体或者一些中间实体。 （5）service continuity（业务连续性）：按照业务的功能规范及行为规范，以及业务的服务水平协议SLA，进行业务的连续传送。业务连续性既涉及到数据平面，也涉及到控制平面。能保证在出现干扰或异常事件（不论是定期的或不定期的、恶意的、有意的还是无意的）时，能够全部完成所发起的事务或者会话。 注1：从终端用户的视角来看，“业务连续性”意味着以多种媒介跨越不同网络域（接入网络、汇聚网络以及核心网）或者不同类型用户终端设备时，正在进行的通信的持续性。 注2：端到端的业务持续性则需要按照相关业务的服务水平协议SLA所定义的服务质量对其进行传送。无论是通过非虚拟化的网络来传送，还是通过虚拟化的网络来传送，还是通过这两类网络的融合网络来传送，都必须要遵从这个原则。 （6）Service Level Agreement（SLA，服务水平协议）：双方或多方之间的谈判所达成的协议，代表着各方对于服务以及/或者服务行为（比如：可用性、性能、业务连续性、对于网络异常的响应、安全性、可维护性、操作性）的共同理解，并以可测量的目标值表征服务水平。 注：上述定义的范畴并不包含服务水平协议SLA的业务环节/方面。 （7）service provider（服务提供商）：通过电子通信网络整体或其中一部分向用户提供服务，或者基于某种商业原则向第三方提供服务的公司或者组织。由此类网络运营商所组成的协会或者组织也可被称为“网络运营商”（参见ETSI Directives: Annex 1: “Definitions in relation to the member categories of ETSI”《ETSI（欧洲电信标准协会）指令：附录1：关于ETSI成员类别的定义》）。 T （1）tenant domain（出租域）：提供虚拟化的网络功能模块VNFs，或者将虚拟化的网络功能融入到Network Services（网络服务）之中的域。出租域负责上述两项功能的管理以及业务编排——包括对它们进行功能配置，以及在应用层对它们进行维护等。 U （1）user service（用户业务）：由服务提供商提供给终端用户/客户/订户（订阅用户）的一系列服务选择组合中的组件。 V （1）Virtual Application（VA，虚拟应用）：对可以被装载到虚拟机VM的一种软件的更为普遍（适用）的术语。 注：一个VNF（虚拟化的网络功能模块）就是一种类型的虚拟应用VA。 （2）virtual link（虚拟链路）：一组连接点，其中包括这些连接点之间的连通性关系以及相关的目标性能指标（比如：带宽、延迟、QoS（服务质量））。 注：虚拟链路可以互联两个以上（含）的实体（虚拟化的网络功能模块VNF组件、虚拟化的网络功能模块VNFs或者物理网络功能模块PNF），而且，虚拟链路需要得到网络功能虚拟化基础设施NFVI的VN（Virtual Network，虚拟网络）的支持。 （3）Virtual Machine（VM，虚拟机）：一种虚拟化的计算环境，其与实体电脑/服务器的功能行为非常相似。 注：一个虚拟机所使用的是其所在的实体电脑/服务器的资源组件（处理器、内存、硬盘存储器、接口、端口等），是通过虚拟机管理程序Hypervisor所产生的——Hypervisor对底层的物理实体资源进行分区，然后分配给各个虚拟机。虚拟机具有承载/代理VNFC（VNF Component，虚拟化的网络功能模块组件）的能力。 （4）virtual network（虚拟网络）：虚拟网络在虚拟机VM实例的网络接口以及物理实体网络接口之间对相关的信息进行路由，提供必要的连接。 注：虚拟网络是由它的一组许可的网络接口所界定的。 （5）virtualisation container（虚拟化容器）：计算节点的分区。虚拟化容器可以被系统分配给单个的、虚拟化的计算环境。 注：虚拟化容器的例子包括虚拟机以及OS（操作系统）容器。 （6）Virtualisation Deployment Unit（VDU，虚拟化部署单元）：承载虚拟功能的虚拟机。 注：在虚拟机管理程序Hypervisor里面，虚拟化部署单元VDU的主要特征是基于能够被映射至单个虚拟机VM的构建体的单个虚拟化网络功能模块VNF实例或者虚拟化网络功能模块VNF子集实例。 （7）Virtualised CPU（vCPU，虚拟化的CPU）：虚拟机管理程序Hypervisor为某个虚拟机创建的虚拟化的中央处理器。 注：在实际的应用之中，虚拟化的CPU可以是对一个实体CPU以及/或者多核CPUs的时分共享，可以给一个虚拟机VM分配一个核或者多个核。此外，也可能是虚拟机管理程序Hypervisor仿真出一个CPU指令集，从而使得虚拟化的CPU指令集不同于原生的CPU指令集（仿真将会对性能产生重大的影响）。 （8）Virtualised Infrastructure Manager（VIM，虚拟化的基础设施管理器）：负责对网络功能虚拟化基础设施NFVI的计算资源、存储资源以及网络资源进行控制与管理的功能模块。虚拟化的基础设施管理器VIM可被部署于基础网络运营商的基础设施域Infrastructure Domain（例如：NFVI接入点/服务提供点）。 （9）Virtualised NIC（vNIC，虚拟化的网络接口控制器）：虚拟机管理程序Hypervisor为某个虚拟机创建的虚拟化的网络接口控制器。 （10）Virtualised Network Function（VNF，虚拟网络功能）：可以被部署于网络功能虚拟化基础设施NFVI的NF（网络功能）。 （11）Virtualised Network Function Instance（VNF Instance，虚拟化的网络功能模块实例）：虚拟化的网络功能软件的运行时间实例化。VNF实例是各个组件及其相互间连接的实例化在完成之后的结果。在相关实例化的过程之中，使用了在VNFD（Virtualised Network Function Descriptor，虚拟化的网络功能模块描述符）之中捕获的虚拟化网络功能部署及运行信息，以及额外的运行时实例特定信息与约束条件。 （12）Virtualised Network Function Component（VNFC，虚拟化网络功能组件）：虚拟化网络功能的内部组件，其可为虚拟化网络功能提供商提供相关功能的子集，主要的特征是：该组件的单个实例被 1：1 地映射到单个的虚拟化容器。 （13）Virtualised Network Function Component(VNFC) Instance（虚拟化的网络功能模块组件实例）：部署于特定的虚拟化容器实例的虚拟化的网络功能模块组件VNFC的实例。VNFC实例的生命周期管理与其父VNF实例具有相关性。 （14）Virtualised Network Function Descriptor（VNFD，虚拟化的网络功能模块描述符）：描述一个虚拟化网络功能模块的部署与操作行为的配置模板。其被用于虚拟化的网络功能模块的运行过程，以及对于虚拟化的网络功能模块实例的生命周期管理。 （15）Virtualised Network Function Manager（VNFM，虚拟化的网络功能模块管理器）：负责 VNF 生命周期管理的功能块。 （16）Virtualised Network Function Package（VNF Package，虚拟化的网络功能模块包）：包括对于虚拟化的网络功能模块描述符、与虚拟化的网络功能模块相关的软件镜像/映像、一些额外事项（例如：检查完整性，以证明相关存档的有效性）在内的归档。 （17）Virtualised NIC（vNIC，虚拟化的网络接口控制器）：虚拟机管理程序Hypervisor为某个虚拟机创建的虚拟化的网络接口控制器。 （18）Virtualised Storage（vStorage，虚拟化的存储）：分配给一个虚拟机VM的虚拟化非易失性存储。 （19）Virtualised Switch（vSwitch，虚拟化的交换机）：由虚拟机管理程序Hypervisor所实施的、可以互联虚拟机VM的虚拟化网络接口控制器的以太交换机（可以是各个虚拟化网络接口相互互联，也可以是虚拟交换机的虚拟化网络接口控制器与计算节点的网络接口控制器的互联）。 （20）Virtual link （VL，虚拟链路）：提供VNF之间的连接。 （21）VNF Forwarding Graph（VNF FG，虚拟网络功能转发表）：一个或一组Network Forwarding Path组成的。 （22）VNF Catalog（虚拟网络功能目录）：所有可用VNF描述符（VNFD）的存储库。 （23）VNF Set（虚拟化的网络功能模块集）：相互间具有未指定连接的虚拟化网络功能模块的集合。 W到Z 无。 参考资料  ETSI NFV 003 Terminology for Main Concepts in NFV ",
      "url"      : "https://y2p.cc/2017/01/11/nfv-define/",
      "keywords" : ""
    } ,
  
    {
      "title"    : "NFV 技术发展现状和未来展望",
      "category" : "NFV",
      "content": "NFV的技术发展现状和未来展望 NFV技术简介 NFV的全称是Network Function Vitiualized，中文翻译是“网络功能虚拟化”，简单理解就是把电信设备从目前的专用平台迁移到通用的X86 COTS（Commercial Off-The-Shelf，商用现成品或技术）服务器上。 深入理解一下，发现真相并不是这样： 在专业领域，通用化设备如果要达到专用设备的处理能力，则成本是无法接受的。例如渲染一定要靠GPU而不是CPU，例如专业网卡一定要靠自己的处理芯片而不是CPU。PCI本身的带宽就确定不能用于网络设备。PCIE的话一般简单用途是OK的，到了万兆以上吞吐就已经需要3.0版的4通道（3.938GB/s，服务器板子上8通道基本是存储专用，16通道基本看不到）才能达到。而同样的网卡用卡载芯片实现的，至少也是双接口甚至4接口，也就是你这一块卡可能需要8﹣16通道的带宽才能搞定。而企业核心网已经都是聚合40G或者80G的通讯量了，电信级的更是大的多，所以作为核心网络设备，这个体系结构本身就承受不了。CPU方面，由于它的设计是用来处理少量复杂任务而不是简单重复的大负载任务，干这事就跟渲染视频一样──不靠谱。 从趋势来看，虚拟化已经从概念走向实际使用。在推进过程中，虚拟化已经开始摒弃所谓的低成本策略（实际上现在的虚拟化方案如果单独从软硬件投资来看，比非虚拟化并不具有价格优势，通常会更贵），而重点强调 灵活、集中管理 等优势。举例：NFV性能优化——架构性并行加速算法思想。 越位于底层，基础设施属性越强，标准化程度越高，灵活性越差；越位于上层，情况则相反。所以可以看到，NFVI标准程度远高于MANO，NFV的灵活性和集中管理更多地体现在MANO上，而不是NFVI上。 在技术上，虚拟化也已经从软件开始向硬件过渡。也就是说，并不是把硬件抽象出来，而是把抽象的业务重新交给专业的硬件去做。比如服务端虚拟化方案中，主板开始支持硬件IO虚拟化，网卡已经可以直接使用硬件为虚拟机提供支持。 在实际的产品中，目前也有一种虚拟化的趋势，是融合方案，以思科为代表。即把所有的交换业务都集中到专业的网络交换机上，代替原来的计算中控、存储网络的交换设备。因为算下来，网络设备的单位价格处理能力，是最经济的，应该是被发展和利用的对象，而不是被取代的对象。 如下图所示：当前电信网络使用的各种设备，均是基于私有平台部署的，这样各种网元都是一个个封闭的盒子，各种盒子间硬件资源无法互用，每个设备扩容必须增加硬件，缩容后硬件资源闲置，耗时长，弹性差，成本高；在NFV方法中，各种网元变成了独立的应用，可以灵活部署在基于标准的服务器，存储，交换机构建的统一平台上，这样软硬件解耦，每个应用可以通过快速增加或减少虚拟资源来达到快速扩缩容的目的，大大提升网络的弹性。 为了实现上述目标，NFV的技术基础就是目前IT业界的云计算和虚拟化技术，通用的COTS计算/存储/网络硬件设备通过虚拟化技术可以分解为多种虚拟资源，供上层各种应用使用，同时通过虚拟化技术，可以使得应用与硬件解耦，使得资源的供给速度大大提高，从物理硬件的数天缩短到数分钟。 通过云计算技术，可以实现应用的弹性伸缩，从而实现了资源和业务负荷的匹配，即提高了资源利用效率，又保证了系统响应速度。 根据NFV的思想，一个虚拟的4G EPC系统部署方式如下图： 图中，一个vEPC系统由4个虚拟网元组成（2个P/SGW，1个MME，1个HSS），分别部署在4个数据中心中，这4个虚拟网元完成EPC网络规定的功能，提供EPC网络服务。 NFV的收益: 降低运营商采购/运维成本及能耗。 快速业务部署，缩小创新周期：包括提升测试与集成效率，降低开发成本。软件的快速安装取代新的硬件部署。 网络应用能实现多版本及多租户。支持不同的应用、用户、租户共享统一的平台。网络共享的支持水到渠成。 使能不同物理区域以及用户群的业务个性化，业务规模能够快速方便得伸缩。 使能网络开放，业务创新。可能孵化新的利润增长点。 引入NFV前 产业链：相对单一，核心成员主要包括设备制造商、芯片制造商等 传统设备制造商销售模式：软硬件一体化 网络架构：封闭 接口：杂乱 引入NFV后 产业链：核心成员主要包括通用硬件设备制造商、芯片制造商、虚拟化软件提供商、网元功能软件提供商、管理设备提供商等 传统设备制造商销售模式：通用硬件、虚拟化平台和网元功能软件三部分的销售模式 网络架构：开放 接口：统一且标准化 传统设备制造商的优劣势 优势：在网元功能软件上有技术壁垒，主要体现在对网络业务知识的积累上，具有较强的系统集成能力，向第三方系统集成商演进阻力较小 劣势：在通用硬件和虚拟化平台软件方面将面临来自IT领域的强大竞争 NFV技术当前发展现状 NFV定义的技术架构如下图所示: 同当前网络架构（独立的业务网络+OSS系统）相比，NFV从纵向和横向上进行了解构。 从纵向看分为三层  基础设施层：NFVI是NFV Infrastructure的简称，从云计算的角度看，就是一个资源池。NFVI映射到物理基础设施就是多个地理上分散的数据中心，通过高速通信网连接起来。 NFVI需要将物理计算/存储/交换资源通过虚拟化转换为虚拟的计算/存储/交换资源池。   虚拟网络层：虚拟网络层对应的就是目前各个电信业务网络，每个物理网元映射为一个虚拟网元VNF，VNF所需资源需要分解为虚拟的计算/存储/交换资源，由NFVI来承载，VNF之间的接口依然采用传统网络定义的信令接口（3GPP+ITU-T)，VNF的业务网管依然采用NE-EMS-NMS体制。   运营支撑层：运营支撑层就是目前的OSS/BSS系统，需要为虚拟化进行必要的修改和调整。 从横向看分为两个域  业务网络域：就是目前的各电信业务网络。   管理编排域：同传统网络最大区别就是，NFV增加了一个管理编排域，简称MANO，MANO负责对整个NFVI资源的管理和编排，负责业务网络和NFVI资源的映射和关联，负责OSS业务资源流程的实施等，MANO内部包括VIM，VNFM和Orchestrator三个实体，分别完成对NFVI，VNF和NS（NetworkService：即业务网络提供的网络服务）三个层次的管理。 NFV技术原理 一个业务网络可以分解为一组VNF和VNFL（VNFL：VNFLink），表示为VNF-FG（VNF Forwarding Graph），然后每个VNF可以分解为一组VNFC（VNF Componet）和内部连接图，每个VNFC映射为一个VM；对于每个VNFL，对应着一个IP连接，需要分配一定的链路资源（流量，QoS，路由等参数）。 通过这样的编排流程，一个业务网络可以通过MANO来自顶向下分解，直到可分配的资源，然后对应VM等资源由NFVI来分配，对应VNFL资源需要同承载网网管系统交互，由IP承载网来分配。 采用NFV部署一个网络服务的示意图如下所示： NFV技术存在问题 NFV定义的标准虽然从技术上看是可行的，但距离商用还有一定的距离，主要问题如下：  标准成熟度问题：NFV由于目标过大，第一阶段即将到期时，也只完成了4个总体规范，其他工作组定义的相关规范尚未完成。很多问题都被推迟到第二阶段实现，因此目前标准距离成熟尚有一定距离。   兼容性问题：NFV定义的架构很庞大，定义了多个新增接口，将原来封闭的电信设备商分解为多个层次：硬件设备供应商，虚拟化管理软件供应商，虚拟化电信网络软件供应商，NFVO（NFV Orchestrator）软件供应商，NFV系统集成商；这样电信网络从一个厂家完成的软硬件集成的事情转换为多个厂家的软硬件集成，复杂度大大提升；同时NFV只是定义架构层次，对应各个接口的具体定义和实现是协调其他开源或技术组织来实现，这样同一个组织制定标准相比，技术标准的严密性就会差一些，未来如何保证多厂家设备兼容就成为很大风险。   业务网络级的SON（Self-Organization Network）技术滞后，影响网络级弹性伸缩：按照NFV架构，虽然一个新的VNF所需资源是由MANO自动部署的，但业务网络的运维架构依然依靠传统的EMS/NMS机制，各VNF之间的连接和话务路由还是由人工来配置的，无法实现一个VNF的即插即用；因此要实现业务网络级的弹性伸缩，还需要发展业务网络的SON技术，实现VNF的即插即用，并且需要SON技术同VNF厂家解耦，可以对多厂家VNF进行SON，这在技术和管理上都是比较困难的。   虚拟化的可靠性技术：传统电信应用通常都要求5个9的可靠性，虚拟化后并不能降低电信应用的可靠性要求，传统电信硬件通过特殊设计，可靠性通常较高，而虚拟化采用的COTS设备可靠性相对降低了，需要通过提升软件可靠性来补偿。   网络虚拟化技术相对滞后：同计算和存储虚拟化技术相比，网络虚拟化技术还比较落后，SDN尚不成熟，目前网络虚拟化技术类型繁多，如何整合到NFVI中是一个较大风险。对于电信业务网络来说，由于通常是一个分布式网络，因此需要配置较大的网络资源来承载，这种网络资源需要分解到数据中心内部的局域网络资源和数据中心间的承载网络资源，业务网络与接入网络间的承载网络资源等，承载网络资源分配又可能涉及到传送网络资源分配；这些资源的分配都需要做到虚拟化，自动化，目前这种分配尚需要通过承载网/传送网网管来进行，距离自动化尚有较大距离；现在最有希望的技术就是SDN了，等待SDN在这些领域中应用后与NFV配合。   系统集成问题：NFV本身解决的是业务网络的自动部署问题，从架构看也是一个巨型的ICT系统集成工程，分解一下包括NFVI的集成，VNF的集成，和业务网络的集成，涉及的系统，厂家，地域，接口都非常多，工程难度比目前公共云/私有云更高；虽然是自动部署，但目前电信网络部署的各环节（规划，实施，调测，升级，优化，运维等）都会涉及并执行，将来如何进行实施部署将是一个很复杂的问题，对集成商的技术要求非常高。 NFV技术未来展望 从上面各小节我们看出，NFV是一个宏大的架构，对传统网络部署方式是颠覆性的变化。NFV拓展了运营商基础设施范围，将数据中心设备/承载网设备/虚拟化软件系统/MANO系统均转化为基础设施，业务部署均转化为软件部署，业务网络资源与负荷实时匹配，资源利用效率得到最大提升。 采用NFV架构后，电信网络的自动化管理和敏捷性将大为提升，一个电信设备的部署周期从几个月缩短为几个小时，扩容周期从几周扩展到几分钟，电信网络新业务部署周期将从数月级缩短到数周级，电信运营商将真正具备“大象跳舞”的能力。 NFV将率先在五大应用场景落地 vBRAS(Virtual Broadband Remote Access Server)虚拟化宽带远程接入服务器 vCPE (Virtual Customer Premise Equipment) 虚拟化用户预制设备 vEPC (Virtual Evolved Packet Core)   虚拟化演进分组核心网 vIMS (Virtual IP Multimedia Subsystem)  虚拟化IP多媒体子系统 vSR (Virtual Service Router)   虚拟化业务路由器 NFV关键技术问题 VNF生命周期管理:MANO,NFVO、VNFM、VIM 性能：  网络I/O能力（现有解决方案）    Single root I/O virtualization (SR-IOV)：一种基于硬件的虚拟化通道技术。虚拟机直接连接到物理网卡上，获得等同于物理网卡的I/O性能和低时延，且多个虚拟机之间高效共享物理网卡   Data Plane Developers Kit (DPDK): 一种内核旁路机制。为了提高包处理的速度，允许虚拟交换机旁路内核并直接与兼容的网卡通信     计算能力（现有解决方案）    超线程技术。要求COTS硬件支持超线程技术提高CPU的并发处理数，使得单个处理器可使用线程级并行计算，减少CPU的闲置时间   硬件加速机制。目前业界主流的专用硬件加速技术包括通用加速资源池、专用PCI加速卡或CPU内置加速芯片等，但具体采用何种专用硬件加速技术仍有待进一步研究评估     中间件性能损耗   可靠性：传统电信设备99.999%的可靠性，通用设备只能达到99.9%的可靠性 安全：  新的潜在安全问题    引入新的高危区域——虚拟化管理层   弹性、虚拟网络使安全边界模糊，安全策略难于随网络调整而实时、动态迁移   用户失去对资源的完全控制以及多租户共享计算资源，带来的数据泄漏与攻击风险   存在安全风险的关键组件     VNF组件实例   绑定到VNF组件实例的本地网络资源   远程设备上对本地VNF组件实例的参考   VNF组件实例占用的本地、远程以及交换存储       安全事故处理     如何保证关键组件所涉及的硬件、内存不被非法访问   如何保证VNF上应用的现有授权不被改变   本地和远程资源彻底清除崩溃的VNF资源及授权不被滥用         现有解决方案    对于物理硬件资源     COTS硬件需要提供基于硬件芯片的可信环境，以便于虚拟层及业务软件层可基于硬件可信环境构筑信任根，实现安全启动和安全存储   目前通过使用可信赖的平台模块（Trusted Platform Module，TPM）来完成加密、签名、认证、密钥生成等功能   对于接口的访问，COTS硬件需要进行访问控制和认证鉴权，防止非法访问       对于虚拟化平台资源     要求提供租户、虚拟机以及不同业务的安全隔离，避免虚拟机之间的数据窃取或恶意攻击   对于虚拟化平台及其所管理的虚拟资源的访问，要求提供访问控制和认证鉴权，防止非法访问对系统的影响   在存储方面，对物理存储实体的直接访问具备禁止或限制的能力，提供虚拟存储数据清除、虚拟存储数据审计、虚拟存储数据访问控制和冗余备份功能；在计算方面，能利用加密算法协处理器，高效实现VNF内部的机密性和完整性保护       对于网络资源     应具备对虚拟机的隔离和访问控制能力，虚拟机之间需要授权和鉴权后才能建立通信连接   通过在线的深度报文检测、虚拟防火墙技术以及基于Netflow采集的流量溯源与关联分析等综合技术，可以建立基本的NFV网络安全防护体系，能够支持抵御畸形报文攻击、DoS攻击和仿冒攻击等         集成部署：多厂商软硬件集成、标准和开源的混合 开放互联：如何规范开放API编程接口 标准和开源项目进展 开源方式：通过开源项目构建产业生态，以迭代开发模式加速产品应用并形成事实标准，影响技术和产品的发展方向 标准方式：传统电信领域的标准化更多是通过文稿制充分讨论后达成共识，再在产品中实现 参考资料  NFV的技术发展现状和未来展望 ",
      "url"      : "https://y2p.cc/2017/01/14/nfv-white-paper/",
      "keywords" : ""
    } ,
  
    {
      "title"    : "CONFIG LEVEL",
      "category" : "Tool",
      "content": "应用配置层次 应用配置层次  程序内内置配置项的初始默认值   配置文件中的配置项值可以覆盖(override)程序内配置项的默认值   命令行选项和参数值具有最高优先级，可以override前两层的配置项值  ",
      "url"      : "https://y2p.cc/2017/02/14/config/",
      "keywords" : ""
    } ,
  
    {
      "title"    : "COBRA",
      "category" : "Tool",
      "content": "COBRA既是一个用来创建强大的现代CLI命令行的GOLANG库，也是一个生成程序应用和命令行文件的程序 git repo https://github.com/spf13/cobra 示例 Cobra提供的功能 简易的子命令行模式，如 app server， app fetch等等 完全兼容posix命令行模式 嵌套子命令subcommand 支持全局，局部，串联flags 使用Cobra很容易的生成应用程序和命令，使用cobra create appname和cobra add cmdname 如果命令输入错误，将提供智能建议，如 app srver，将提示srver没有，是否是app server 自动生成commands和flags的帮助信息 自动生成详细的help信息，如app help 自动识别-h，–help帮助flag 自动生成应用程序在bash下命令自动完成功能 自动生成应用程序的man手册 命令行别名 自定义help和usage信息 可选的紧密集成的viper apps 如何使用 安装cobra go get -v github.com/spf13/cobra/cobra 在安装的过程中，由于GWF（长城防火墙）的原因，golang的资源无法访问  解决的思路：git clone 源代码，在GOPATH的src目录中建立相关目录，将clone的源码拷贝进去 注意：目录的名称会有变化 举例如下：  直接找到github上的源码地址，Git clone https://github.com/go-yaml/yaml.git  在src 目录下 创建 gopkg.in 目录 把yaml 重命名为yaml.v2   little example package main import (  fmt github.com/spf13/cobra ) func main() {  var name string  var food string  var myFood string // 1. 主命令  var rootCmd = &amp;cobra.Command{  Use: personLikeEat,  Short: Input the chinese name and food name,  //命令执行的函数  Run: func(cmd *cobra.Command, args []string) {   if len(name) == 0 {    cmd.Help()    return   }   if len(food) == 0 {    cmd.Help()    return   }   fmt.Printf(%s like eat %s.  , name, food)  },  }  // 2. 子命令  var subCmd = &amp;cobra.Command{  Use: ILikeEat,  Short: Input food information,  //命令执行的函数  Run: func(cmd *cobra.Command, args []string) {   if len(myFood) == 0 {    cmd.Help()    return   }   fmt.Printf(I like eat %s.  , myFood)  },  }  // 添加子命令  rootCmd.AddCommand(subCmd)  // 3.1 主命令添加选项  rootCmd.Flags().StringVarP(&amp;name, name, n, , person's name)  rootCmd.Flags().StringVarP(&amp;food, food, f, , food's name)  // 3.2 子命令添加选项  subCmd.Flags().StringVarP(&amp;myFood, food, f, , food's name)  // 执行命令  rootCmd.Execute() } 运行结果如下： E:  mygo  src  test  demo  personLikeEat&gt;personLikeEat Input the chinese name and food name Usage: personLikeEat [flags] personLikeEat [command] Available Commands: ILikeEat Input food information Flags: -f, --food string food's name -n, --name string person's name Use personLikeEat [command] --help for more information about a command. E:  mygo  src  test  demo  personLikeEat&gt;personLikeEat -n yp -f apple yp like eat apple. E:  mygo  src  test  demo  personLikeEat&gt;personLikeEat ILikeEat Input food information Usage: personLikeEat ILikeEat [flags] Flags: -f, --food string food's name E:  mygo  src  test  demo  personLikeEat&gt;personLikeEat ILikeEat -f banana I like eat banana. 基本用法就是这四步： 主命令 子命令 添加选项 执行命令 使用cobra生成应用程序 假设现在我们要开发一个基于CLIs的命令程序，名字为cobraDemo。首先打开CMD，切换到GOPATH的src目录下，执行如下指令: E:  mygo  src&gt;..  bin  cobra.exe init cobraDemo Your Cobra application is ready at E:  mygo  src  cobraDemo Give it a try by going there and running `go run main.go` Add commands to it by running `cobra add [cmdname]` 在src目录下会生成一个cobraDemo的文件夹，如下： cobraDemo │ LICENSE │ main.go │ └─cmd  root.go 如果你的cobraDemo程序没有subcommands，那么cobra生成应用程序的操作就结束了。 如何实现没有子命令的CLIs程序 接下来就是可以继续cobraDemo的功能设计了。例如我在demo下面新建一个包，名称为imp。如下： cobraDemo │ LICENSE │ main.go │ ├─cmd │ root.go │  └─imp  imp.go imp.go文件的代码如下： package imp import(  fmt ) func Show(name string, age int) {  fmt.Printf(My Name is %s, My age is %d  , name, age) } cobraDemo程序成命令行接收两个参数name和age，然后打印出来。打开cobra自动生成的main.go文件查看： // Copyright © 2017 NAME HERE &lt;EMAIL ADDRESS&gt; // // Licensed under the Apache License, Version 2.0 (the License); // you may not use this file except in compliance with the License. // You may obtain a copy of the License at // //  http://www.apache.org/licenses/LICENSE-2.0 // // Unless required by applicable law or agreed to in writing, software // distributed under the License is distributed on an AS IS BASIS, // WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. // See the License for the specific language governing permissions and // limitations under the License. package main import cobraDemo/cmd func main() {  cmd.Execute() } 可以看出main函数执行cmd包，所以我们只需要在cmd包内调用imp包就能实现demo程序的需求。接着打开root.go文件查看： // Copyright © 2017 NAME HERE &lt;EMAIL ADDRESS&gt; // // Licensed under the Apache License, Version 2.0 (the License); // you may not use this file except in compliance with the License. // You may obtain a copy of the License at // //  http://www.apache.org/licenses/LICENSE-2.0 // // Unless required by applicable law or agreed to in writing, software // distributed under the License is distributed on an AS IS BASIS, // WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. // See the License for the specific language governing permissions and // limitations under the License. package cmd import (  fmt  os github.com/spf13/cobra  github.com/spf13/viper ) var cfgFile string // RootCmd represents the base command when called without any subcommands var RootCmd = &amp;cobra.Command{  Use: cobraDemo,  Short: A brief description of your application,  Long: `A longer description that spans multiple lines and likely contains examples and usage of using your application. For example: Cobra is a CLI library for Go that empowers applications. This application is a tool to generate the needed files to quickly create a Cobra application.`, // Uncomment the following line if your bare application // has an action associated with it: // Run: func(cmd *cobra.Command, args []string) { }, } // Execute adds all child commands to the root command sets flags appropriately. // This is called by main.main(). It only needs to happen once to the rootCmd. func Execute() {  if err := RootCmd.Execute(); err != nil {  fmt.Println(err)  os.Exit(-1)  } } func init() {  cobra.OnInitialize(initConfig) // Here you will define your flags and configuration settings.  // Cobra supports Persistent Flags, which, if defined here,  // will be global for your application. RootCmd.PersistentFlags().StringVar(&amp;cfgFile, config, , config file (default is $HOME/.cobraDemo.yaml))  // Cobra also supports local flags, which will only run  // when this action is called directly.  RootCmd.Flags().BoolP(toggle, t, false, Help message for toggle) } // initConfig reads in config file and ENV variables if set. func initConfig() {  if cfgFile != { // enable ability to specify config file via flag  viper.SetConfigFile(cfgFile)  } viper.SetConfigName(.cobraDemo) // name of config file (without extension)  viper.AddConfigPath($HOME) // adding home directory as first search path  viper.AutomaticEnv()  // read in environment variables that match // If a config file is found, read it in.  if err := viper.ReadInConfig(); err == nil {  fmt.Println(Using config file:, viper.ConfigFileUsed())  } } 从源代码来看cmd包进行了一些初始化操作并提供了Execute接口。十分简单，其中viper是cobra集成的配置文件读取的库，这里不需要使用，我们可以注释掉（不注释可能生成的应用程序很大约10M，这里没用到最好是注释掉）。cobra的所有命令都是通过cobra.Command这个结构体实现的。为了实现demo功能，显然我们需要修改RootCmd。修改后的代码如下： // Copyright © 2017 NAME HERE &lt;EMAIL ADDRESS&gt; // // Licensed under the Apache License, Version 2.0 (the License); // you may not use this file except in compliance with the License. // You may obtain a copy of the License at // //  http://www.apache.org/licenses/LICENSE-2.0 // // Unless required by applicable law or agreed to in writing, software // distributed under the License is distributed on an AS IS BASIS, // WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. // See the License for the specific language governing permissions and // limitations under the License. package cmd import (  fmt  os github.com/spf13/cobra  // github.com/spf13/viper  cobraDemo/imp ) // var cfgFile string var name string var age int // RootCmd represents the base command when called without any subcommands var RootCmd = &amp;cobra.Command{  Use: cobraDemo,  Short: A test demo,  Long: `Demo is a test appcation for print things`,  // Uncomment the following line if your bare application  // has an action associated with it:  Run: func(cmd *cobra.Command, args []string) {  if len(name) == 0 {   cmd.Help()   return  }  imp.Show(name, age)  }, } // Execute adds all child commands to the root command sets flags appropriately. // This is called by main.main(). It only needs to happen once to the rootCmd. func Execute() {  if err := RootCmd.Execute(); err != nil {  fmt.Println(err)  os.Exit(-1)  } } func init() {  // cobra.OnInitialize(initConfig) // Here you will define your flags and configuration settings.  // Cobra supports Persistent Flags, which, if defined here,  // will be global for your application. // RootCmd.PersistentFlags().StringVar(&amp;cfgFile, config, , config file (default is $HOME/.cobraDemo.yaml))  // Cobra also supports local flags, which will only run  // when this action is called directly.  // RootCmd.Flags().BoolP(toggle, t, false, Help message for toggle)  RootCmd.Flags().StringVarP(&amp;name, name, n, , person's name)  RootCmd.Flags().IntVarP(&amp;age, age, a, 0, person's age)  } // initConfig reads in config file and ENV variables if set. // func initConfig() { // if cfgFile != { // enable ability to specify config file via flag // viper.SetConfigFile(cfgFile) // } // viper.SetConfigName(.cobraDemo) // name of config file (without extension) // viper.AddConfigPath($HOME) // adding home directory as first search path // viper.AutomaticEnv()  // read in environment variables that match // // If a config file is found, read it in. // if err := viper.ReadInConfig(); err == nil { // fmt.Println(Using config file:, viper.ConfigFileUsed()) // } //} 到此demo的功能已经实现了，我们编译运行一下看看实际效果： E:  mygo  src  cobraDemo&gt;go build E:  mygo  src  cobraDemo&gt;cobraDemo.exe Demo is a test appcation for print things Usage: cobraDemo [flags] Flags: -a, --age int  person's age -n, --name string person's name E:  mygo  src  cobraDemo&gt;cobraDemo.exe -n yp -a 26 My Name is yp, My age is 26 如何实现带有子命令的CLIs程序 在执行cobra.exe init demo之后，继续使用cobra为demo添加子命令test： E:  mygo  src  cobraDemo&gt;..  ..  bin  cobra add test test created at E:  mygo  src  cobraDemo  cmd  test.go 在src目录下demo的文件夹下生成了一个cmd  test.go文件，如下： cobraDemo │ cobraDemo.exe │ LICENSE │ main.go │ ├─cmd │ root.go │ test.go │  └─imp  imp.go 接下来的操作就和上面修改root.go文件一样去配置test子命令。效果如下： E:  mygo  src  cobraDemo&gt;go build E:  mygo  src  cobraDemo&gt;cobraDemo.exe Demo is a test appcation for print things Usage: cobraDemo [flags] cobraDemo [command] Available Commands: test  A brief description of your command Flags: -a, --age int  person's age -n, --name string person's name Use cobraDemo [command] --help for more information about a command.  可以看出demo既支持直接使用标记flag，又能使用子命令 E:  mygo  src  cobraDemo&gt;cobraDemo.exe test -h A longer description that spans multiple lines and likely contains examples and usage of using your command. For example: Cobra is a CLI library for Go that empowers applications. This application is a tool to generate the needed files to quickly create a Cobra application. Usage: cobraDemo test [flags] 调用test命令输出信息，这里没有对默认信息进行修改。 E:  mygo  src  cobraDemo&gt;cobraDemo.exe tet Error: unknown command tet for cobraDemo Did you mean this?  test Run 'cobraDemo --help' for usage. unknown command tet for cobraDemo Did you mean this?  test 这是错误命令提示功能 函数解析 函数说明 cobra doc 关于代码执行顺序 // The *Run functions are executed in the following order: // * PersistentPreRun() // * PreRun() // * Run() // * PostRun() // * PersistentPostRun() 修改root.go文件如下： // Copyright © 2017 NAME HERE &lt;EMAIL ADDRESS&gt; // // Licensed under the Apache License, Version 2.0 (the License); // you may not use this file except in compliance with the License. // You may obtain a copy of the License at // //  http://www.apache.org/licenses/LICENSE-2.0 // // Unless required by applicable law or agreed to in writing, software // distributed under the License is distributed on an AS IS BASIS, // WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. // See the License for the specific language governing permissions and // limitations under the License. package cmd import (  fmt  os github.com/spf13/cobra  // github.com/spf13/viper  cobraDemo/imp ) // var cfgFile string var name string var age int // RootCmd represents the base command when called without any subcommands var RootCmd = &amp;cobra.Command{  Use: cobraDemo,  Short: A test demo,  Long: `Demo is a test appcation for print things`,  // Uncomment the following line if your bare application  // has an action associated with it:  // The *Run functions are executed in the following order:  // * PersistentPreRun()  // * PreRun()  // * Run()  // * PostRun()  // * PersistentPostRun()  PersistentPreRun: func(cmd *cobra.Command, args []string) {  fmt.Printf(Inside rootCmd PersistentPreRun with args: %v  , args)  },  PreRun: func(cmd *cobra.Command, args []string) {  fmt.Printf(Inside rootCmd PreRun with args: %v  , args)  },   Run: func(cmd *cobra.Command, args []string) {  if len(name) == 0 {   cmd.Help()   return  }  imp.Show(name, age)  },  PostRun: func(cmd *cobra.Command, args []string) {  fmt.Printf(Inside rootCmd PostRun with args: %v  , args)  },  PersistentPostRun: func(cmd *cobra.Command, args []string) {  fmt.Printf(Inside rootCmd PersistentPostRun with args: %v  , args)  }, } // Execute adds all child commands to the root command sets flags appropriately. // This is called by main.main(). It only needs to happen once to the rootCmd. func Execute() {  if err := RootCmd.Execute(); err != nil {  fmt.Println(err)  os.Exit(-1)  } } func init() {  // cobra.OnInitialize(initConfig) // Here you will define your flags and configuration settings.  // Cobra supports Persistent Flags, which, if defined here,  // will be global for your application. // RootCmd.PersistentFlags().StringVar(&amp;cfgFile, config, , config file (default is $HOME/.cobraDemo.yaml))  // Cobra also supports local flags, which will only run  // when this action is called directly.  // RootCmd.Flags().BoolP(toggle, t, false, Help message for toggle)  RootCmd.Flags().StringVarP(&amp;name, name, n, , person's name)  RootCmd.Flags().IntVarP(&amp;age, age, a, 0, person's age)  } 测试结果如下： E:  mygo  src  cobraDemo&gt;go build E:  mygo  src  cobraDemo&gt;cobraDemo.exe Inside rootCmd PersistentPreRun with args: [] Inside rootCmd PreRun with args: [] Demo is a test appcation for print things Usage: cobraDemo [flags] cobraDemo [command] Available Commands: test  A brief description of your command Flags: -a, --age int  person's age -n, --name string person's name Use cobraDemo [command] --help for more information about a command. Inside rootCmd PostRun with args: [] Inside rootCmd PersistentPostRun with args: [] E:  mygo  src  cobraDemo&gt;cobraDemo.exe -n yp -a 26 Inside rootCmd PersistentPreRun with args: [] Inside rootCmd PreRun with args: [] My Name is yp, My age is 26 Inside rootCmd PostRun with args: [] Inside rootCmd PersistentPostRun with args: [] E:  mygo  src  cobraDemo&gt;cobraDemo.exe test Inside rootCmd PersistentPreRun with args: [] test called Inside rootCmd PersistentPostRun with args: [] 当使用–help命令的时候，PersistentPreRun、PreRun、PostRun、PersistentPostRun不会被调用 示例如下： E:  mygo  src  cobraDemo&gt;cobraDemo.exe --help Demo is a test appcation for print things Usage: cobraDemo [flags] cobraDemo [command] Available Commands: test  A brief description of your command Flags: -a, --age int  person's age -n, --name string person's name Use cobraDemo [command] --help for more information about a command. “数据类型” + “P” 此种类型支持短标记 // BoolP is like Bool, but accepts a shorthand letter that can be used after a single dash. func (f *FlagSet) BoolP(name, shorthand string, value bool, usage string) *bool {  p := new(bool)  f.BoolVarP(p, name, shorthand, value, usage)  return p } “数据类型” + “Var” 此种类型表示变量 // BoolVar defines a bool flag with specified name, default value, and usage string. // The argument p points to a bool variable in which to store the value of the flag. func BoolVar(p *bool, name string, value bool, usage string) {  BoolVarP(p, name, , value, usage) } 全局短标记过时  RootCmd.PersistentFlags().BoolP(help, h, false, Print usage)  RootCmd.PersistentFlags().MarkShorthandDeprecated(help, please use --help) 示例如下： E:  mygo  src  cobraDemo&gt;cobraDemo.exe -h Flag shorthand -h has been deprecated, please use --help Demo is a test appcation for print things Usage: cobraDemo [flags] cobraDemo [command] Available Commands: test  A brief description of your command Flags: -a, --age int  person's age  --help  Print usage -n, --name string person's name Use cobraDemo [command] --help for more information about a command. E:  mygo  src  cobraDemo&gt;cobraDemo.exe test -h Flag shorthand -h has been deprecated, please use --help A longer description that spans multiple lines and likely contains examples and usage of using your command. For example: Cobra is a CLI library for Go that empowers applications. This application is a tool to generate the needed files to quickly create a Cobra application. Usage: cobraDemo test [flags] Global Flags:  --help Print usage ",
      "url"      : "https://y2p.cc/2017/02/17/cobra/",
      "keywords" : ""
    } ,
  
    {
      "title"    : "DOCKER CLI 代码解析",
      "category" : "Docker",
      "content": "DOCKER CLI 代码解析 cobra示例 docker将客户端程序和服务器程序分为docker和dockerd，两者的命令行解析都采用第三方库：cobra 首先用个小程序介绍一下cobra的简单用法： 基本用法是这四步： 主命令 子命令 添加选项 执行命令 docker职责 接受并解析客户端的操作指令。 docker cli 代码解析 从main函数开始 dcokerd的入口函数在文件docker/cmd/docker/docker.go中 func main() {  // 日志采用第三方库logrus  stdin, stdout, stderr := term.StdStreams()  logrus.SetOutput(stderr) dockerCli := command.NewDockerCli(stdin, stdout, stderr) // 主命令、子命令、添加选项都在此函数中  cmd := newDockerCommand(dockerCli) // 执行命令  if err := cmd.Execute(); err != nil {  if sterr, ok := err.(cli.StatusError); ok {   if sterr.Status != {    fmt.Fprintln(stderr, sterr.Status)   }   // StatusError should only be used for errors, and all errors should   // have a non-zero exit status, so never exit with 0   if sterr.StatusCode == 0 {    os.Exit(1)   }   os.Exit(sterr.StatusCode)  }  fmt.Fprintln(stderr, err)  os.Exit(1)  } } newDockerCommand继续解析 func newDockerCommand(dockerCli *command.DockerCli) *cobra.Command {  // 选项结构对象，通过传入的选项选项参数进行填充  opts := cliflags.NewClientOptions() // 选项集合  var flags *pflag.FlagSet // 主命令  cmd := &amp;cobra.Command{  Use:   docker [OPTIONS] COMMAND [ARG...],  Short:  A self-sufficient runtime for containers,  SilenceUsage:  true,  SilenceErrors: true,  TraverseChildren: true,  Args:   noArgs,  RunE: func(cmd *cobra.Command, args []string) error {   if opts.Version {    showVersion()    return nil   }   return dockerCli.ShowHelp(cmd, args)  },  PersistentPreRunE: func(cmd *cobra.Command, args []string) error {   // daemon command is special, we redirect directly to another binary   if cmd.Name() == daemon {    return nil   }   // flags must be the top-level command flags, not cmd.Flags()   opts.Common.SetDefaultOptions(flags)   dockerPreRun(opts)   if err := dockerCli.Initialize(opts); err != nil {    return err   }   return isSupported(cmd, dockerCli.Client().ClientVersion(), dockerCli.HasExperimental())  },  } // 设置默认的处理方式  cli.SetupRootCommand(cmd) // 主命令添加选项  flags = cmd.Flags()  flags.BoolVarP(&amp;opts.Version, version, v, false, Print version information and quit)  flags.StringVar(&amp;opts.ConfigDir, config, cliconfig.Dir(), Location of client config files) // 添加公共选项  opts.Common.InstallFlags(flags) setFlagErrorFunc(dockerCli, cmd, flags, opts) setHelpFunc(dockerCli, cmd, flags, opts) // 设置命令的输出  cmd.SetOutput(dockerCli.Out()) // 添加daemon选项，服务器功能在docker命令中已经作废，这里只是提示“请使用dockerd”  cmd.AddCommand(newDaemonCommand()) // 子命令  commands.AddCommands(cmd, dockerCli) // 参数合法性验证  setValidateArgs(dockerCli, cmd, flags, opts) return cmd } 这里主要是主命令的解析，子命令还需要继续看这个函数  // 子命令  commands.AddCommands(cmd, dockerCli) AddCommands添加子命令 // AddCommands adds all the commands from cli/command to the root command func AddCommands(cmd *cobra.Command, dockerCli *command.DockerCli) {  cmd.AddCommand(  // checkpoint  checkpoint.NewCheckpointCommand(dockerCli),  // container  container.NewContainerCommand(dockerCli),  container.NewRunCommand(dockerCli),  // image  image.NewImageCommand(dockerCli),  image.NewBuildCommand(dockerCli),  // node  node.NewNodeCommand(dockerCli),  // network  network.NewNetworkCommand(dockerCli),  // plugin  plugin.NewPluginCommand(dockerCli),  // registry  registry.NewLoginCommand(dockerCli),  registry.NewLogoutCommand(dockerCli),  registry.NewSearchCommand(dockerCli),  // secret  secret.NewSecretCommand(dockerCli),  // service  service.NewServiceCommand(dockerCli),  // system  system.NewSystemCommand(dockerCli),  system.NewVersionCommand(dockerCli),  // stack  stack.NewStackCommand(dockerCli),  stack.NewTopLevelDeployCommand(dockerCli),  // swarm  swarm.NewSwarmCommand(dockerCli),  // volume  volume.NewVolumeCommand(dockerCli),  // legacy commands may be hidden  hide(system.NewEventsCommand(dockerCli)),  hide(system.NewInfoCommand(dockerCli)),  hide(system.NewInspectCommand(dockerCli)),  hide(container.NewAttachCommand(dockerCli)),  hide(container.NewCommitCommand(dockerCli)),  hide(container.NewCopyCommand(dockerCli)),  hide(container.NewCreateCommand(dockerCli)),  hide(container.NewDiffCommand(dockerCli)),  hide(container.NewExecCommand(dockerCli)),  hide(container.NewExportCommand(dockerCli)),  hide(container.NewKillCommand(dockerCli)),  hide(container.NewLogsCommand(dockerCli)),  hide(container.NewPauseCommand(dockerCli)),  hide(container.NewPortCommand(dockerCli)),  hide(container.NewPsCommand(dockerCli)),  hide(container.NewRenameCommand(dockerCli)),  hide(container.NewRestartCommand(dockerCli)),  hide(container.NewRmCommand(dockerCli)),  hide(container.NewStartCommand(dockerCli)),  hide(container.NewStatsCommand(dockerCli)),  hide(container.NewStopCommand(dockerCli)),  hide(container.NewTopCommand(dockerCli)),  hide(container.NewUnpauseCommand(dockerCli)),  hide(container.NewUpdateCommand(dockerCli)),  hide(container.NewWaitCommand(dockerCli)),  hide(image.NewHistoryCommand(dockerCli)),  hide(image.NewImagesCommand(dockerCli)),  hide(image.NewImportCommand(dockerCli)),  hide(image.NewLoadCommand(dockerCli)),  hide(image.NewPullCommand(dockerCli)),  hide(image.NewPushCommand(dockerCli)),  hide(image.NewRemoveCommand(dockerCli)),  hide(image.NewSaveCommand(dockerCli)),  hide(image.NewTagCommand(dockerCli)),  ) } 这里添加了很多子命令，这些命令都是一级子命令，后面还可跟二级子命令。 有些是被hide函数包裹的，表示这些函数是之前的版本使用的，在新的版本中不推荐使用， 在实际环境中可以通过设置DOCKER_HIDE_LEGACY_COMMANDS环境变量来实现，具体见HIDE LEGACY COMMANDS。 我们拿一条命令来继续解析 hide(image.NewHistoryCommand(dockerCli)), NewHistoryCommand函数 func NewHistoryCommand(dockerCli *command.DockerCli) *cobra.Command {  var opts historyOptions cmd := &amp;cobra.Command{  Use: history [OPTIONS] IMAGE,  Short: Show the history of an image,  Args: cli.ExactArgs(1),  // 命令执行函数，错误类型的变量作为返回值  RunE: func(cmd *cobra.Command, args []string) error {   opts.image = args[0]   return runHistory(dockerCli, opts)  },  } // 添加选项  flags := cmd.Flags() flags.BoolVarP(&amp;opts.human, human, H, true, Print sizes and dates in human readable format)  flags.BoolVarP(&amp;opts.quiet, quiet, q, false, Only show numeric IDs)  flags.BoolVar(&amp;opts.noTrunc, no-trunc, false, Don't truncate output) return cmd } 当我们执行docker history时，就会执行命令中定义的函数，进而执行runHistory(),该函数根据选项执行不同的操作。 runHistory函数 func runHistory(dockerCli *command.DockerCli, opts historyOptions) error {  ctx := context.Background() history, err := dockerCli.Client().ImageHistory(ctx, opts.image)  if err != nil {  return err  } w := tabwriter.NewWriter(dockerCli.Out(), 20, 1, 3, ' ', 0) if opts.quiet {  for _, entry := range history {   if opts.noTrunc {    fmt.Fprintf(w, %s  , entry.ID)   } else {    fmt.Fprintf(w, %s  , stringid.TruncateID(entry.ID))   }  }  w.Flush()  return nil  } var imageID string  var createdBy string  var created string  var size string fmt.Fprintln(w, IMAGE  tCREATED  tCREATED BY  tSIZE  tCOMMENT)  for _, entry := range history {  imageID = entry.ID  createdBy = strings.Replace(entry.CreatedBy,   t, , -1)  if !opts.noTrunc {   createdBy = stringutils.Ellipsis(createdBy, 45)   imageID = stringid.TruncateID(entry.ID)  }  if opts.human {   created = units.HumanDuration(time.Now().UTC().Sub(time.Unix(entry.Created, 0))) + ago   size = units.HumanSizeWithPrecision(float64(entry.Size), 3)  } else {   created = time.Unix(entry.Created, 0).Format(time.RFC3339)   size = strconv.FormatInt(entry.Size, 10)  }  fmt.Fprintf(w, %s  t%s  t%s  t%s  t%s  , imageID, created, createdBy, size, entry.Comment)  }  w.Flush()  return nil } ",
      "url"      : "https://y2p.cc/2017/02/22/docker-cli/",
      "keywords" : ""
    } ,
  
    {
      "title"    : "DOCKER环境变量解析",
      "category" : "Docker",
      "content": "[WIP] DOCKER环境变量解析 环境变量的设置 查看docker环境变量 查看全局环境变量：env | grep DOCKER 查看所有环境变量：set | grep DOCKER 修改环境变量 1. 用命令直接修改全局环境变量，但只能在当前会话生效  export XXXXXX=XXXXXX 2. 修改全局配置文件，机器重启后依然有效  vim /etc/profile  在文档最后，添加: export XXXXXX=XXXXXX  执行 source /etc/profile 否则需要重写登陆 3. 类似个人环境变量配置文件  vim ~/.bash_profile  执行 source /etc/profile 否则需要重写登陆 在代码中搜索环境变量 通过关键字：os.Getenv(DOCKER DOCKER_HIDE_LEGACY_COMMANDS 含义 这些命令在旧的版本中使用过，在新版本中不推荐使用。 使用 将其赋为非空字符串，则不推荐使用的命令被隐藏 将其赋为空字符串，则不推荐使用的命令出现 示例 $ export DOCKER_HIDE_LEGACY_COMMANDS= [node1] (local) root@10.0.5.3 ~ $ env | grep DOCKER DOCKER_BUCKET=get.docker.com DOCKER_HIDE_LEGACY_COMMANDS= DOCKER_SHA256=97892375e756fd29a304bd8cd9ffb256c2e7c8fd759e12a55a6336e15100ad75 DOCKER_STORAGE_DRIVER=overlay2 DOCKER_VERSION=1.13.1 [node1] (local) root@10.0.5.3 ~ $ docker --help Usage: docker COMMAND A self-sufficient runtime for containers Options:  --config string Location of client config files (default /root/.docker) -D, --debug   Enable debug mode  --help   Print usage -H, --host list  Daemon socket(s) to connect to (default []) -l, --log-level string Set the logging level (debug, info, warn, error, fatal) (default info)  --tls   Use TLS; implied by --tlsverify  --tlscacert string Trust certs signed only by this CA (default /root/.docker/ca.pem)  --tlscert string  Path to TLS certificate file (default /root/.docker/cert.pem)  --tlskey string Path to TLS key file (default /root/.docker/key.pem)  --tlsverify  Use TLS and verify the remote -v, --version  Print version information and quit Management Commands: checkpoint Manage checkpoints container Manage containers image  Manage images network  Manage networks node  Manage Swarm nodes plugin Manage plugins secret Manage Docker secrets service  Manage services stack  Manage Docker stacks swarm  Manage Swarm system Manage Docker volume Manage volumes Commands: attach Attach to a running container build  Build an image from a Dockerfile commit Create a new image from a container's changes cp  Copy files/folders between a container and the local filesystem create Create a new container deploy Deploy a new stack or update an existing stack diff  Inspect changes on a container's filesystem events Get real time events from the server exec  Run a command in a running container export Export a container's filesystem as a tar archive history  Show the history of an image images List images import Import the contents from a tarball to create a filesystem image info  Display system-wide information inspect  Return low-level information on Docker objects kill  Kill one or more running containers load  Load an image from a tar archive or STDIN login  Log in to a Docker registry logout Log out from a Docker registry logs  Fetch the logs of a container pause  Pause all processes within one or more containers port  List port mappings or a specific mapping for the container ps  List containers pull  Pull an image or a repository from a registry push  Push an image or a repository to a registry rename Rename a container restart  Restart one or more containers rm  Remove one or more containers rmi  Remove one or more images run  Run a command in a new container save  Save one or more images to a tar archive (streamed to STDOUT by default) search Search the Docker Hub for images start  Start one or more stopped containers stats  Display a live stream of container(s) resource usage statistics stop  Stop one or more running containers tag  Create a tag TARGET_IMAGE that refers to SOURCE_IMAGE top  Display the running processes of a container unpause  Unpause all processes within one or more containers update Update configuration of one or more containers version  Show the Docker version information wait  Block until one or more containers stop, then print their exit codes Run 'docker COMMAND --help' for more information on a command. [node1] (local) root@10.0.5.3 ~ $ export DOCKER_HIDE_LEGACY_COMMANDS=test [node1] (local) root@10.0.5.3 ~ $ env | grep DOCKER DOCKER_BUCKET=get.docker.com DOCKER_HIDE_LEGACY_COMMANDS=test DOCKER_SHA256=97892375e756fd29a304bd8cd9ffb256c2e7c8fd759e12a55a6336e15100ad75 DOCKER_STORAGE_DRIVER=overlay2 DOCKER_VERSION=1.13.1 [node1] (local) root@10.0.5.3 ~ $ docker --help Usage: docker COMMAND A self-sufficient runtime for containers Options:  --config string Location of client config files (default /root/.docker) -D, --debug   Enable debug mode  --help   Print usage -H, --host list  Daemon socket(s) to connect to (default []) -l, --log-level string Set the logging level (debug, info, warn, error, fatal) (default in fo)  --tls   Use TLS; implied by --tlsverify  --tlscacert string Trust certs signed only by this CA (default /root/.docker/ca.pem)  --tlscert string  Path to TLS certificate file (default /root/.docker/cert.pem)  --tlskey string Path to TLS key file (default /root/.docker/key.pem)  --tlsverify  Use TLS and verify the remote -v, --version  Print version information and quit Management Commands: checkpoint Manage checkpoints container Manage containers image  Manage images network  Manage networks node  Manage Swarm nodes plugin Manage plugins secret Manage Docker secrets service  Manage services stack  Manage Docker stacks swarm  Manage Swarm system Manage Docker volume Manage volumes Commands: build  Build an image from a Dockerfile deploy Deploy a new stack or update an existing stack login  Log in to a Docker registry logout Log out from a Docker registry run  Run a command in a new container search Search the Docker Hub for images version  Show the Docker version information Run 'docker COMMAND --help' for more information on a command. ",
      "url"      : "https://y2p.cc/2017/02/24/docker-env/",
      "keywords" : ""
    } ,
  
    {
      "title"    : "OPNFV简介",
      "category" : "NFV",
      "content": "OPNFV项目与它关联的众多上游项目的关系 opnfv 大背景 目标 通过构建一个运营商级的平台来推动NFV的演进，重点是确保多个开源组件的互操作性、一致性和高性能。在这方面，OPNFV与多个项目持续集成并测试推动技术发展。 最初的目标 组装一套最小的可供实际部署的基础设施，包括软件操作系统（操作系统Nova，KVM），存储（OpenStack Glance和Cinder），网络（OpenDaylight，OVS和ONOS），基础设施（RabbitMQ，PaceMaker，MySQL），操作（OpenStack Horizon，Keystone，Heat），测试（OpenStack Tempest，Robot，Rally）。 最初的重点 虚拟化基础设施，考虑到OpenStack作为虚拟基础设施的管理，下图描述了最初OPNFV的关注点： OPNFV用法 上图总结了OPNFV的用法和相关过程，用户可以自己添加新的开源组件来增强功能，适应新的用例或者运行自定义虚拟网络函数参考平台。 OPNFV安装过程可以分为两个阶段，第一阶段是建立虚拟基础设施管理器（VIM）。第二阶段是OPNFV具体安装和维护，如常见的配置、系统测试等。 OPNFV项目 OPNFV项目可以有很多种：（一）需求项目（二）开发项目（三）集成和测试项目（四）文档。 需求项目关注的是识别和解决与上游项目如OpenStack，OpenDaylight等的差距，这些项目是由需求决定用例，确保上游组件能够满足运营商级NFV部署需求，因此这些项目通常包括用例的输出，差距分析，需求、架构和实施计划。 OPNFV项目与它关联的众多上游项目的关系 协作开发 OPNFV的协作开发项目旨在将源码与现有开源/持续的开源项目相结合，接下来提供这些项目的简要概述。 1、 开放网络操作系统架构（ONOSFW） ONOS SDN控制器是由一堆代码实现的与OpenStack集成，从而实现控制器性能增强，包括ML2，L3，服务插件和驱动。 2、 OpenVSwitch (OVS) for NFV: 该协作开发计划是OvS项目在软件加速用户空间中改善OvS的性能，增强电信级NFV部署的适用性。 3、OpenContrail 该项目将使OpenContrail成为OPNFV部署虚拟网络技术的选择，该项目的主要任务是将OpenContrail构件集成到OPNFV基础设施，并确保在各种安装项目上的支持。 4、Moon Security 这个项目提出了一种Moon安全管理系统来满足用户的安全需求，它将安全管理通过各种机制如授权访问控制、网络防火墙、存储隔离、易处理日志记录等执行。 5、Fast Datapath 软件快速路径服务质量指标项目专注于公共事业和图书馆的发展，通过NFV技术设施（NFVI）具有低延迟、高绩效包处理路径等优点，VNFs可以测量电信流量和性能关键指标（kpi），检测和报告违规行为。具有衡量数据平面KPIs流量质量的能力，不管数据平面是基于硬件或软件实现的。 6、Kernel-based Virtual Machine (KVM) NFV监控在NFVI中提供了重要功能，现有的虚拟机监控程序设计的初衷并不一定能满足NFVI的要求，需要根据NFV的特性补充后期的工作。 7、OpenDaylight服务功能链（ODL SFC） 服务功能链（SFC）能够定义网络服务列表（如：防火墙、NAT、QoS），这些服务组合在一起形成一个服务的网络链，该项目提供了在NFV环境实施项目中的ODL SFC基础设施。 集成测试 OPNFV包括系统集成（CI），测试项目包括功能测试和性能测试： 1、Octopus OPNFV和许多开源项目合作，通常情况下，这些项目是独立开发和测试的，这些项目的用例不会覆盖NFV-specific用例，因此，这些项目的集成可能揭露出一些缺陷，Octopus第一个旨在整合这些开源项目。 2、BootStrap/Get Started 这个项目组装并测试OPNFV基础设施组件最小的能运行VNFs的组合，基本上该项目提供的解决方案，能使现有的安装程序自动安装和配置所需的组件和配置工具，并执行一组基本的系统测试，它以Linux系统中安装的Ubuntu 14.04/Cento 7操作系统为基础。 3、VSPerf 该项目旨在描述虚拟交换机（VSwitch）的性能，VSPerf致力于开发VSwitch测试架构和所有相关的测试，这有助于验证使用VSwitch的NFV电信级部署。 4、Functest 被用于测试用例中设置测试工具，包括性能测试，并将测试用例整合到CI。 5、Yardstick 这个项目将从VNF角度验证NFVI，它提供了两个解决系统功能和性能测试的用例。 6、Q-Tip OPNFV性能基准测试套件，旨在描述平台中的裸机组件。 7、IPV6 这个项目将在OPNFV平台中创建一个启用IPv6的meta-distribution，并促进IPv6 OPNFV的演进。IPv6项目的可交付成果是自动化脚本、安装、用户指南、差距分析和建议。 8、Pharos 该项目侧重于建立NFV测试能力。这个测试设备由OPNFV社区不同公司提供，同时考虑全世界各学科和技术环境，努力创建一个规范的OPNFV-compliant测试环境,以及工具、流程和文档，它是一个可以用来支持集成、测试、协作开发的项目。 潜在的需求/开发项目 该项目需要系统可视化，这些是管理员、应用程序开发人员和网络管理的要求，在当前NFV部署中，对服务、基础设施管理、虚拟网络拓扑、虚拟和物理网络元素、流表、统计数据和配置，各级的工具在堆栈上提供可视化功能。 在某些用例中，这些工具缺乏一些重要特性，看着流表进行故障排除极具挑战性，我认为有必要简化可视化、发展基础元素、可视化构建模块问题。 因此，第一幅图将指定一组理想的可视化工具，要求考虑不同方面的可视化NFV部署，并启动与上游项目协作开发，以实现这些需求。 ",
      "url"      : "https://y2p.cc/2017/02/27/opnfv/",
      "keywords" : ""
    } ,
  
    {
      "title"    : "AWESOME",
      "category" : "Tool",
      "content": "  Git  Book    Pro Git   Visual Git Guide   Successful Git Branching Model   Everyday GIT With 20 Commands Or So   learnGitBranching     Github  Book    GotGitHub   Visual Github Guide   Github Help   Octotree     Course    Try Github   Gitimmersion     Github Stackalytics    Linux Foundation Analytics     Kubernetes  Kubernetes Course  Kubernetes PlayGround  Kubernetes Handbook CN  Kops: Kubernetes Operations - Production Grade K8s Installation, Upgrades, and Management  Docs kubernetes.org.cn  Krew: the plugin manager for kubectl command-line tool    kubectl tree     Docker  Dockerfile Lint  Docker PlayGround  Docker Course  Docker Swarm Example   Markdown  Markdown Lint  Edit    Stackedit   Zybuluo   Typora     Golang  Go Lint  Check Deadlocks   License  Choose License  Scan License Tool    Fossology Docker Image     Creative Commons    “知识共享”（CC协议）简单介绍     Version  Semantic Version   Html  HTMLHint  Code Quality Tool   IDE  VISUAL STUDIO CODE    使用VS CODE+PlantUML高效画图     Plantuml Help         IDEA   Container  Container Ecosystem   color  color-wheel   Awesome Cloud Native Github Awesome ",
      "url"      : "https://y2p.cc/2017/03/01/awesome/",
      "keywords" : ""
    } ,
  
    {
      "title"    : "NFV FAQ",
      "category" : "NFV",
      "content": "NFV 常见问题解答 NFV FAQ Q1：SDN 和 NFV 的作用是什么？ 术语 NFV 和 SDN 经常被联合使用，这可能给人一种印象，即 SDN 是 NFV 体系结构中的一个基本要素。 SDN 是一个重要的概念和衍生技术，极大地提高了 NFV 基础网络的控制和管理。 相反，SDN 架构的功能模块可以以 NFV 基础架构中承载的虚拟化网络功能的形式进行部署。 但是，SDN 并不是实现 NFV 的强制性技术。 2012年10月的第一届NFV白皮书： “NFV 与软件定义网络（SDN）高度互补，但不依赖于它（反之亦然）。 NFV 可以在不需要 SDN 的情况下实现，尽管这两个概念和解决方案可以结合起来并可能获得更大的价值“ 更多内容可以参考十张图看懂 SDN 与 NFV 的区别与联系 Q2：NFVO 与 VNFM 有什么区别？ NFVO 和 VNFM 都是 NFV-MANO 中管理和编排架构框架的功能块。 VNFM 负责 VNF（s） 的生命周期管理 （LCM） 。这包括支持界面操作和工作流程的执行：  实例化 VNF 。 扩缩容 VNF（增加或减少 VNF 的容量）。 修复 VNF（修复有故障的 VNF 和/或其 VNFC 实例和内部 VNF 虚拟链路的纠正措施） 更改 VNF 风格（更改已部署的 VNF 的风格，即部署选项） 操作 VNF（更改 VNF 实例的状态） 修改 VNF 信息（支持 VNF 配置和信息更改）。 更改外部 VNF 连接（更改 VNF 实例的外部连接） 终止 VNF （释放与 VNF 相关联的 NFVI 资源并将其返回给 NFVI 资源池）。 VNFM 还负责 VNF 性能，故障和配置管理。 另一方面，NFVO 负责管理网络服务（NS）生命周期，并结合 VNF 生命周期（由 VNFM 支持）和协调NFIM资源（由 VIM 支持）来确保优化分配必要的资源和连接。 因此，NFVO 功能可以分为两大类：端到端资源编排和网络服务编排。 NFVO 还负责 NS 性能和故障管理以及VNF包管理。 与 VNF LCM 类似，NS 生命周期与许多受支持的操作和执行工作流相关，例如用于实例化，缩放，修复和终止 NS 的工作流，以及不同类型的更新，例如添加/删除 VNF 来自/来自 NS 的实例等等。 Q3：VNF 部署的最佳做法是基于虚拟机还是容器？ 目前大多数电信公司的 NFV 部署都是基于虚拟机，目前适合市场上提供的大部分 VNF 实施。 然而，最近的实现开始引入基于容器的 VNF 。 由于云原生 VNF 的诉求引入了行业，因此需要更多地考虑使用基于容器的虚拟化了。 Q4：VNFM 和 EM 有什么区别？ 一方面，从资源的角度来看，VNFM 负责 VNF 的生命周期管理，如实例化，缩放等（同样参考 Q2）。 另一方面，EM 负责驻留在 VNF 中的实际网络应用的 FCAPS （故障，配置，计费，性能和安全）管理。 Q5: 水平和垂直缩放有什么区别？ 缩放可以分为两类：水平缩放和垂直缩放。 以 VNF 为例，水平缩放可以分为两类 Scale out: 涉及增加容量，它是指将一个或多个 VNFC 实例添加到现有应用程序的过程。 Scale in: 涉及容量的减少，它指的是从现有应用程序中删除一个或多个 VNFC 实例的过程。 以 VNF 为例，垂直缩放可以分为两类 Scale up: 涉及增加容量，它是指将资源（例如 CPU /内存）添加到现有 VNFC 实例的过程。 Scale down: 涉及容量的减少，它是指从现有的 VNFC 实例释放资源（例如 CPU /内存）的过程。 每当某个预先配置的策略（例如关于 CPU 利用率）被激活时，或者通过例如由操作员的明确请求，自动触发缩放。 例如，NFV 生态系统中的自动扩展策略可能意味着，如果 CPU 利用率超过70％，系统应该扩容，如果 CPU 利用率低于 20％，系统应缩容。 VNF缩放可以从几个来源触发： NFVO VNFM VNF / EM（元素管理器） OSS / BSS 由操作员手动 Q6: 包处理解决方案在 NFV 中的实施 许多包处理解决方案（如 DPDK 功能）在专门的硬件中工作。 VNF 实施者如何确保商品硬件的性能也同样出色？ DPDK 需要兼容的硬件，即兼容的网络接口卡 （NIC） 和 CPU 型号。但是，与其他一些加速技术不同，DPDK 不需要专门的处理器。在后一种情况下，可以将专用处理器添加到商品服务器（例如，带有嵌入式处理器的 SmartNIC ），以便将来自主通用处理器的流量处理的一部分卸载到这些专用处理器。 此外，尽管最初是为 x86 生态系统设计的，但 DPDK 并不是特定于供应商的功能，而最近的版本与特定的硬件无关。如今，DPDK 是一个开源社区驱动的软件解决方案（软件库），用于增强数据包处理。 DPDK 已经发展成为一个庞大的生态系统，支持许多不同的 NIC 卡和处理器，因此不受特定硬件或供应商解决方案的约束。 尽管 DPDK 可用于各种平台，但要保证性能，还需要 VNF 实施人员和基础设施提供商联合实验室/测试工作。 Q7: 我们可以在一个单一的实体中有 EM 和 VNFM 吗？ EM 和 VNFM 是提供 NFV 体系结构框架中指定的不同功能的功能块。 然而，与许多功能体系结构一样，实际的实现可以将两个功能包含在一个单独的实体中。 Q8: 如何评估运营商在NFV和开源计划中的贡献？ 2012年10月发布的第一份 NFV 白皮书是一级运营商（参考网络功能虚拟化 - 介绍性白皮书）合作的结果。 从那时起，生态系统已经扩展到 ETSI NFV ISG 中的300多家公司/组织，包括39家网络运营商。 NFV ISG 是运营商主导的行业倡议，旨在提高认识，并培养所有行业利益相关者在 NFV 愿景之下提供可互操作的解决方案。 运营商的贡献已经扩展到直接参与 OpenStack 等上游社区和 OPNFV 等中游社区。 除了所有 ETSI NFV 和其他 SDO 所做的标准化工作外，运营商还开发了其他市场领先的开放源码举措，如 OSM 和 ONAP。 Q9: Hypervisors 在 NFV 中的确切作用是什么？ Hypervisors 是一种软件，它将底层的硬件物理资源（CPU，内存，存储，网卡）进行分区，生成虚拟机，并将分配的资源分配给它们。 另一方面，虚拟机通常作为具有物理服务器（CPU，内存，存储，NIC）所需要的所有要素。 当虚拟化计算资源以虚拟机的形式交付时，Hypervisors 在 NFV 中的角色是 NFVI 的“虚拟化层”实现者。 Q10: 旧的硬件/基础设施如何重复使用？ 运营商是否可以重新使用即将结束的现有硬件/基础设施作为NFVI的一部分，例如即将退役的2G基础设施？ 大多数旧的电信平台是专用硬件，不能与其应用软件分离使用，这实际上是 NFV 背后的主要概念和驱动力之一。 具体的硬件是否可以重复使用可能取决于具体情况，需要经过运营商和设备供应商的分析。 Q11: NFV 引入的时机有什么考量？ 为什么现在才引入 NFV for Mobile Core（LTE / EPC），而不是在 CDMA PS 核心（PDSN）和其他 NF BSC / RAN 之前就引入？ 要进行虚拟化的网络功能的选择取决于运营商或供应商的许多标准。 影响网络功能虚拟化的因素包括：虚拟化平台上运行的网络功能软件的准备情况，市场需求，现有硬件的使用寿命等。 其他因素之一是网络功能目标本身。 例如，如果这是一个繁重的代码转换工作量，这种功能的虚拟化可能会影响网络功能的性能，从而延迟其虚拟化，直到虚拟化基础架构能够提供所需的性能。 Q12: Ve-Vnfm-em 和 Ve-Vnfm-vnf 是否具有相同类型的功能？ 在标准中，有两个参考点，一个在 VNFM 和 VNF 之间，另一个在 VNFM 和 EM 之间。 两个参考点是否具有相同类型的功能？ Ve-Vnfm-em 参考点用于 EM 和 VNF Manager 之间的交换。 Ve-Vnfm-vnf 参考点用于 VNF 和 VNF Manager 之间的交换。 它们共享一些共同的功能，例如 VNF Indicator 接口，这是由 VNF 或 EM 提供的用于标识 VNF 行为的一些指示的信息。 VNFM 可以将这些指标与虚拟资源数据结合使用来执行自动缩放决策。 此外，两个参考点都共享由 VNFM 生成并由 EM 和/或 VNF 使用的 VNF 生命周期管理界面的大部分操作。 其中一个不同之处就是仅在 Ve-Vnfm-vnf 上可用的VNF配置界面。 Ve-Vnfm-vnf 参考点上的 VNF Configuration 界面支持设置或更新 VNF / VNFC 实例的虚拟化相关配置参数。 Q13: 多供应商？ 我们是否可以在真正的部署中拥有独立的多供应商 VIM，VNF Manager 和 NFV Orchestrator？ 是的，这样的多厂商设置是可能的。 在这种情况下，系统集成工作需要考虑 VIM，VNFM 和 NFVO 之间的所有三个参考点。 ETSI NFV 指定的 REST API 旨在实现多供应商解决方案。 见：One more step towards NFV MANO interoperability Q14: VNF Manager 是一种云管理平台吗？ VNF Manager 是主要负责 VNF 生命周期管理以及某些配置管理功能的实体。 在 NFV 的背景下，云管理平台由 VIM （虚拟化基础架构管理）来表示。 Q15: NFVO 和 VNFM 可以组合在同一个实体中吗？ NFVO 和 VNFM 是提供 NFV 体系结构框架中指定的不同功能的功能模块。 然而，与许多功能体系结构一样，实际的实现可以将两个功能包含在一个单独的实体中。 Q16: NFV 是否只专注于虚拟化电信网络功能？ 任何具有相同特征和 NFV 要求的企业或环境（例如电信级性能，高可用性，类似电信公司的工作负载）都可以利用 NFV 的潜力和 ETSI NFV 所定义的框架。 Q17:为什么我们需要使用 SDN，如果 NFVO 可以执行其功能？ 这是不正确的。 这两个实体有不同的角色。 NFVO 主要负责资源编排和网络服务编排，而 NFVI PoP（又名 DC SDN）中的 SDN 通常负责 DC 网络。 说到网络服务的网络部分， VNF 间路径或链路被定义为网络服务描述符的一部分并由 NFVO 处理。 这些信息用于派生发送给 VIM 的请求的内容，而这些请求又可以将基础设施路由器和交换机的配置委托给 SDN。 所以，我们可以说这个定义来自于 NFVO，配置代理/主机是由 VIM / SDN 完成的。 Q18: 从性能角度来看，除了监控应用层 KPI 还必须监控云层的性能吗？ 从 FM / PM 的角度来看，运营商需要对整个 NFV 的垂直堆栈进行 E2E 监测。 因此，从 EM 获取应用计数器和告警是不够的。 软/硬件故障和计数器需要通过引入相关的基础设施监控工具进行监控。 拥有正确的工具集将使运营商能够为网络服务和VNF提供高效的根源分析（RCA）引擎。 与虚拟资源有关的故障和性能报告应由 NFVI 报告给 VIM，由 VNFM 和 NFVO 传播/关联到 OSS 和/或 EM 或 VNF 本身。 NFVO 和 VNFM 也可能会对某些事件立即采取行动。 Q19: NFV-MANO 是否真的不考虑物理网络功能？ 是的。NFVO 可以管理 VNF 和 PNF 结合的混合网络服务。 但是，NFV-MANO 不管理 PNF 资源的生命周期。 从 NFVO 的角度来看，PNF 是暴露多个连接点的黑盒实体。 Q20: NS 是否可以嵌套，但只允许一层嵌套呢？ 不是的。嵌套层数没有理论上的限制。 尽管实际上这个数字预计很低。 Q21: 异体系结构使用 NFV API？ ETSI 在 NFV 架构框架的某些参考点指定了 REST API，实现了多供应商互操作性。 如果我的体系结构不同于 NFV 体系结构框架，我可以使用这些 API 吗？ 这一切都取决于差异有多重要。 例如，如果 NFVO 和 VNFM 之间的功能划分不符合 ETSI 标准，或者如果两个功能块被包含在一个单一的内容中，则很难使用 ETSI NFV-SOL 003 中为 or-Vnfm 参考点指定的 API 实体。 但是，这并不妨碍使用 ETSI GS NFV-SOL 002 中指定的 Ve-Vnfm 参考点的 API。 此外，ETSI GS 中指定的 API 是彼此独立的。 例如，ETSI GS NFV-SOL 005 中指定的不同 API 可以由不同的 OSS 组件使用。 Q22: PNF 的可靠性与 VNF 的弹性不同吗？ 这两个术语涵盖了相同的概念，即在故障，中断正常操作的事件的情况下，能够限制中断并恢复到正常或在可接受的最低服务交付水平。 参考资料  ETSI_NFV_FAQ ",
      "url"      : "https://y2p.cc/2017/03/03/nfv-faq/",
      "keywords" : ""
    } ,
  
    {
      "title"    : "OPEN SOURCE MANO",
      "category" : "NFV",
      "content": "OSM简介 OSM简介 OSM（开源MANO）是由ETSI托管的一个项目，用于开发一个开源的NFV MANO软件堆栈，这与其提出的架构相一致。 该项目首次在2016年移动世界大会上作为运营商用例展示。 有趣的是，OSM同时使用了两个项目——OpenMANO和RIFT.io——以及OpenStack和Ubuntu JuJu。 考虑到这些项目的重用，OSM得到电信公司（如Telefónica of Spain ，英国电信，奥地利电信，韩国电信和Telenor）的支持，以及英特尔，Mirantis，RIFT.io，博科，戴尔，RADware等设备商的支持。 OSM Mapping to ETSI NFV MANO 借鉴OpenMANO OpenMANO是Telefonica发布的一个项目，由VIM（OpenVIM）、VNF管理器和Orchestrator组成，如下图所示 该图还强调了OpenMANO也可以在其架构中与OpenStack一起作为VIM。OpenVIM是NFV VIM的参考实现。 它非常类似于OpenStack，与NFV基础设施中的计算节点以及提供计算和网络功能以及部署虚拟机的OpenFlow控制器接口。 它提供了一个基于REST的北向接口。 OpenVIM是EPA(早期访问计划)感知的，包括功能支持，如CPU和NUMA(Non-Uniform Memory Access)固定，PCI(Peripheral Component Interconnect)传输等。 截至今天，OpenMANO是一个非常基本的实现，不适合商业部署。 ",
      "url"      : "https://y2p.cc/2017/03/03/open-source-mano/",
      "keywords" : ""
    } ,
  
    {
      "title"    : "OpenStack &amp; Container",
      "category" : "OpenStack",
      "content": "[WIP] 关注的项目 OpenStack Containerization 目前，容器技术的火爆使沉默已久的PaaS焕发了青春，各种概念曾出不穷，比如CaaS(Container-as-a-Service)，ePaaS(elastic PaaS)，其核心思想就是通过通过Container打包应用。仔细想想，过去几年PaaS一直不瘟不火，或者直白一点在IaaS面前显得那么鸡肋，总结起来主要有以下几点原因： 严重依赖IaaS层的实现，比如需要IaaS层提供服务编排和弹性扩展的功能; 处于SaaS和IaaS的夹缝中，很多功能SaaS层和IaaS实现起来更方便; 没有统一的对外接口，各种平台林立(OpenShift，CloudFundry，GAE，SAE)，接口不统一; 应用上PaaS难，没有规范的应用发布标准。 Container的出现，彻底解决了1和2两个问题，而且提供了更好的用户体验(部署快，性能好)，并且能够反过来蚕食SaaS和IaaS的空间(比如CaaS的概念) OpenStack没有坐以待毙，而是与容器拥抱，创建了以下的一些项目  Zun wiki，github   Magnum wiki，github   Heat wiki，github   Murano wiki，github 其中Nova-Docker为初次尝试，通过它来支持 Docker 容器，实质是将 Docker 容器作为虚机来管理，但在实际使用中，会发现有不少的问题。毕竟，Nova 设计的初衷是管理虚拟机，而容器跟虚拟机在行为和特性上存在较大的不同，无论是管理层还是底层的虚拟化支持层都完全不同。而且，让 Nova 支持各种各样的容器机制（Docker、OpenVZ、Rocket、LXC 等）要进行修改的地方着实不少，可能跟现有框架形成冲突。 Nova-Docker已经不再维护，取而代之的是Zun项目。 Zun和Nova-Docker的区别是    Zun  Nova-Docker     新增API  复用Nova的API   Zun和Magnum的区别    Zun  Magnum     提供APIs管理容器  提供APIs管理容器编排引擎，例如kubernetes    将容器作为OpenStack的资源管理       可以与OpenStackde其他组件集成，例如Neutron network和Cinder volume       为用户提供简单的APIs管理容器，屏蔽了各种容器技术之间的差异      ",
      "url"      : "https://y2p.cc/2017/03/14/openstack-container/",
      "keywords" : ""
    } ,
  
    {
      "title"    : "DOCKERD CLI 代码解析",
      "category" : "Docker",
      "content": "DOCKERD CLI 代码解析 cobra示例 docker将客户端程序和服务器程序分为docker和dockerd，两者的命令行解析都采用第三方库：cobra 首先用个小程序介绍一下cobra的简单用法： 基本用法是这四步： 主命令 子命令 添加选项 执行命令 dockerd职责 docker接受并解析客户端的操作指令，然后访问dockerd相应的url，让dockerd做实际的处理。 dockerd实际上可简化为一个web后台，提供各种web接口API让docker访问。 dockerd cli 代码解析 dcokerd的入口函数在文件docker/cmd/dockerd/docker.go中 从main函数开始 func main() {  if reexec.Init() {  return  } // Set terminal emulation based on platform as required.  _, stdout, stderr := term.StdStreams()  logrus.SetOutput(stderr) cmd := newDaemonCommand()  cmd.SetOutput(stdout)  if err := cmd.Execute(); err != nil {  fmt.Fprintf(stderr, %s  , err)  os.Exit(1)  } } newDaemonCommand继续解析 func newDaemonCommand() *cobra.Command {  opts := daemonOptions{  daemonConfig: config.New(),  common:  cliflags.NewCommonOptions(),  } // 主命令  cmd := &amp;cobra.Command{  Use:   dockerd [OPTIONS],  Short:  A self-sufficient runtime for containers.,  SilenceUsage: true,  SilenceErrors: true,  Args:  cli.NoArgs,  RunE: func(cmd *cobra.Command, args []string) error {   opts.flags = cmd.Flags()   // 主命令回调函数   return runDaemon(opts)  },  }  cli.SetupRootCommand(cmd) flags := cmd.Flags()  flags.BoolVarP(&amp;opts.version, version, v, false, Print version information and quit) // 默认配置文件，unix、linux为/etc/docker/daemon.json，windows和Solaris为  flags.StringVar(&amp;opts.configFile, flagDaemonConfigFile, defaultDaemonConfigFile, Daemon configuration file) // 通过install，从flag获取选项值，并将相应值传到各个模块  // cli界面上的呈现的命令都能在以下函数中找到  opts.common.InstallFlags(flags)  installConfigFlags(opts.daemonConfig, flags) // 试验用  installServiceFlags(flags) return cmd } 命令解析完就会执行函数runDaemon func runDaemon(opts daemonOptions) error {  if opts.version {  showVersion()  return nil  } daemonCli := NewDaemonCli() // Windows specific settings as these are not defaulted.  if runtime.GOOS == windows {  if opts.daemonConfig.Pidfile == {   opts.daemonConfig.Pidfile = filepath.Join(opts.daemonConfig.Root, docker.pid)  }  if opts.configFile == {   opts.configFile = filepath.Join(opts.daemonConfig.Root, `config  daemon.json`)  }  } // On Windows, this may be launching as a service or with an option to  // register the service.  // unix、linux下为空函数  stop, err := initService(daemonCli)  if err != nil {  logrus.Fatal(err)  } if stop {  return nil  } // 启动一个server  err = daemonCli.start(opts)  notifyShutdown(err)  return err } 启动一个server func (cli *DaemonCli) start(opts daemonOptions) (err error) {  stopc := make(chan bool)  defer close(stopc) // warn from uuid package when running the daemon  uuid.Loggerf = logrus.Warnf opts.common.SetDefaultOptions(opts.flags) if cli.Config, err = loadDaemonCliConfig(opts); err != nil {  return err  }  cli.configFile = &amp;opts.configFile  cli.flags = opts.flags if opts.common.TrustKey == {  opts.common.TrustKey = filepath.Join(   getDaemonConfDir(cli.Config.Root),   cliflags.DefaultTrustKeyFile)  } if cli.Config.Debug {  debug.Enable()  } if cli.Config.Experimental {  logrus.Warn(Running experimental build)  } logrus.SetFormatter(&amp;logrus.TextFormatter{  TimestampFormat: jsonlog.RFC3339NanoFixed,  DisableColors: cli.Config.RawLogs,  }) if err := setDefaultUmask(); err != nil {  return fmt.Errorf(Failed to set umask: %v, err)  } if len(cli.LogConfig.Config) &gt; 0 {  if err := logger.ValidateLogOpts(cli.LogConfig.Type, cli.LogConfig.Config); err != nil {   return fmt.Errorf(Failed to set log opts: %v, err)  }  } // Create the daemon root before we create ANY other files (PID, or migrate keys)  // to ensure the appropriate ACL is set (particularly relevant on Windows)  if err := daemon.CreateDaemonRoot(cli.Config); err != nil {  return err  } if cli.Pidfile != {  pf, err := pidfile.New(cli.Pidfile)  if err != nil {   return fmt.Errorf(Error starting daemon: %v, err)  }  defer func() {   if err := pf.Remove(); err != nil {    logrus.Error(err)   }  }()  } serverConfig := &amp;apiserver.Config{  Logging:  true,  SocketGroup: cli.Config.SocketGroup,  Version:  dockerversion.Version,  EnableCors: cli.Config.EnableCors,  CorsHeaders: cli.Config.CorsHeaders,  } if cli.Config.TLS {  tlsOptions := tlsconfig.Options{   CAFile: cli.Config.CommonTLSOptions.CAFile,   CertFile: cli.Config.CommonTLSOptions.CertFile,   KeyFile: cli.Config.CommonTLSOptions.KeyFile,  }  if cli.Config.TLSVerify {   // server requires and verifies client's certificate   tlsOptions.ClientAuth = tls.RequireAndVerifyClientCert  }  tlsConfig, err := tlsconfig.Server(tlsOptions)  if err != nil {   return err  }  serverConfig.TLSConfig = tlsConfig  } if len(cli.Config.Hosts) == 0 {  cli.Config.Hosts = make([]string, 1)  } api := apiserver.New(serverConfig)  cli.api = api for i := 0; i &lt; len(cli.Config.Hosts); i++ {  var err error  if cli.Config.Hosts[i], err = dopts.ParseHost(cli.Config.TLS, cli.Config.Hosts[i]); err != nil {   return fmt.Errorf(error parsing -H %s : %v, cli.Config.Hosts[i], err)  }  protoAddr := cli.Config.Hosts[i]  protoAddrParts := strings.SplitN(protoAddr, ://, 2)  if len(protoAddrParts) != 2 {   return fmt.Errorf(bad format %s, expected PROTO://ADDR, protoAddr)  }  proto := protoAddrParts[0]  addr := protoAddrParts[1]  // It's a bad idea to bind to TCP without tlsverify.  if proto == tcp &amp;&amp; (serverConfig.TLSConfig == nil || serverConfig.TLSConfig.ClientAuth != tls.RequireAndVerifyClientCert) {   logrus.Warn([!] DON'T BIND ON ANY IP ADDRESS WITHOUT setting --tlsverify IF YOU DON'T KNOW WHAT YOU'RE DOING [!])  }  ls, err := listeners.Init(proto, addr, serverConfig.SocketGroup, serverConfig.TLSConfig)  if err != nil {   return err  }  ls = wrapListeners(proto, ls)  // If we're binding to a TCP port, make sure that a container doesn't try to use it.  if proto == tcp {   if err := allocateDaemonPort(addr); err != nil {    return err   }  }  logrus.Debugf(Listener created for HTTP on %s (%s), proto, addr)  api.Accept(addr, ls...)  } if err := migrateKey(cli.Config); err != nil {  return err  } // FIXME: why is this down here instead of with the other TrustKey logic above?  cli.TrustKeyPath = opts.common.TrustKey registryService := registry.NewService(cli.Config.ServiceOptions)  containerdRemote, err := libcontainerd.New(cli.getLibcontainerdRoot(), cli.getPlatformRemoteOptions()...)  if err != nil {  return err  }  signal.Trap(func() {  cli.stop()  &lt;-stopc // wait for daemonCli.start() to return  }) // Notify that the API is active, but before daemon is set up.  preNotifySystem() d, err := daemon.NewDaemon(cli.Config, registryService, containerdRemote)  if err != nil {  return fmt.Errorf(Error starting daemon: %v, err)  } if cli.Config.MetricsAddress != {  if !d.HasExperimental() {   return fmt.Errorf(metrics-addr is only supported when experimental is enabled)  }  if err := startMetricsServer(cli.Config.MetricsAddress); err != nil {   return err  }  } name, _ := os.Hostname() c, err := cluster.New(cluster.Config{  Root:    cli.Config.Root,  Name:    name,  Backend:   d,  NetworkSubnetsProvider: d,  DefaultAdvertiseAddr: cli.Config.SwarmDefaultAdvertiseAddr,  RuntimeRoot:  cli.getSwarmRunRoot(),  })  if err != nil {  logrus.Fatalf(Error creating cluster component: %v, err)  } // Restart all autostart containers which has a swarm endpoint  // and is not yet running now that we have successfully  // initialized the cluster.  d.RestartSwarmContainers() logrus.Info(Daemon has completed initialization) logrus.WithFields(logrus.Fields{  version:  dockerversion.Version,  commit: dockerversion.GitCommit,  graphdriver: d.GraphDriverName(),  }).Info(Docker daemon) cli.d = d // initMiddlewares needs cli.d to be populated. Dont change this init order.  if err := cli.initMiddlewares(api, serverConfig); err != nil {  logrus.Fatalf(Error creating middlewares: %v, err)  }  d.SetCluster(c)  initRouter(api, d, c) cli.setupConfigReloadTrap() // The serve API routine never exits unless an error occurs  // We need to start it as a goroutine and wait on it so  // daemon doesn't exit  serveAPIWait := make(chan error)  go api.Wait(serveAPIWait) // after the daemon is done setting up we can notify systemd api  notifySystem() // Daemon is fully initialized and handling API traffic  // Wait for serve API to complete  errAPI := &lt;-serveAPIWait  c.Cleanup()  shutdownDaemon(d)  containerdRemote.Cleanup()  if errAPI != nil {  return fmt.Errorf(Shutting down due to ServeAPI error: %v, errAPI)  } return nil } ",
      "url"      : "https://y2p.cc/2017/03/16/dockerd-cli/",
      "keywords" : ""
    } ,
  
    {
      "title"    : "Kubernetes的基本概念",
      "category" : "Kubernetes",
      "content": "Kubernetes 的基本概念 设计理念 用户定义应用程序的规格，Kubernetes 负责按照定义的规格部署并运行应用程序。 Kubernetes 特点  可移植: 支持公有云，私有云，混合云，多重云 可扩展: 模块化, 插件化, 可挂载 自动化: 自动部署，自动重启，自动伸缩 Pod Pod基本特性 Pod是由一个或多个业务紧耦合的容器组成的容器组合，目前支持Docker容器技术，CoreOS的rkt等容器技术，并很容易扩展支持更多容器技术。 每个Pod都有一个被称为根容器的特殊容器：Pause容器，它对应的镜像属于Kubernetes平台的一部分。 为什么Kubernetes会设计出一个根容器？ 原因之一：在一组容器作为一个单元的情况下，我们很难对整体简单地进行判断并有效地采取行动。比如一个容器死亡了，此时算是整体死亡吗？是N/M的死亡率吗？ 引入业务无关且不容易死亡的Pause容器，以它的状态代表整个容器的状态，就简单、巧妙地解决了这个难题。 原因之二：Pod里的多个业务容器共享Pause容器的IP，共享Pause容器挂架的Volume，这样既简化了密切关联的业务容器之间的通信问题，也很好地解决了它们之间的文件共享问题。 Pod中的所用容器会被一致调度、同节点部署，并且在一个共享环境中运行。这里的共享环境包括以下几点：  所有容器共享一个IP地址，即Pause容器的IP地址，以及端口空间，意味着容器之间可以通过localhost高效访问，不能有端口冲突   允许容器之间共享存储卷，即Pause容器挂接的Volume，通过文件系统交互信息   容器之间可以通过IPC（inter-processcommunication)进行通信  Pod有两种类型：普通Pod和Static Pod，后者比较特殊，它的信息并不保存在etcd中，而是存放在某个具体的Node上的一个具体文件中，并且只在此Node上启动运行。 而普通Pod一旦被创建，其信息会被放到etcd中存储，随后被Kubernetes Master调度到摸个具体的Node上并被绑定，随后该Pod被对应的Node上kubelet进程实例化成一组相关的Docker容器并启动起来。 从生命周期来说，Pod应该是短暂的而不是长久的应用。 Pods被调度到节点，保持在这个节点上直到被销毁。当节点死亡时，分配到这个节点的Pods将会被删掉。在实际使用时，我们一般不直接创建Pods, 我们通过replication controller来负责Pods的创建，复制，监控和销毁。 Pod应用场景 Pod的主要目的还是支持需要一起部署、一起管理的进程，包括：  内容管理系统，文件和数据加载进程，本地cache管理进程等   日志压缩、备份、快照等   数据变化监听、日志和监控适配器，事件分发等   控制、管理、配置、升级程序 实际的使用场景: 业务服务需要收集日志 某服务模块已经实现了一些核心的业务逻辑，并且稳定运行了一段时间，日志记录在了某个目录下，按照不同级别分别为 error.log、warning.log、info.log，现在希望收集这些日志并发送到统一的日志处理服务器上。 这时我们可以修改原来的服务模块，在其中添加日志收集、发送的服务，但这样可能会影响原来服务的配置、部署方式，从而带来不必要的问题和成本，也会增加业务逻辑和基础服务的藕合度。 如果使用Pod的方式，通过简单的编排，既可以保持原有服务逻辑、部署方式不变，又可以增加新的日志收集服务。 而且如果我们对所有服务的日志生成有一个统一的标准，或者仅对日志收集服务稍加修改，就可以将日志收集服务和其他服务进行Pod编排，提供统一、标准的日志收集方式。 这里的核心业务服务、日志收集服务分别是一个Docker镜像，运行在隔离的容器环境中。 提供ssh、ftp访问容器数据的能力 Docker Hub或者很多第三方的镜像并没有安装sshd的服务，不方便我们进入容器进行配置、代码的修改、调试，很多时候需要重新构建镜像、或者在镜像基础上安装sshd的服务，这都需要时间和一定的学习成本。 而通过Pod的方式，我们就可以将现有镜像和一个ssh、ftp镜像进行编排，获得操作容器内数据的能力。 代码自动更新 我们部署了一个node.js的应用，而且部署了几十、上百个节点，那么我希望这个应用可以定时的同步最新的代码，以便自动升级线上环境。 这时，我们当然也不希望改动原来的node.js 应用，可以开发一个Git代码仓库的自动同步服务，然后通过Pod的方式进行编排，并共享代码目录，就可以达到更新node.js应用代码的效果。 并且这个同步服务还可以同其他使用Git代码仓库的服务编排，实现同样的需求。 适配不同IaaS平台的环境 我们开发一个节点管理的agent，这个agent需要读取当前部署环境的一些信息，可以通过底层平台的API实现。 但是，当部署到AWS、阿里云、青云等不同平台时，API就无法统一了。这样，我们可以实现不同平台的适配服务来获取各自的信息，并且和agent通过Pod编排部署，在不改变agent逻辑的情况下，通过服务组合来适配于不同平台。 其实，Kubernetes 的一些新的功能需求，也会建议先通过Pod的编排来解决，而不是直接修改Kubernetes的代码，可见Pod还是用处多多的。 Replication controller 复制控制器确保Pod的一定数量的份数(replica)在运行。如果超过这个数量，控制器会杀死一些，如果少了，控制器会启动一些。控制器也会在节点失效、维护的时候来保证这个数量。所以强烈建议即使我们的份数是1，也要使用复制控制器，而不是直接创建Pod。 它其实定义了一个期望场景，即声明某种Pod的副本数在任意时刻都符合某个预期，所以RC的定义包含了如下几个部分：  Pod期待的副本数(replica)   用于筛选目标Pod的Label Selector   当Pod副本数量小于预期值的时候，用于创建新Pod的模板（template） 由于Replication controller无法准确表达它的本意，在kubernetes 1.2的时候，它升级为一个新的概念：replica set，与RC当前存在的唯一区别是，replica set支持基于集合的Label selector，而RC只支持基于等式的Label selector。 在生命周期上讲，复制控制器自己不会终止，但是跨度不会比Service强。Service能够横跨多个复制控制器管理的Pods。而且在一个Service的生命周期内，复制控制器能被删除和创建。Service和客户端程序是不知道复制控制器的存在的。 复制控制器创建的Pods应该是可以互相替换的和语义上相同的，这个对无状态服务特别合适。 Pod是临时性的对象，被创建和销毁，而且不会恢复。复制器动态地创建和销毁Pod。虽然Pod会分配到IP地址，但是这个IP地址都不是持久的。 Service Service定义了一个Pod的逻辑集合和一个服务的入口访问地址。 集合是通过定义Service时提供的Label Selector完成的。举个例子，我们假定有3个Pod来处理后端业务。这些后端备份逻辑上是相同的，前端不关心哪个后端在给它提供服务。虽然组成这个后端的实际Pod可能变化，前端客户端不会意识到这个变化，也不会跟踪后端。 Service就是用来实现这种分离的抽象。 当在Master节点上创建Service时，Kubernetes会给它分配一个地址，这个地址即Cluster IP，假定为10.0.0.1，此地址从service-cluster-ip-range参数指定的地址池中分配，比如–service-cluster-ip-range=10.0.0.0/16。 假设这个Service的端口是1234。集群内的所有node节点上的kube-proxy都会注意到这个Service。 当kube-proxy发现一个新的service后，它会在本地节点打开一个任意端口，建立相应的iptables规则，重定向Cluster IP和port到这个新建的端口，开始接受到达这个服务的连接。 当一个客户端访问这个service时，这些iptable规则就开始起作用，客户端的流量被重定向到kube-proxy为这个service打开的端口上，kube-proxy随机选择一个后端Pod来服务客户。这个流程如下图所示： 在kubernetes集群内，Node IP、Pod IP、Cluster IP 之间的通信，采用的是kubernetes自己设计的一种特殊的路由规则，与我们熟悉的IP路由有很大不同。 对于每个service，我们通常需要配置一个负载均衡实例来转发到对应的Node上，这的确增加了工作量以及出错率，于是kubernetes提供了自动化的解决方案，如果我们的集群运行在google的GCE公有云上，那么我们只要把service的type=NodePort修改为type=LoadBalance，此时kubernetes会自动创建一个对应的LoadBalance实例并返回它的IP地址供外部客户端使用。 ",
      "url"      : "https://y2p.cc/2017/03/28/kubernetes-concept/",
      "keywords" : ""
    } ,
  
    {
      "title"    : "微服务探讨",
      "category" : "Architecture",
      "content": "微服务探讨 微服务 目的 有效地拆分应用，实现敏捷开发和部署 最终的目标是实现敏捷开发和部署，实现的方式是围绕业务能力有效地拆分应用。 微服务就是从各种角度，包括组织的、技术的等来阐释怎样有效地拆分应用，相对于其他拆分应用的方式，例如六边形架构、12-Factor以及《The art of scalability》涉及的方面更多。 定义 微服务的流行，Martin功不可没，那我们就来看看在James Lewis and Martin Fowler的文章中，微服务是怎么定义的 The microservice architectural style is an approach to developing a single application as a suite of small services, each running in its own process and communicating with lightweight mechanisms, often an HTTP resource API. These services are built around business capabilities and independently deployable by fully automated deployment machinery. There is a bare minimum of centralized management of these services, which may be written in different programming languages and use different data storage technologies. 翻译成中文就是 微服务架构即是采用一组小服务来构建应用的方法。 每个服务运行在独立的进程中，不同服务通过一些轻量级交互机制来通信， 例如 RPC、HTTP 等。 服务围绕业务能力来构建，并依赖自动部署机制来独立部署。 虽然勾勒出了微服务的一些关键概念：小、独立进程、自动化，但是这样的定义还是太抽象，太务虚，很难落地。一解释以为懂了，一问还是不知道，一讨论就打架。换句话说，就是道可道，非常道。 从Martin作为ThoughtWorks公司的首席科学家的角度来看，他把微服务“炒”起来了，如果像12-Factor写得这样具体实在，那咨询业务还怎么开展呢。 相比于Martin的文章，Chris Richardson的文章microservices.io就要具体多了，它从更多角度来阐释了微服务。 起源 从微服务的定义，我们感觉似曾相识。早在 1994 年 Mike Gancarz 曾提出了 9 条著名原则，其中前 4 条和微服务架构理念特别接近。微服务就像把 UNIX 哲学应用到了分布式系统。 1. Small is beautiful. 2. Make each program do one thing well. 3. Build a prototype as soon as possible. 4. Choose portability over efficiency. 翻译成中文就是 1. 小即是美：小的服务，代码少，bug 也少，易测试，易维护，也更容易不断迭代完善。 2. 一个程序只做好一件事：一个服务也只需要做好一件事。 3. 尽可能早地创建原型：尽可能早的提供服务 API，建立服务契约，达成服务间沟通的一致性约定， 至于实现和完善可以慢慢再做。 4. 可移植性比效率更重要：服务间的交互协议在效率和可移植性二者间，首要考虑移植性。 可见微服务其实不是凭空产生的，它自有其历史渊源。而在微服务之前的十年，大家经常谈论的是一个叫 SOA（面向服务）的架构模式，它和微服务又是什么关系？在 Sam Newman 的《Building Microservices》一书中，作者对 SOA 和 Micorservices 的区别给出了定义： You should instead think of Microservices as a specific approach for SOA in the same way that XP or Scrum are specific approaches for Agile software development. 翻译成中文就是 就像认为 XP 或者 Scrum 是敏捷软件开发的一种特定方法一样， 你也可以认为微服务架构是 SOA 的一种特定方法。 面向服务架构（SOA）的概念已有十多年，它提出了一种架构设计思想， 但没有给出标准的参考实现，而早期企业软件业界自己摸索了一套实践方式 —— 企业服务总线（ESB）。 但历史证明 ESB 的实现方案甚至在传统企业软件行业也未取得成功，Martin Fowler 在文中说正是因为 ESB 当年搞砸了很多项目， 投入几百万美金，产出几乎为零，因此 SOA 这个概念也蒙上了不详的标签，所以当微服务架构出现时， 其拥护者开始拒绝使用包裹着失败阴影的 SOA 这个标签，而直接称其为微服务架构（Microservices Architecture Style）， 让人以为是一套全新的架构思想，但事实上它的本质依然是 SOA 的一种实践方式。 特征 Martin自己也说了，每个人对微服务都可以有自己的理解，不过大概的标准还是有一些的。  分布式服务组成系统 按照业务而不是技术来划分组织 做有生命的产品而不是项目 智能终端与哑管道 去中心化 自动化运维（DevOps） 容错 快速演化 分布式服务组成系统 传统实现组件的方式是通过库（library），库是和应用一起运行在进程中，库的局部变化意味着整个应用的重新部署。 通过服务来实现组件，意味着将应用拆散为一系列的服务运行在不同的进程中，那么单一服务的局部变化只需重新部署对应的服务进程。 按照业务而不是技术来划分组织 直接理解就是：服务提供的能力和业务功能对应。 比如：订单服务和数据访问服务，前者反应了真实的订单相关业务，后者是一种技术抽象服务不反应真实的业务。所以按微服务架构理念来划分服务时，是不应该存在数据访问服务这样一个服务的。 更深一层，Melvin Conway 在 1967 年观察到一个现象并总结出了一条著名的康威定律 Organizations which design systems are constrained to produce designs which are copies of the communication structures of these organizations. 翻译成中文就是 设计系统的组织，其产生的设计等同于组织之内、组织之间的沟通结构。 通俗的说法就是：组织形式等同系统设计。 看看下面的图片（来自互联网），再想想Apple的产品、微软的产品设计，就能形象生动的理解这句话。 就像六边形架构是微服务的理论基础之一一样，康威定律也是微服务架构的理论基础之一。 传统开发方式中，我们将工程师按技能专长分层为前端层、中间层、数据层，前端对应的角色为 UI、页面构建师等，中间层对应的角色为后端业务开发工程师，数据层对应着 DBA 等角色。 事实上传统应用设计架构的分层结构正反应了不同角色的沟通结构。所以若要按微服务的方式来构建应用，也需要对应调整团队的组织架构。每个服务背后的小团队的组织是跨功能的，包含实现业务所需的全面的技能。 做有生命的产品而不是项目 传统的应用开发都是基于项目模式的，开发团队根据一堆功能列表开发出一个软件应用并交付给客户后，该软件应用就进入维护模式，由另一个维护团队负责，开发团队的职责结束。 而微服务架构建议避免采用这种项目模式，更倾向于让开发团队负责整个产品的全部生命周期。Amazon 对此提出了一个观点： You build it, you run it. 开发团队对软件在生产环境的运行负全部责任，让服务的开发者与服务的使用者（客户）形成每日的交流反馈，来自直接客户的反馈有助于开发者提升服务的品质。 智能终端与哑管道 微服务架构抛弃了 ESB 过度复杂的业务规则编排、消息路由等。 服务作为智能终端，所有的业务智能逻辑在服务内部处理，而服务间的通信尽可能的轻量化，不添加任何额外的业务规则。所以这里的智能终端是指服务本身，而哑管道是通信机制，可以是同步的 RPC，也可以是异步的 MQ，它们只作为消息通道，在传输过程中不会附加额外的业务智能。 去中心化 去中心化包含两层意思： 1. 技术栈的去中心化。 2. 数据去中心化。 每个服务面临的业务场景不同，可以针对性的选择合适的技术解决方案。但也需要避免过度多样化，结合团队实际情况来选择取舍，要是每个服务都用不同的语言的技术栈来实现，想想维护成本真够高的。 每个服务独享自身的数据存储设施（缓存，数据库等），不像传统应用共享一个缓存和数据库，这样有利于服务的独立性，隔离相关干扰。 自动化运维 无自动化不微服务，自动化包括测试和部署。单一进程的传统应用被拆分为一系列的多进程服务后，意味着开发、调试、测试、监控和部署的复杂度都会相应增大，必须要有合适的自动化基础设施来支持微服务架构模式，否则开发、运维成本将大大增加。 容错 著名的 Design For Failure 思想，微服务架构采用粗粒度的进程间通信，引入了额外的复杂性和需要处理的新问题，如网络延迟、消息格式、负载均衡和容错，忽略其中任何一点都属于对“分布式计算的误解”。 快速演化 一旦采用了微服务架构模式，那么在服务需要变更时我们要特别小心，服务提供者的变更可能引发服务消费者的兼容性破坏，时刻谨记保持服务契约（接口）的兼容性。一条普适的健壮性原则，伯斯塔尔法则，给出了很好的建议： Be conservative in what you send, be liberal in what you accept. 翻译成中文就是 发送时要保守，接收时要开放。 通俗的说法就是：宽进严出 实施 前提 微服务虽然很火热，但它不是一枚银弹。它帮助大型应用打散和转移了复杂性，使其可以被更高效的并行解决，但并没有减少任何复杂性，甚至还引入了额外的分布式计算固有的复杂性。 所以什么时候该选择微服务架构，是一个权衡利弊的结果。 上面的图来自 Martin Fowler 的文章，揭示了生产率和复杂度的一个关系。在复杂度较小时采用单体应用（Monolith）的生产率更高，复杂度到了一定规模时，单体应用的生产率开始急剧下降，这时对其进行微服务化的拆分才是合算的。 图上标明了复杂度和生产率拐点的存在，但并没有量化复杂度的拐点到底是多少？或者换种说法系统或代码库的规模达到具体多大才适合开始进行微服务化的拆分。在一篇有趣的文章《程序员职业生涯中的 Norris 常数》中提到大部分普通程序员成长生涯的瓶颈在 2 万行代码左右。 当代码是在 2,000 行以下，你可以写任何混乱肮脏的代码并依靠你的记忆拯救你。 深思熟虑的类和包分解会让你的代码规模达到 20,000 行。 两万行是作者经历过并反复碰到的一个瓶颈点。 初级程序员，学会了爬行，接着蹒跚学步，然后行走，然后慢跑，然后再跑步，最后冲刺， 他认为，“以这样加速度前进我可以赶上超音速喷气式飞机的速度！“ 但他跑进了 2,000 行的极限，因为他的技能不会再按比例增加。 他必须改变移动方式，比如开车去获得更快的速度。然后，他就学会了开车， 开始很慢，然后越来越快，但又进入到了 20,000 行的极限。 驾驶汽车的技术不会让你能够开喷气式飞机。 所以每一个瓶颈点的突破意味着需要新的技能和技巧，而结合我自己的经历和经验，微服务的合适拆分拐点可能就在两万行代码规模附近，而每个微服务的规模大小最好能控制在一个普通程序员的舒适维护区范围内。借用前面的比喻，一个受过职业训练的普通程序员就像一个拿到驾照的司机，一般司机都能轻松驾驭 100 公里左右的时速，但很少有能轻松驾驭 200 公里或以上时速的司机，即使能够风险也是很高的。而能开喷气式飞机的飞行员级别的程序员恐怕在大部分的团队里一个也没有。 另外一个实施前提是基础设施的自动化，把 1 个应用进程部署到 1 台主机，部署复杂度是 1 x 1 = 1，若应用规模需要部署 200 台主机，那么部署复杂度是 1 x 200 = 200。 把 1 个应用进程拆分成了 50 个微服务进程，则部署复杂度变成了 50 x 200 = 10000，缺乏自动化设施，光部署就会把人搞死。所以前面微服务的特征才有基础设施自动化，这和规模也是有关的，这也是因为其运维复杂度的乘数级飙升， 从开发之后的构建、测试、部署都需要一个高度自动化的环境来支撑才能有效降低边际成本。 维度 实施微服务架构，可以从下面一些维度来做全面考量。 建模 微服务只是从软件实现的结果说事，没有提供一套方法论来对复杂系统进行分解，从而得到一个个微服务。 依照服务围绕业务能力来构建的方式，DDD（Domain Driven Design）中的BoundedContext，它是针对复杂系统设计的一套软件工程方法：把系统分割为一个个有边界的上下文，正好契合了微服务的这一需求。 BoundedContext，有一个比喻比较贴切：“细胞之所以会存在，是因为细胞膜定义了什么在细胞内，什么在细胞外，并且确定了什么物质可以通过细胞膜。” 协作 采用微服务架构模式后，开发和运维的协作模式都会发生变化。 按微服务的组织方式，不同人或小团队负责一个或一组微服务，服务之间可能存在相互调用关系，所以在服务之间也完全采用了面向外部开放的契约化开发模式。 每一个服务都提供了一份契约文档，发布到公开的内部 wiki，方便服务干系人可自由获取查看。契约文档要求至少对服务的几个基本方面作出说明，如下： 1. API，具体接口的 API 接入技术说明。 2. 能力，服务能力的描述。 3. 契约，提供这些能力所约定的一些限制条件说明。 4. 版本，支持的最新和历史的版本说明。 使用契约文档来减少多余且可能反复重复的口头沟通，降低协作成本。 采用微服务后一个业务功能的调用会涉及多个服务间的协同工作，由于服务间都是跨进城的调用通信，一个业务功能的完成涉及的服务调用链条可能较长，这就涉及到服务间需遵循一些规则来确保协作的可靠性和可用性。我们采用的原则是：长链条的内部服务之间的调用异步化。若一个调用链条中的个别服务变慢或阻塞可能导致整个链条产生雪崩效应，采用异步化来规避调用阻塞等待导致的雪崩情形。 测试 测试从不同的维度可以划分，如下四个象限，四个象限从不同维度视角对测试做了观察和判断，从中可以看出除了体验和探索性测试需要人工介入，其他维度的测试都可以通过自动化来实现，以降低测试人工成本和重复性工作。 而从测试所处的层次，又可以得到下面这样个一个测试金字塔： 而微服务的测试，服务开发和运营人员专注于做好服务实现层面的单元测试和服务契约层面的接口测试。而面向业务功能的端到端测试，更多是依赖自动化脚本完成。而为了维护好这些自动化测试脚本，也需要保持服务接口和契约的兼容性和稳定性，这些自动化测试脚本也属于服务的消费方之一。 监控 大量松耦合的微服务通过相互协作来完成业务功能的流程处理，在这样一个复杂的生产环境中，出现异常或错误是很难迅速定位的。 对监控进行分层，顶层的监控站在用户视角，底层的监控站在系统视角，形成更完善的反馈链路。 参考资料  Microservice 微服务的理论模型和现实路径   微服务（Microservice）那点事   微服务架构知识总结  ",
      "url"      : "https://y2p.cc/2017/05/02/microservice/",
      "keywords" : ""
    } ,
  
    {
      "title"    : "UNIT TESTING ONLINE",
      "category" : "Tool",
      "content": "本文介绍一种在线可视化单元测试方法 测试工具 docker playground goconvey 步骤 修改代码并创建包含多种工具的docker环境  选择在线docker环境 相比在本地搭建docker环境，我更乐意选择在线docker环境docker playground，一来方便，二来其网络速度要快很多。贴张图大家感受一下：  在docker playground上clone官方的docker代码  $ git clone https://github.com/docker/docker.git 在docker源码目录下，执行如下命令用以构建Docker镜像 $ cd docker $ docker build -t docker-unit-testing . 启动docker容器 执行如下命令，启动Docker镜像  $ docker run --privileged --rm -ti -p 8080:80 docker-unit-testing /bin/bash 此处，参数privileged使容器有权限访问操作系统的Kernel及设备，参数rm指示从/bin/bash终端退出后清除该容器。 -p 8080:80 80端口是nginx默认的代理端口。 此时，调试代码的docker环境已经准备就绪。 单元测试修改后的代码 执行上一步的命令后，cli会跳转到/go/src/github.com/docker/docker目录，此目录下已经包含了docker的源码，但是此源码是官方docker的代码，并不是需要调试的修改后的代码，将其删除，clone自己分支下的代码 # cd ../ # rm -rf docker/ # git clone https://github.com/yu3peng/docker.git 切换到要调试的分支 # cd docker/ # git checkout dev 由于goconvey只能用http://localhost:8080访问，我们需要借助Nginx，使其在docker playground上呈现。  安装Nginx # apt-get install -y nginx 修改配置监听8080端口 # vim /etc/nginx/nginx.conf 在http配置项中，注释掉最后一行  # include /etc/nginx/sites-enabled/*; 并在下面添加如下内容： server {  listen 80;  server_name localhost;  location / {   proxy_pass http://localhost:8080;   proxy_redirect off;   proxy_set_header Host $host;   proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;   proxy_set_header X-Real-IP $remote_addr;  }  } Nginx重启 # service nginx restart 安装并运行goconvey. # go get github.com/smartystreets/goconvey # $GOPATH/bin/goconvey &amp; 在如下页面上点击8080跳转链接 得到goconvey单元测试页面 ",
      "url"      : "https://y2p.cc/2017/05/23/unit-testing-online/",
      "keywords" : ""
    } ,
  
    {
      "title"    : "Hypernetes &amp; Kubernetes",
      "category" : "Kubernetes",
      "content": "本文介绍Hypernetes与Kubernetes之间的关系 Hypernetes相关链接 Hypernetes的卖点 github地址 基于 HyperContainer 和 Hypernetes 项目的公有云：Hyper_ 知乎话题：如何评价 hyper_ Secure NFV (Clearwater) deployment on Kubernetes (Hypernetes) in 10 min 项目拥有者博客 将Hypernetes整合至Kubernetes带来安全性与多租户机制 Hypernetes安装日志 Hypernetes简介 Hypernetes在Kubernetes基础上增加了多租户认证授权、容器SDN网络、基于Hyper的容器执行引擎以及基于Cinder的持久化存储等。 基本上Hypernetes = Bare-metal + Hyper + Kubernetes + Cinder(Ceph) + Neutron + Keystone 在介绍Hypernetes细节之前先首先提一下相关背景，也就是Kubernetes的多租户支持情况。 Kubernetes在多租户方面的支持还是比较少的，没有“租户”这一概念，也只有namespace提供了一个逻辑的资源池（可以给这个namespace设定一些资源的配额），并且它在认证授权、网络、Container Runtime等方面离真正的多租户还都比较远。  认证方面，虽然支持client certificates，tokens，http basic auth等，但没有用户管理的功能。如果想要新建用户，需要手动修改配置文件并重启服务。最新版本增加了Keystone的认证，可以借助Keystone来管理用户。   授权方面，目前只有AlwaysDeny ，AlwaysAllow以及ABAC模式。ABAC模式根据一个策略文件来配置不同用户的权限，比如限定用户只能访问特定的namespace等，但对于新创建的namespace等资源需要重复修改策略文件。   Kubernetes要求cluster内部所有的容器之间是全通的，而多租户情况下需要不同租户之前网络是隔离的。   Docker的安全性问题，跟虚拟化还是有不少距离。 正是由于上面这些原因，很多公司都在虚拟机里面来跑Kubernetes，比如Google Container Engine、OpenStack Magnum等。 在虚拟机内部跑容器虽然提升了安全性，但也引入了一些问题，比如容器的网络不能通过IaaS层来统一管理，容器无法直接使用IaaS层的持久化存储，无法集中调度所有容器的资源等。 先来看一下Hypernetes的架构图 Hypernetes在Kubernetes基础上增加了一些组件  增加了Tenant的概念，不同Tenant之间的网络和资源(ns, pod, svc, rc等)是隔离的。这些租户通过keystone管理，并提供认证和授权   通过Neutron给不同租户提供二层隔离的网络，并支持Neutron的各种插件（目前主要是ovs）   通过Hyper提供基于虚拟化的容器执行引擎，保证容器的隔离   还有通过Cinder给容器引入各种持久化存储（目前主要是ceph） 具体到Hypernetes内部，详细的架构是这样的 为了支持多租户，Hypernetes基于Kubernetes增加了很多组件，这些组件都是以Plugin的形式提供的。 这样非常方便扩展，也很容易将Hypernetes与现有的IaaS在同一个基础架构上运行起来 比如，如果你不喜欢Neutron，可以把它替换成odl等。 与Kubernetes的关系  The hypervisor-based container runtime for Kubernetes，通过Hyper提供基于虚拟化的容器执行引擎，保证容器的隔离，并将其贡献在kubernetes组织下：frakti，再通过CRI（Container Runtime Interface），原生接入Kubernetes，类似CoreOS公司的rkt接入kubernetes。 在Hypernetes v1.6版本中，Hypernetes的容器运行时也将从一套名为runV的OCI标准容器运行时切换到frakti。   Hypernetes 是从Kubernetes项目中fork出来的。为了支持多租户，Hypernetes基于Kubernetes增加了很多组件，这些组件都是以Plugin的形式提供的。   关于虚拟化和容器化的思考，可以参见项目拥有者的一篇报告Re-Think of Virtualization and Containerization   Hypernetes 借助Kubernetes中的RBAC（Role-Based Access Control）来实现接入控制，具体方式如下图所示  userA和userB是通过[role_binding1][role1]连接到name space 1，得到在name space1进行操作的权限。userB则是[role_binding2]-&gt;[role2]得到在name space2进行操作的权限。 Proposal of hypernetes1.6 auth  Continue to use keystone, Keystone authentication is enabled by passing the –experimental-keystone-url= option to the API server during startup   Use RBAC for authorization，enable the authorization module with –authorization-mode=RBAC   Add auth-controller to manage RBAC policy auth workflow: kubectl    apiserver   keystone   rbac  auth+controller +     +     +    +    + |  1   |     |    |    | +-----+request+---------&gt;     |    |    | |     |   2   |    &lt;--+update policy++ |     +----+Authentication+-----&gt;    |    | |     |   3   |    |    | |     &lt;---+user.info,success+---+    |    | |     |    4 +    |    | |     +-------------------+Authorization+------------&gt;    | |     |    5 +    |    | |     &lt;--------------------+success+-----------------+    | |  6  |     +    |    | &lt;-----+response+--------+     |    |    | |     |     |    |    | +     +     +    +    +  To update rbac policy, auth-controller need to get user-info. Auth-controller update ClusterRole and RoleBinding to get permissions of namespaces, when user is updated. May be we should add user as a kind of resource.   Watch Namespace and update the permissions of namespace scoped resources for regular user.  kubectl send request with (username, password) to apiserver Apiserver receive request from kubectl, and call AuthenticatePassword(username, password) to check (username, password) via keystone. If check successfully return (user.info, true), continue to step4. if check fail return (user.info, fales), request fail. call Authorize(authorizer.Attributes) to check RBAC roles and binding. if check success return true and execute operation of the request. if check fail return false ,request fail. return request result Add user Use Third Party Resources to create user resource : apiVersion: extensions/v1beta1 kind: ThirdPartyResource metadata: name: user.stable.example.com description: A specification of a User versions: - name: v1 Add a user object. May be there more detail user information will be set using custom fields like username. At least username are required. apiVersion: stable.example.com/v1 kind: User metadata: name: alice username: alice chen auth-controller auth-controller is responsible of updating RBAC roles and binding when  new user is created: allow the user to create/get namespace new namespace is created/deleted network is created/deleted user is removed: delete all user related data auth -controller will watch resources including User, NameSpace, Network and update RBAC roles and binding using superuser. First create ClusterRole for regular user to get permissions of NameSpace: kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1alpha1 metadata: # namespace omitted since ClusterRoles are not namespaced. name: namespace-creater rules: - apiGroups: []  resources: [namespace]  verbs: [get, create] # I think verbs should include delete，user have permission of deleting their namespace new user are created: create ClusterRoleBinding for new regular user reference namespace-creater role: kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1alpha1 metadata: name: username-namespace-creater subjects: - kind: User name: username roleRef: kind: ClusterRole name: namespace-creater apiGroup: rbac.authorization.k8s.io a new namespace are created: create Role namespaced: kind: Role apiVersion: rbac.authorization.k8s.io/v1alpha1 metadata: namespace: namespace name: access-resources-within-namespace rules: - apiGroups: []  resources: [ResourceAll]  verbs: [VerbAll] create Rolebinding reference to role namespace: kind: RoleBinding apiVersion: rbac.authorization.k8s.io/v1alpha1 metadata: name: namespace-binding namespace: namespace subjects: - kind: User  name: username roleRef: kind: Role name: access-resources-within-namespace apiGroup: rbac.authorization.k8s.io role and rolebinding will be removed when namespace are deleted a network is created/deleted Network will be defined non-namespaced resource. we can set permission of Network in ClusterRole when user are created. a user is deleted: ClusterRoleBinding related the user will be removed at least. ",
      "url"      : "https://y2p.cc/2017/06/05/hypernetes-and-kubernetes/",
      "keywords" : ""
    } ,
  
    {
      "title"    : "Twelve-Factor and Backing Service",
      "category" : "Architecture",
      "content": "本文介绍云原生的Twelve-Factor、它的扩展以及实现其中一个Factor：后端支撑服务的具体方式 Twelve-Factor 背景介绍 云原生 应用是指 出生 在云里的应用。 娘胎：IaaS 当本地应用迁移到公有云时，第一步就是把它们重新安置到云上。举个例子：如果一个公司将一个100节点的集群从本地迁移到云，它将会租用100个实例，以和本地完全相同的方式运行应用和服务（安装相同版本的操作系统以及支撑软件）。 这种租用基础设施的方式被称为基础设施即服务，也就是IaaS。 IaaS提供的好处是双重的：缩放和抽象。 缩放的好处体现在，机器可以按需添加和删除。这个过程只需点击一个按钮，而在本地情况下则需要几周的时间。 抽象的好处体现在硬件/数据中心基础设施上。IaaS提供全球基础设施，不需要在同一地理区域建立和维护多个数据中心。 有了IaaS，也无需担心管理机器、机架、网络、冷却和功耗等方面的问题。 IaaS是一个很好的开始，也是最基本的。每一个试图跳过IaaS，而直接出售PaaS和SaaS产品的云供应商都遇到了挑战。 例如，微软最初在Azure提供PaaS服务，成功之路受限。他们在2012年为Azure增加了IaaS，并持续至今。 脱胎：PaaS 迁移到公有云后，公司通常需要一些时间来适应新环境。对于一些公司，这个时间可能是几个月甚至几年。其中最大的挑战来自于在公有云上寻求与本地应用同样的安全感。 当公司熟悉并融入新的环境时，他们可能会观察到一些有趣的东西。 他们所看到的是，有现成的服务可以做他们多年来一直在做的事情。 IaaS帮他们把硬件抽离出来，这些服务帮助他们甚至把操作系统抽离出来。 应用程序可以简单地重建在这些服务之上，这为不同的需求提供了通用的平台。 这种强化被称为PaaS，平台即服务。 PaaS不仅带来了简单，也在IaaS的基础上显著降低成本。 PaaS是一个重要的步骤，因为这是云原生的第一接触点。它引入了在公有云中重构应用并充分发挥其潜力的思维过程。 云原生与容器技术 事实上，云原生应用的设计理念和原则在较早的时候就被软件架构大师们提出来，只不过在容器技术火热之前，特别是docker技术出现之前， 以虚拟机镜像为基础的应用打包发布方式开销过大，普通开发人员很难将云原生应用的设计理念和原则落实到实际开发中，导致其流行不起来。 以云原生应用的核心设计原则12-Factor来看，我们可以发现容器技术，特别是Docker，正是在设计中融入了对云原生应用的深层支持，才得以风靡软件世界。 随着对12-Factor理解的加深，我们也同时会发现12-Factor之间的彼此紧密联系，是围绕一个核心目标的12项具体指导原则。 而这个核心目标就是提高应用的可移植性和移动性。 下文中，就以Docker技术为例，来说明容器技术是怎样来支持12-Factor的。 容器技术对Twelve-Factor的支持 1. 基准代码  应用有一套基准代码，可以部署到多种环境中。这样可以保证同一套代码容易迁移到不同环境中去运行。 在Docker的体系中，Dockerfile也是基准代码的一部分，跟应用业务代码保存在同一个代码仓库中，用同一套版本标号。基于Docker体系交付的软件产品不再是一个可执行程序，而是一个Docker镜像。Docker镜像的移动性比传统的可执行程序高得多。 2. 依赖  应用要清楚地声明和隔离自己依赖的程序库。这样才能保证应用移动到其他环境时，自己所依赖的程序库也正常运行，并且与环境中其他软件不互相干扰。 Docker通过Dockerfile中的命令将自己依赖的程序声明出来，并通过docker build命令将这些依赖的程序库打包到交付的Docker镜像中。 3. 配置  运行时配置要存储到运行时环境中。一个应用的行为逻辑受两方面因素控制，一方面因素是代码，另一方面因素是配置；代码是与运行时环境无关的，要保存在应用开发的代码仓库中，而配置是与运行时环境相关的。将配置存储到运行时环境中保证了配置与环境的一致性。 Docker在Dockerfile中用ENV命令声明自己运行时所依赖的环境变量和环境变量的默认值，在docker run命令的–env参数可以在运行时设定环境变量的值。 4. 后端支撑服务  将数据库、缓存、消息队列服务这些后台支撑服务当作可挂载的资源。保证这些后端支撑服务对应用业务完全透明，应用只是把这些服务当作一种透明的资源来使用，这样更换环境部署应用只需要更改与资源相关的环境变量。 在Docker体系中，通行的做法正是将数据库、缓存和消息队列等后台支撑服务当作资源在Docker容器运行时挂载。而应用程序在构建镜像时，需要将所需要的资源的环境变量用ENV声明出来，在容器运行时，实际环境中的资源环境变量则以–env参数的方式设置到容器中。 5. 构建 发布 运行  严格区分构建和运行这两个不同的阶段。通过清楚地区分构建期和运行期两个阶段，对应的软件控制因素也清楚地分为代码和配置两类；代码在构建完成后是不会改变的，而且一套代码可以自由地部署到多套环境中去运行，不同环境中应用软件运行所需要改变的仅仅是配置。 在Docker体系中，构建器的“代码”不仅仅是应用程序编程语言的代码（例如Java，C，Python等），也包括这些代码运行时所固定依赖的程序库和这些程序库的“静态配置”。称其为“静态配置”主要是指这些配置不会因为部署环境的改变而改变，因此这些“静态配置”实际上成为的Docker镜像的“代码”。因此，对Docker镜像来说，应用业务代码和静态配置都是“代码”；只有根据部署环境可能变动的“动态配置”，才是真正的“配置”，而这些配置对应了Docker运行时的环境变量。 6. 进程  将应用作为无状态的进程来运行。无状态进程保证应用可以随时启动和关闭，随时根据业务压力而增加或减少运行实例数，保证应用的移动性。 由于Docker容器技术相对于虚拟机技术来说，大大降低了运行应用实例的开销，提高了启动和关闭应用实例的速度，应用Docker体系发布的应用与无状态应用的模型更加匹配。 7. 端口绑定  通过端口绑定来发布服务。保证一个应用服务在不同的运行环境中，可以用指定的任何端口来发布，这也是提高应用移动性的一个重要原则。 Docker在Dockerfile中用EXPOSE命令声明自己运行时容器所要发布的端口，在docker run命令的-p参数可以指定主机上发布服务的端口与容器端口的映射，这样的设计帮助应用开发者自然而然的实现云原生应用对端口绑定的要求：内部的端口在构建期决定，而对外发布的实际端口在在运行环境中决定。 8. 并发  可以通过水平伸缩应用的进程数来增大或缩小系统的容量。 如前所述，Docker容器的设计保证了水平伸缩的高效率。 9. 可丢弃性 应用进程可以快速启动也可以优雅地关闭。 Docker容器在快速启动和优雅关闭方面的效率要大大高于传统虚拟机，使得结合Docker容器实现应用的可丢弃性成为自然而然的事情。 10. 开发生产对等  保持开发环境、测试环境、预发布环境和生产环境尽量一致。保持各种环境一致，才能减少因为环境不一致造成的与业务代码无关的错误，提高应用的可移动性。 Docker容器流行起来的一大原因，就是它将应用程序所依赖的程序库以及这些程序库的“静态配置”一并打包成Docker镜像，以Docker镜像部署到不同的环境中，从而大大减少了不同环境上应用的差别，保证了一次构建，任何地方部署运行。 11. 日志  将日志以带时间戳的事件流方式来管理。把日志当作事件流来管理，实际是将日志作为结构化的数据而不是非结构化的文件来管理，这样使得日志方便在云环境中由云平台统一管理和分析；否则，在云环境中分布在各处各自为政的日志将给系统分析和排错打来极大困难。 Docker正是将日志以事件来管理的。利用docker logs命令可以查询指定容器的日志，同时，所有容器的日志正是以结构化JSON数据的格式默认保存在/var/lib/docker/containers//目录中。云平台管理软件可以统一管理处理相应的日志文件给用户提供方便的差错工具。 12. 管理进程  将管理任务当作一次性任务来运行。这样保证所有的管理任务都是可以在云平台环境下自动化的，从而为大规模应用的自动运维，大幅度的自动伸缩提供了基础。 在Docker体系下，所有的管理工作都有相应的docker客户端命令和docker的REST API服务提供支持，使得云平台可以通过调用命令行或REST API来管理容器。 Backing Service 背景介绍 Backing Service指需要通过网络调用来完成的服务。比方数据库啊，消息队列啊，以及API访问的服务比如Twitter API等等。 结合12-Factor来看，它是从后端支撑服务中引申出来的需求：开发者只需要关注应用业务本身的开发，对依赖的服务只需通过外部接入，直接使用。 举例说明 外部服务有诸如持久化、插件化，服务化等特性。拿MySQL来做比方，如果要把MySQL跟应用打包在一起跑在集群容器里，需考虑把数据存储挂一个Volume出来。 而这一点在集群中会表现的更加复杂，因为涉及到跨主机数据同步的问题，开发者也许并不熟悉这些细节原理，如果总是纠结于应用之外的问题，那毫无疑问，开发效率会明显降低。 目前主流的方式是把MySQL的连接信息注入到应用的环境变量中，如果应用不再使用这个数据库，那么可以随时卸载这个数据库，或者接入另一个数据库，只要简单地清除或修改环境变量就可以，而不用再关心怎样优雅地去停掉那个不再被使用的数据库。 在PaaS上，应用开发者依赖的服务都会产生交集，这也是我们为什么以后端服务的形式来提供这些服务的原因之一，我们把这些后端服务作为一个支撑服务体系，给开发者提供服务。 还以MySQL来比方，如果没有后端服务，那每一个依赖MySQL的应用都要起一个MySQL的容器。 对于集群管理者，意味着集群里会跑许多MySQL容器。 另一个原因是，有很多后端服务比如大数据算法和分析工具，只以API或者独立服务的形式提供出来，如此一来，开发者也没有能力在自己的应用里打包一个这样的容器/镜像。 最后一个主要原因是，这些后端服务对开发者是热插拔的。使用的时候只要通过命令/接口来创建一个服务实例，然后绑定实例到自己的应用上就可以了，非常便捷。 那么再具体地，开发者如何使用这些后端服务，与自己的应用对接呢？我们采用了Service Broker协议规范。 Service Broker是CloudFoundry中的概念，目标就是把应用与服务分离。 Service Broker实现7个REST API，通过API，我们可以知道一个Broker会提供何种服务，并且轻松地实现对服务实例的创建、绑定、解绑，以及销毁。 采用ServiceBroker规范还有一个好处就是，所有实现了Service Broker协议的服务，都可以以后端服务的方式无缝接入我们的PaaS平台。 这样，一些第三方服务就可以轻松地为开发者提供诸多服务。 我们来看一看Service Broker API:  上面7个API便是Service Broker协议中需要实现的接口。获取服务，创建实例，绑定应用，这一系列操作都通过这几个api完成。 关于Service Broker API的更多信息，请查看： open service broker api Beyond the Twelve-Factor App 建立在原有的12 factors上，根据优先顺序做了调整，添加了API first、Telemetry、Authentication and authorization，具体内容请查看原文 1. One codebase, one App Single version-controlled codebase, many deploys Multiple apps should not share code  Microservices need separate release schedules  Upgrade, deploy one without impacting others   Tie build and deploy pipelines to single codebase 2. API first Service ecosystem requires a contract  Public API   Multiple teams on different schedulers  Code to contract/API, not code dependencies   Use well-documented contract standards  Protobuf IDL, Swagger, Apiary, etc   API First != REST first  RPC can be more appropriate in some situations   3. Dependency Management Explicitly declare dependencies Include all dependencies with app release Create immutable build artifact (e.g. docker image) Rely on smallest docker image  Base on scratch if possible   App cannot rely on host for system tools or libraries 4. Design, Build, Release, Run Design part of iterative cycle  Agile doesn’t mean random or undesigned   Mature CI/CD pipeline and teams  Design to production in days not months   Build immutable artifacts Release automatically deploys to environment  Environments contains config, not release artifact   5. Configuration, Credentials, Code “3 Cs” volatile substances that explode when combined Password in a config file is as bad as password in code App must accept “3 Cs” from environment and only use harmless defaults Test - Could you expose code on Github and not reveal passwords, URLs, credentials? 6. Logs Emit formatted logs to stdout Code should not know about destination or purpose of log emissions Use downstream log aggregator collect, store, process, expose logs ELK, Splunk, Sumo, etc  Use structured logs to allow query and analysis  JSON, csv, KV, etc   Logs are not metrics 7. Disposability App must start as quickly as possible App must stop quickly and gracefully Processes start and stop all the time in the cloud Every scale up/down disposes of processes Slow dispose == slow scale Slow dispose or startup can cause availability gaps 8. Backing Services Assume all resources supplied by backing services Cannot assume mutable file system  “Disk as a Service” (e.g. S3, virtual mounts, etc)   Every backing service is bound resource  URL, credentials, etc-&gt; environment config   Host does not satisfy NFRs  Backing services and cloud infrastructure   9. Environment Parity “Works on my machine”  Cloud-native anti-pattern. Must work everywhere   Every commit is candidate for deployment Automated acceptance tests  Provide no confidence if environments don’t match   10. Administrative Processes Database migrations Run-once scripts or jobs Avoid using for batch operations, consider instead:  Event sourcing  Schedulers  Triggers from queues, etc  Lambdas/functions   11. Port Binding In cloud, infrastructure determines port App must accept port assigned by platform Containers have internal/external ports  App design must embrace this   Never use reserved ports Beware of container “host mode” networking 12. Stateless Processes What is stateless? Long-term state handled by a backing service In-memory state lives only as long as request Requests from the same client routed to different instances  “Sticky sessions” cloud native anti-pattern   13. Concurency Scale horizontally using the process model Build disposable, stateless, share-nothing processes Avoid adding CPU/RAM to increase scale/throughput Where possible, let platform/libraries do threading  Many single-threaded services &gt; 1 multi-threaded monolith   14. Telemetry Monitor apps in the cloud like satellite in orbit No tether, no live debugger Application Perf Monitoring (APM) Domain Telemetry Health and system logs 15. Authentication &amp; Authorization Security should never be an afterthought Auth should be explicit, documented decision  Even if anonymous access is allowed  Don’t allow anonymous access   Bearer tokens/OAuth/OIDC best practices Audit all attempts to access 参考资料  Docker和云原生应用的12要素   Kubernetes集成外部服务实践   真正的云原生应用是什么样   Beyond the Twelve-Factor App   High Level Cloud Native From Kevin Hoffman  ",
      "url"      : "https://y2p.cc/2017/08/01/12factor-and-backingservice/",
      "keywords" : ""
    } ,
  
    {
      "title"    : "Service-based Architecture",
      "category" : "5G",
      "content": "本文介绍基于服务的架构在3GPP 5G 核心网中的应用 广义的基于服务的架构 通常意义上的基于服务的架构，也就是广义的基于服务的架构，它强调将 服务 作为其架构中的首要组件，用于实现各种功能（包括业务层面和非业务层面）。 它包括SOA、MSA、SBA。具体信息可以参见：基于服务架构的世界 以及Comparing Service-based Architectures SOA 面向服务架构（SOA）主张根据层而不是领域来对整体架构进行拆分，导致一个简单的业务变更会影响到多个层，需要更多的测试才能发布出去。 这是一种 能共享就共享 的架构模式，侧重抽象和业务功能的重用。 business services 业务服务（business services）是一种抽象的、高层级的、粗粒度的服务，定义在企业层面的执行的核心业务操作。 因为抽象，所以不依赖于任何实现或者协议，一般只包括服务名字，期望的输入以及期望的输出。 可选地，这些服务类型还可以包括处理步骤或者跟服务相关的特殊编排规则。 业务服务一般都用XML、Web Services Definition Language（WSDL）或者Business Process Execution Language（BPEL）等语言来表述。 一般确认某个服务是否属于业务服务会在服务名上下文前后加上“我们是否在做某某的业务”来加以判断。 例如，有两个服务，分别名为ProcessTrade（处理交易）和InsertCustomer（插入客户）。 那么“我们是否在做处理交易的业务”可以很清楚看出ProcessTrade是一个业务服务； 而“我们是否在处理插入客户的业务”听上去就不对，所以不是一个好的业务服务抽象，更像是一个在处理业务服务时所调用的某个具体服务。 messaging middleware message transformation 消息转换（message transformation）指的是架构将一种数据格式转换为另外一种格式的能力。 例如，如下图所示，客户调用服务并以JSON格式发送数据，而服务期望接收的是Java对象。 消息转换并不关心请求相关的数据本身，而只是关注数据格式的转换。 微服务架构并不提供这种能力，而SOA架构则通过消息中间件提供这种功能。 message enhancement 消息增强（message enhancement）指的是架构在请求的数据部分到达服务之前之前对其进行修改、删除或者增加的能力。 消息增强的例子包括改变日期格式、添加额外数值或者查询数据库进行数据转换，例如将统一安全鉴定程序委员会（Committee on Uniform Security Identification Procedures，CUSIP）代码转换为股票代码或者从后者转换为前者。 微服务模式不提供这种能力，主要是因为其架构中不包含实现这一功能的中间件组件。 SOA通过其消息中间件完全支持这种能力。 下图展示了这种能力。 注意，图中客户所发送的是CUSIP代码（一种标准交易标识码）和格式为MM/DD/YY的日期数据，而服务期望接收的是证券交易所每日正式牌价表（Stock Exchange Daily Official List，SEDOL）代码（另外一种交易标识码）以及格式为YYYY.MM.DD的日期以及股票代码（假设是股票交易）。 本例中，消息中间件将CUSIP代码（苹果公司是037833100）转化为SEDOL数（苹果公司是2046251），查询并添加代码（AAPL），将日期从04/23/15转化为2015.04.23。 protocol transformation 协议转换（protocol transformation）所描述的是架构允许客户采用与服务端预期不匹配的协议来调用服务的能力。 下图展示了这种能力。 注意，图中服务客户采用REST进行通信，但是负责处理请求的服务要求建立RMI/IIOP链接（例如，Enterprise JavaBeans 3 [EJB3]）和AMQP连接。 微服务可以支持多种协议类型，但是服务的客户和服务必须采用同一通信协议。 在SOA架构中，多个协议则可以根据需要混合使用。 enterprise services 是具体的、企业层级的、粗粒度的服务，用以实现业务服务所定义的功能。 它们可以用任何语言和平台进行定制，或者采用第三方采购的产品（COTS）来实现。 企业服务很独特的一点是它们通常会在组织内共享。 例如，一个RetrieveCustomer（检索客户）的企业服务可能被组织内很多模块使用，用来接收客户信息。 其它例如CheckTradeCompliance（检查交易合规） , CreateCustomer（创建客户）, ValidateOrder（验证订单） 和 GetInventory（获取库存目录）等都是企业服务很好的例子。 企业服务通常依赖应用服务（application services）和基础服务（infrastructure services）来完成特定业务请求。 但是在某些情况下，某个企业服务也可能把完成特定请求所需要的所有业务功能都归入自身，形成自包含的服务。 application services 应用服务（application services）是细粒度的、特定于具体应用的服务，与某个特定应用的语境相关。 应用服务提供在企业服务中没有的特定的业务功能。 例如，一个大型保险公司汽车报价应用可能提供服务来计算汽车保险费率。 这是一个只针对该应用而并不适用于整个企业的服务。 应用服务可以从某个专用的用户界面直接调用，或者通过某个企业服务调用。 应用服务的例子包括：AddDriver（添加司机）、AddVehicle（添加车辆）以及CalculateAutoQuote（计算机车报价）等等。 infrastructure services 基础服务（infrastructure services），与微服务架构相同，这些服务用于实现非功能性任务，例如审计、安全和日志。在SOA中，基础服务可以从应用服务或者企业服务调用。 MSA 微服务架构（MSA）带来一些复杂性，比如调用关系链以及网络调用隐含的性能问题。单个微服务不管从业务领域方面还是从性能方面来说都很简单，易于理解。但如果是一群微服务，就不是这么回事了。微服务架构的优点只有在一定条件下才能得以体现，比如每个微服务对它们的数据具有独占所有权，或者有属于自己的数据库存储。 这是一种 能不共享就不共享 的架构模式，非常强调有边界的上下文。 api layer API层作为服务接入层。 在客户和服务之间放置一个API层通常是个不错的主意，因为这个组件实质上构造了一个抽象层，使得客户不需要知道服务端的确切位置。 同时也使得服务粒度的改变不会影响到服务客户。对服务粒度进行抽象的确需要在API层提供一定的智能和一定程度的调配能力，但这些问题可以慢慢通过重构解决，重要的是服务端可以根据需要演化，而不是要求服务的客户也经常做出变更。 例如，假设有个服务跟产品订单业务功能有关。 我们发现它的粒度太粗了，想把它分解成两个粒度更细的服务，从而提高服务的伸缩能力并简化部署。 如果没有API层来为实际的服务端提供抽象，使用该服务的每个客户都要做出变更，从调用一个服务转为调用两个服务。 如果使用了API层，服务的客户端就不需要知道（甚至不在乎）同一请求现在将被分解成两个服务调用。 SBA 基于服务架构比单体架构或面向服务架构具有更快的交付速度，它使用 微服务软件架构 和面向领域开发拥护者们所推崇的 以领域为中心 的方式对代码进行拆分。 面向服务架构主张根据层而不是领域来对整体架构进行拆分，导致一个简单的业务变更会影响到多个层，需要更多的测试才能发布出去。 以领域为中心的架构把测试范围减少到单个要发布的组件上，相比单体架构或面向服务架构交付得更快。 要发布的组件越小，测试范围就越小，这就是微服务优化的目标。 基于服务架构在提升软件交付速度方面也卓有成效。 狭义的基于服务的架构 此处讨论的是3GPP中的基于服务的架构 SBA：Network function service + Service-based interface 机制 更多信息请参考：5G Network Architecture:Standard Progress, and Tranfromation to SBA and Network Slicing 从上图，我们可以看到 NF Service 具有以下特性： 自包含 可复用 同一个 NF 下的各个 NF Service 独立管理 SBA架构借鉴了IT领域的“微服务”设计理念，从业务的角度，将网络功能定义为多个相对独立、可被灵活调用的服务模块。基于此，运营商可以按照业务需求进行灵活定制组网。 由此可见，SBA架构就是：把网元按照业务功能的维度进行解耦，形成相互独立、模块化的功能，然后再通过服务化的方式，在统一的构架里按业务需要组织起来，敏捷地支持多种接入方式和多种业务的需求，而且每个功能都可以独立迭代更新，以快速支持新的业务需求。 从上图中我们可以看到，基于服务的接口”则为“网络功能服务”定义了网络功能间的两种通信方式，分别是“请求–响应”、“订阅–通知”。 这也为 NF Service 提供了两种交互方式： 优点 SBA 被确定为3GPP 5G网络统一/唯一基础架构，它具备多方面的优点： 网络快速升级 提升网络资源利用率 加速网络新能力引入 在授权的情况下开放给第三方 总之，3GPP正式确定把SBA作为5G核心网的统一/唯一基础架构，意味着5G网络真正走向开放化、服务化、软件化方向，这就实现了5G网络采用模块化功能设计模式，通过对功能组件的组合，构建满足不同应用场景需求的专用逻辑网络，从而有利于实现5G与垂直行业融合发展。 影响 3GPP《5G系统总体架构标准（V15.0.0）》确定把SBA作为5G核心网的统一/唯一基础架构！ 这份3GPP《5G系统总体架构标准》，对5G系统架构、功能、接口关系、流程、漫游、与现有网络共存关系进行标准化，定义了5G系统的第二阶段系统架构。 其中第一项即与SBA相关，具体内容是：面向5G核心网控制平面应该具有 基于服务的接口 的SBA架构。 也就是说，3GPP已经正式把SBA写入/引入到目前最新版本的《5G系统总体架构标准》，SBA成为5G核心网的统一/唯一基础架构。 这是一个重磅消息，因为其重大意义，从下面这两句句话可以看出来： （1）《5G系统总体架构标准》是3GPP整个5G设计的第一个技术标准，也是事关5G全系统设计的基础性标准。 （2）5G架构和5G核心网的设计对5G网络的管理及运营至关重要，对运营商及整个产业链来说有非常重要的意义。 挑战 挑战主要来自以下两个方面： 1. NF Service 标准化 由于 NF Service 接口还在标准化中，导致同一个 NF 下的所有 NF Service 只能来自同一个供应商，目前这个问题这在 3GPP Stage 3 中处理。 这份文档规定了5GC基于SBA的技术实现。 其他相关文档： TS 22.261定义了5G系统的要求。 TS 23.501中定义了5G系统架构的要求，TS 23.502中定义了5G系统程序和流程，是TS 23.501的配套规范。 TS 29.501中规定了5GC SBI API的设计原则和文档指南。 2. SBA 业务架构落地 网络功能服务化的过程，就是细化网络功能的过程，在这个过程中划分出一个个具备不同职责的网络功能业务逻辑模块，每个网络功能业务逻辑模块都会提供相应业务逻辑的服务化接口。 这些网络功能业务逻辑模块可以独立部署和运行，并提供相应的网络功能业务能力。 但是每一个网络功能业务逻辑模块只具备相对单一的业务职能，如果要完成整体的网络功能流程和目标，就需要和周围其他的网络功能业务逻辑模块交互。 同时，这个过程中还需要依赖各种与业务无直接关联、相对独立的基础设施和组件，比如数据库、缓存、消息队列等等。 所以，除了网络功能业务逻辑模块这个实体外，还存在其他各类基础设施和组件实体，如何将这两类实体关联起来，已经超出了网络功能服务化的研究范畴。 3GPP 23.501 解析 非漫游状态下的架构以及参考点示意图  漫游状态下的架构以及参考点示意图  网络功能 Network function AF Application Function 与3GPP核心网互动，以提供服务，例如支持以下内容： 应用对流量路由的影响 访问网络曝光功能 与策略控制进行互动 基于运营商部署，允许被运营商信任的应用程序功能，可以直接与相关网络功能进行交互。 运营商不允许直接访问网络功能的应用功能，应通过NEF使用外部曝光框架与相关网络功能进行交互。 AMF Access and Mobility Management Function 在AMF的单个实例中可以支持部分或全部AMF功能： 终止RAN CP接口（N2）。 终止NAS（N1），NAS加密和完整性保护。 注册管理。 连接管理。 可达性管理。 移动管理。 合法拦截（AMF事件和LI系统接口）。 为UE和SMF之间的SM消息提供传输。 用于路由SM消息的透明代理。 访问身份验证。 访问授权。 为UE和SMSF之间的SMS消息提供传输。 安全锚定功能（SEAF）。它与AUSF和UE交互，接收作为UE认证过程的结果建立的中间密钥。在基于USIM的身份验证的情况下，AMF从AUSF检索安全资料。 安全上下文管理（SCM）。 SCM从SEAF接收用于导出访问网络特定密钥的密钥。 除了上述AMF的功能之外，AMF还可以支持非3GPP接入网络。 AUSF Authentication Server Function 支持3GPP TSG SA WG3 Security规定的认证服务器功能。 DN Data Network 比如运营商服务、互联网接入和第三方服务。 NEF Network Exposure Function 网络曝光功能（NEF）支持以下功能： 它提供了一种手段，安全地暴露3GPP网络功能提供的服务，例如边缘计算。另外，它为应用功能提供了一种安全地向3GPP网络提供信息的方法，例如，移动模式，通信模式。在这种情况下，NEF可以认证，授权和限制应用功能。 AF交换的信息与内部网络功能交换的信息之间进行转换。 网络曝光功能从其他网络功能（基于其他网络功能的暴露功能）接收信息。它可以实现前端（NEF FE），使用统一数据存储库（UDR）（由3GPP定义的接口）的标准化接口将接收到的信息存储为结构化数据。存储的信息可以被NEF访问和“重新曝光”到其他网络功能和应用功能，并用于其他目的，如分析。 应用触发服务流程   NEF（网络曝光功能）接收应用触发请求（外部标识符或MSISDN，请求标识符，有效期，应用端口ID和触发有效负载）消息。 应用端口ID用于识别UE内的接收应用。有效期指示触发消息有效的时间。 如果触发消息没有首先到达UE，并且触发消息有效，则仍然可以再次发送触发消息。 触发有效载荷包含目的地为UE上的应用的信息。 NEF检查应用程序是否被授权基于请求者标识符发送应用程序触发器。 NEF还检查请求者是否超出其触发提交的配额。   NEF调用UDM提供的“Get Subscriber Serving NF（External Identifier or MSISDN，NF type）”服务，以确定为UE服务的NF，并获得UE的SUPI。   NEF将应用程序触发器请求（SUPI，NAS Container）发送到AMF。 NAS容器包括在步骤1中接收的应用端口ID和触发有效负载。   AMF向UE发送应用触发请求（NAS容器）。如果UE未连接，则AMF寻呼UE。当UE接收到NAS容器时，应用端口ID标识要通知哪个应用，并将有效载荷发送到应用。 相应的响应消息被发送回AMF以确认接收到请求消息。   NEF从AMF接收应用程序触发响应。   NEF将应用程序触发响应发送到应用程序服务器。 NRF Network Repository Function NF Repository功能（NRF）支持以下功能： 支持服务发现功能。从NF实例接收NF发现请求，并向NF实例提供发现的NF实例（被发现）的信息。 维护可用的NF实例的NF配置文件及其支持的服务。 在NRF中维护的NF实例的信息如下： NF实例ID NF类型 PLMN ID 网络切片相关标识符，例如S-NSSAI，NSI ID NF的FQDN或IP地址 NF容量信息 NF特定服务授权信息 支持的服务名称 每个支持的服务实例的端点信息 NF服务有兴趣接收的每种通知类型的其他服务参数，例如DNN，通知端点。 服务注册流程 服务注销流程 服务发现流程 备注：每个PLMN可以由Proxy提供对外SBI接口，不同的PLMN之间，NRF可以互通 服务订阅流程 服务去订阅流程 服务通知流程 心跳检测流程 备注：心跳检测过程中可以传递负载信息 NSSF Network Slice Selection Function 网络切片选择功能（NSSF）支持以下功能： 选择为UE服务的一组网络切片实例。 确定允许的NSSAI（Network Slice Selection Assistance Information）。 确定用于服务于UE的AMF集合，或者基于配置，可能通过查询NRF来选择候选AMF的列表。 PCF Policy Control Function 策略控制功能（PCF）包括以下功能： 支持统一的策略框架来管理网络行为。 提供控制平面功能的策略规则以执行它们。 实施前端（PCF FE）以访问与统一数据存储库（UDR）中的策略决策相关的订阅信息。 (R)AN (Radio) Access Network 5G Access Network。 SMF Session Management Function 会话管理功能（SMF）包括以下功能。在SMF的单个实例中可以支持部分或全部SMF功能： 会话管理，会话建立，修改和释放，包括UPF和AN节点之间的隧道维护。 UE IP地址分配和管理（包括可选授权）。 UP功能的选择和控制。 配置UPF的流量转向，将流量路由到正确的目的地。 终止策略控制功能的接口。 控制部分策略实施和QoS。http://nealford.com/downloads/Comparing_Service-based_Architectures_by_Neal_Ford.pdf 合法拦截（SM事件和LI系统接口）。 充电数据收集和支持充电接口。 控制和协调UPF收费数据。 终止SM消息。 下行数据通知。 通过N2通过AMF向AN发送的特定SM信息的发起者。 确定会话的SSC模式。 漫游功能。 处理本地执行应用QoS SLA（VPLMN）。 收费数据收集和计费接口（VPLMN）。 合法拦截（用于SM事件的VPLMN和与LI系统的接口）。 支持与外部DN进行交互，以便通过外部DN传输PDU会话授权/认证信令。 注意：并非所有功能都需要在网络切片的实例中得到支持。 UDM Unified Data Management 统一数据管理（UDM）包括对以下功能的支持： 3GPP AKA认证证书处理。 用户识别处理。 访问授权。 注册/移动管理。 订阅管理。 短信管理。 UDM使用可以存储在UDR中的订阅数据和认证数据，在这种情况下，UDM前端（FE）实现应用程序逻辑，并且不需要内部用户数据存储。 不同的UDM FE可以在不同的交易中为同一用户服务。 UE User Equipment UPF User plane Function 用户平面功能（UPF）包括以下功能。 在UPF的单一实例中可以支持部分或全部UPF功能： Intra / RAT间移动性的锚点（如适用）。 与数据网络互连的外部PDU会话点。 分组路由和转发。 分组检查和用户平面策略规则实施的一部分。 合法拦截（UP收集）。 流量使用情况报告。 上行链路分类器，用于支持路由到数据网络的流量。 分支点支持多归属PDU会话。 用户平面的QoS处理，例如 包过滤，门控，UL / DL速率执行 上行流量验证（SDF到QoS流映射）。 上行链路和下行链路中的传输级别数据包标记。 下行数据包缓存和下行数据通知触发。 注意：在网络切片的用户平面功能的实例中，不需要支持所有的UPF功能。 Service-based interface Namf Service-based interface exhibited by AMF. NF Services provided by AMF.    Service Name  Description  Reference in TS 23.502 [3]  Example Consumer(s)     Namf_Communication  This service enables an NF to communicate with the UE and/or the AN through the AMF.  5.2.2.1  SMF, SMSF, PCF, NEF, Peer AMF    Namf_EventExposure  This service enables other NFs to subscribe or get notified of the mobility related events and statistics.  5.2.2.2  SMF, NEF, PCF, UDM   Nsmf Service-based interface exhibited by SMF. NF Services provided by SMF.    Service Name  Description  Reference in TS 23.502 [3]  Example Consumer(s)     Nsmf_PDUSession  This service manages the PDU sessions and uses the policy and charging rules received from the PCF. The service operations exposed by this NF service allows the consumer NFs to handle the PDU sessions.  5.2.8.2  V-SMF, H-SMF, AMF    Nsmf_EventExposure  This service exposes the events happening on the PDU sessions to the consumer NFs.  5.2.8.3  PCF, NEF, AMF   Nnef Service-based interface exhibited by NEF. NF Services provided by NEF.    Service Name  Description  Reference in TS 23.502 [3]  Example Consumer(s)     Nnef_EventExposure  Provides support for event exposure     Provides support for event exposure   Npcf Service-based interface exhibited by PCF. NF Services provided by PCF.    Service Name  Description  Reference in TS 23.502 [3]  Example Consumer(s)     Npcf_AMPolicyControl  This PCF service provides access control, network selection and mobility management related policies, UE Route Selection Policies to the NF consumers.  5.2.5  AMF    Npcf_SMPolicyControl  This PCF service provides session related policies to the NF consumers.  5.2.5  SMF    Npcf_Policy Authorization  This PCF service authorises an AF request and creates policies as requested by the authorised AF for the PDU session to which the AF session is bound to. This service allows the NF consumer to subscribe/unsubscribe the notification of access type and RAT type, PLMN identifier, access network information, usage report etc.  5.2.5  AF, NEF   Nudm Service-based interface exhibited by UDM. NF Services provided by UDM.    Service Name  Description  Reference in TS 23.502 [3]  Example Consumer(s)     Nudm_UE context management  provide the NF consumer of the information related to UE’s transaction information, e.g. UE’s serving NF identifier, UE status, etc.  5.2.3  AMF, SMF, SMSF    Nudm_Subscriber data management  Allow NF consumer to retrieve user subscription data when necessary  5.2.3  AMF, SMF    Nudm_Authentication  Provide updated authentication related subscriber data to the subscribed NF consumer.  5.2.3  AUSF    Nudm_EventExposure  Allow NF consumer to subscribe to receive an event.Provide monitoring indication of the event to the subscribed NF consumer.  5.4.2  NEF   Nnrf Service-based interface exhibited by NRF. NF Services provided by NRF.    Service Name  Description  Reference in TS 23.502 [3]  Example Consumer(s)     NF_management  Provides support for Discovery of NF, NF services.  5.2.7.1  AMF, SMF, PCF, NEF, NRF, SMSF, AUSF, UDM   Nausf Service-based interface exhibited by AUSF. NF Services provided by AUSF.    Service Name  Description  Reference in TS 23.502 [3]  Example Consumer(s)     Nausf UEauthentication  The AUSF provides UE authentication service to requester NF  5.2.10.1  AMF   参考资料  微服务架构之外的选择——基于服务架构   3GPP Identifies Service-Based Architecture Proposed by China Mobile as 5G Unified Infrastructure   3GPP TS 23.501v1 5G核心网架构和流程   再见MME…5G核心网你彻底变了！   鲜枣课堂：5G核心网，到底长啥样？   3GPP 5G UPF  ",
      "url"      : "https://y2p.cc/2017/09/21/service-based-architecture/",
      "keywords" : ""
    } ,
  
    {
      "title"    : "PaaS的产生、形态以及突破",
      "category" : "PaaS",
      "content": "PaaS的产生、形态以及突破 PaaS的产生 传统意义上，云计算领域从底层往上被划分成IaaS-PaaS-SaaS三层，PaaS天然的被夹在了IaaS和SaaS当中。 沿着云计算发展的轨迹分析，基础层IaaS厂商的率先爆发为云计算开辟出了万亩良田，随之而来，数以万计的SaaS应用得以在“辽阔的田野上”播种和生长，而PaaS平台则一边扎根在IaaS土壤中，一边为SaaS们出售各种生产工具、肥料和生产技术，为的是让SaaS作物能够更好、更快的破土而出。 换句话说，如果没有PaaS，SaaS应用一样可以在IaaS上长出来，只不过长得会慢一点、产量会低一点。因此，与其说PaaS平台成就SaaS应用，不如说是SaaS开发者对开发效率和易用性更高的要求，倒逼出了PaaS平台。 PaaS平台安身立命的前提是，确确实实能够帮助SaaS开发者降低成本，提高效率，即开发者用PaaS平台“卖一个锄头”所花费的时间和效率成本小于他自己去“做一个”。 PaaS的形态 PaaS，平台即服务。平台的概念本就十分宽泛，放在PaaS当中便越发难以准确诠释。咨询公司Gartner将PaaS平台分为很多类： API Management Services (apiPaaS) Application Development Services (adPaaS) Application Platform Services (aPaaS) Business Analytics Platform Services (baPaaS) Business Process Management Services (bpmPaaS) Business Rule Platform Services (brPaaS) Communications Platform Services (cPaaS) Database Platform Services (dbPaaS) Data Broker Platform Services (dbrPaaS) Event-Processing Services (epPaaS) Function Platform Services (fPaaS) Enterprise Horizontal Portal Services (Portal PaaS) In-Memory Data Grid Services (imdgPaaS) Integration Platform Services (iPaaS) Internet of Things Platform Services (IoTPaaS) Managed File Transfer Services (mftPaaS) Master Data Management Services (mdmPaaS) Message-Oriented Middleware Services (momPaaS) Mobile Back-End Platform Services (mbPaaS) 不同的业务领域，要面对的是不同的传统应用如何通过PaaS平台迁移到云上，这就会导致各自对PaaS的需求或多或少有着不同的差异，从而诞生出如此多类型的PaaS。 在此，我们拿iPaaS这种形态来举例： 目前，许多企业级SaaS厂商都在产品体系中设置了“开放平台”。主要原因在于，SaaS应用开放的API接口普遍不够多，导致企业用户在租用多个SaaS服务后很难集成，在企业内部仍形成一个个的信息孤岛。 站在企业用户的角度，随着API调用的增加，跨层应用的需求也越来越多，他们需要更加集成化的SaaS产品。但是，当他们发现多个SaaS产品事实上却是很难做到完全“打通”的，这直接导致企业在选择SaaS产品的决策成本非常之高。 当SaaS产品无法通过售后集成来全面满足企业用户的个性化需求，为了能“卖出去”，SaaS厂商只能将集成环节前置，通过“开放平台”，尽量将各种跨层应用集成在自身的产品上。这种做法实际造成了SaaS应用开发成本的增加，羊毛出在羊身上，成本最终还是会通过销售转嫁到企业用户身上。 按理来说，用户对集成的需求更应该是由SaaS厂商通过iPaaS平台来搞定。但市面上并没有能够帮助SaaS开发者解决问题的iPaaS平台，他们只能自己来做。某种意义，SaaS厂商已经在做PaaS平台的工作。一不做二不休，部分SaaS企业便将PaaS写进了自家的宣传资料里，更有甚者刻意给自己贴上PaaS的标签，先行占坑，以期标榜出一个更大的概念。 PaaS的突破 PaaS平台安身立命的根本不外乎以下三个方面： 让SaaS系统更好用 对B端企业用户来说，决心摒弃传统IT系统，转而选择SaaS化产品的内在驱动力是老系统不再适用于新形势。换句话说，即原来的系统不好用了，希望SaaS模式的产品能具备更好的易用性。 企业级SaaS行业经过近几年的蓬勃发展，企业全生命周期上的每个节点在市面上都可以找到形形色色的SaaS应用，因此竞争日趋激烈，导致行业卡位者不断寻求差异化优势。但在企业用户眼中，无论SaaS提供商如何定义其差异化的竞争优势，归根结底落在一个“通”字上，即利用云端技术，打通企业的信息流、资金流和物流。 举一个最简单的例子，企业客户在售前咨询时往往会问“一个账户是否能管理所有的系统？”，这个私有部署时代不是问题的问题，放在SaaS面前却成了个难题。试问，一套连账户体系都无法与其他系统打通的SaaS产品如何称得上“好用”？ SaaS的问题便是PaaS的追求，PaaS平台想要刷存在感，或在云计算大蛋糕上多切一块，就要从企业用户追求“易用性”的角度去思考，如何解决SaaS开发者的实际问题，而不仅仅只是告诉SaaS，我能满足你某某方面的需求，至于你客户的需求，与我无关。 让SaaS产品更便宜 从PaaS的业务逻辑上来看，这个命题是成立的。在工业时代，制造商从其他工厂购买通用零件组装成机器的成本远比自己加工更低，且效率更高。 回到云计算领域，趋势已经非常明显，不断有刚需性质的上层服务成为下层标配，也不断有下层服务集成打包升级为上层服务。那么PaaS平台便如同工业时代提供通用零件的厂商，提供数据库、存储、计算、中间件、流程等等服务，以此降低SaaS应用的开发成本。尤其是，容器技术已经成为业务快速交付、产品快速迭代的代名词，越来越多的软件厂商基于容器开发产品。 让SaaS产品更适合于企业用户所处行业 左右用户选择SaaS产品的胜负手往往来自该产品与其行业的适配性的考量，即SaaS产品能否满足企业的个性化需求。 与SaaS厂商自己搞行业解决方案的做法相比，从PaaS平台的角度切入细分行业显然更具优势，原因很简单，PaaS平台对垂直行业支持服务可以“卖”很多次，组合调整的空间和余地也更大。 因此，在企业级SaaS应用愈发重视垂直行业特性的背景下，如果PaaS平台能够让SaaS开发者获得更好的行业适配能力，未来或许大有可期。 参考资料  夹缝求生，PaaS要靠什么来刷存在感？ ",
      "url"      : "https://y2p.cc/2017/09/28/paas/",
      "keywords" : ""
    } ,
  
    {
      "title"    : "Containerd的前世今生和未来",
      "category" : "Docker",
      "content": "Containerd的前世今生和未来 Containerd官网 Containerd代码 Containerd的未来 按照前世今生和未来的顺序，这个章节不是应该放到最后吗？ 好菜不怕早，赶紧端上来，言归正传，先来看看Containerd的雄心。 OCI 标准 Linux基金会于2015年6月成立OCI（Open Container Initiative）组织，旨在围绕容器镜像格式和容器运行时制定一个开放的工业化标准， 让一个兼容性的容器可以在主要的具有兼容性的操作系统和平台之间进行移植，没有人为的技术屏障。 OCI 定义了容器基础设施的规范，获得了众多厂家的支持。 该组织一成立便得到了包括谷歌、微软、亚马逊、华为等一系列云计算厂商的支持。 Kubernetes对Containerd的支持 Kubernetes孵化项目：通过CRI引入Containerd 为了让Kubernetes在容器运行时使用containerd，需要实现CRI接口。 CRI代表“容器运行时接口”，负责群集上运行的pod和容器的分发以及生命周期管理。 从这张图，我们可以看到Kubernetes集成Containerd方式的变化，也是Kubernetes摆脱Docker依赖的方式。 虽然目前这个项目还处在孵化阶段，但是 Containerd 已经在2017年3月15日，CloudNativeCon + KubeCon Europe 2017峰会上，被 Docker 捐赠给 CNCF 基金会， 它和 Kubernetes 都属于 CNCF 基金会下面的项目，一母同胞，何况 Containerd 还有 Docker 撑腰，发展还是可以期待的。 Docker对Containerd的支持 从这张图中，我们可以看到，Containerd是基于runC的，是runC的一个集成。 runC是Docker贡献给 OCI 组织的， 按照该开放容器格式标准（OCF, Open Container Format）制定的一种具体实现，用来屏蔽各种操作系统上执行容器的差异， 这种差异主要体现在各种操作系统的凌乱系统调用和各种驱动程序的不同支持方式。 Containerd的前世 Containerd的最初目标是解决容器引擎的升级问题，并提供一个代码库。 containerd向上为Docker Engine提供了gRPC接口，使得Docker Engine屏蔽下面的结构变化，确保原有接口向下兼容；向下通过containerd-shim结合runC。 这样使得引擎可以独立升级，避免之前Docker Engine升级会导致所有容器不可用的问题。 Containerd的今生 随着Containerd项目的逐步开发，其目标不仅仅只是解决容器引擎的升级问题，也不仅仅只是在runC的基础上抽象出一系列的API。 目前变成了一个具有存储，镜像分发和运行时功能的全功能容器守护进程，以便可以为用户构建一个功能集。 其架构如下图所示：  参考资料  Docker捐出containerd，CoreOS捐出rkt   docker 内部组件结构 – docker daemon, container,runC   OCI标准和runC原理解读   KUBERNETES 1.8 RELEASE INTEGRATES WITH CONTAINERD 1.0 BETA  ",
      "url"      : "https://y2p.cc/2017/10/11/containerd/",
      "keywords" : ""
    } ,
  
    {
      "title"    : "NFV 中与 Virtual Links 相关的内容",
      "category" : "NFV",
      "content": "NFV中VL、NFP、VNF-FG、NS之间的关联 Virtual Links 下面，你会看到三个需要互相连接的VNF。 让我们假设这三个VNF是运营商的移动核心网络的组成部分，每个都有不同的角色（了解他们的角色在这里并不重要） 第一步是在三个VNF之间创建虚拟链接。 通常情况下，虚拟化层（管理程序）将抽象下面的硬件资源，使VNF通过虚拟链接连接，如下所示。 现在为什么他们被称为虚拟是因为你没有看到他们的物理实体。 一个简单的例子就是两个VNF可能存在于一台服务器上，并通过虚拟链接进行内部连接，而您并没有看到。 现在您可以看到VNF之间的多个虚拟链接。 虚拟连接可以基于二层拓扑，例如，MEF E-Line，E-LAN或E-Tree服务。 Network Forwarding Path(NFP) 下一个重要概念是网络转发路径。 网络转发路径显示虚拟链路上实际业务流的路径。 在移动核心网中，有两种流量：控制流量和用户流量。 在以下情况下，控制流量有两条路径，用户流量只有一条路径： NFP1 - 需要通过VNF1，VNF2和VNF3并使用VL2的“控制流量”的转发路径 NFP2 - 需要通过VNF1和VNF3（但不是VNF2）但仍然使用VL2（VL2可以应用某些策略限制VNF2）的“控制流量”转发路径 NFP3 - 需要通过VNF1和VNF3的“用户流量”转发路径，但需要使用VL3 VNF Forwarding Graph (VNF-FG) 下一个非常重要的概念是VNF转发图（VNF-FG） 在当前的例子中，控制流量将制作一个VNF-Forwarding Graph（VNF-FG），而用户流量将制作另一个VNF-FG。 将转发路径看作是VNF-FG的子集。 也就是说，一个VNF-FG可以有多个转发路径。 为了描述这些VNF之间的业务流，VNF-FG显示了连接VNF节点的逻辑链路图。 在这种情况下，由于这两条路径之间的不同虚拟链接的参与，将会有两个不同的VNF-FG。 VNF-FG1 - 用于控制业务的VNF-FG VNF-FG2 - 用于用户流量的VNF-FG Network Service (NS) 一旦你了解了NFP和VNF-FG的概念，网络服务的概念变得非常简单。 将网络服务看作VNF-FG之上的一层。 根据定义，网络服务是其组成功能块的行为的组合，其可以包括VNF，VNF-FG和/或基础设施网络。 注意！ 我们不只是在谈论某些VNF的组合，而是在谈论将某些VNF连接在一起时所定义的行为。 知道VNF组合是否构成网络服务的最好方法是找出是否有与之相关的网络服务描述符（NSD）。 NSD模板描述了网络服务的部署，包括服务拓扑（组成部分VNF，虚拟链路，VNF-FG）以及相关的服务特性，如SLA。 通过将移动RAN业务与移动核心业务相结合，两个或两个以上的网络业务可以结合起来，提供端到端的业务，如下面的端到端业务。 Key Takeaway:  端到端业务是网络业务的组合。   网络服务是VNF-FG的组合。   VNF-FG是NFP的组合。 参考资料  Beginner’s Guide to Network Service and VNF Forwarding Graph in NFV ",
      "url"      : "https://y2p.cc/2017/11/23/nfv-link/",
      "keywords" : ""
    } ,
  
    {
      "title"    : "5G 网络切片",
      "category" : "5G",
      "content": "5G 网络切片介绍 何为网络切片？ 打个比方，车辆是用户，道路是网络。随着车辆的增多，城市道路变得拥堵不堪… 为了缓解交通拥堵，交通部门不得不根据不同的车辆、运营方式进行分流管理，比如设置BRT快速公交通道，非机动车专用通道等… 网络亦是如此，要实现从人-人连接到万物连接，连接数量成倍上升，网络必将越来越拥堵，我们就得像交通管理一样，对网络实行分流管理——网络切片。 其实，网络切片并不是一个全新概念，例如VPN就是网络切片的基本版本；本质上网络切片就是将运营商的物理网络划分为多个虚拟网络，每一个虚拟网络根据不同的服务需求，比如时延、带宽、安全性和可靠性等来划分，以灵活的应对不同的网络应用场景。 5G的主要应用场景 具体的讲，5G网络将应对三类场景：移动宽带、大规模物联网和关键任务型物联网。三大应用场景对网络服务的需求是不相同的： 移动宽带 面向4K/8K超高清视频、全息技术、增强现实/虚拟现实等应用，对网络带宽和速率要求较高。 大规模物联网 海量的物联网传感器部署于测量、建筑、农业、物流、智慧城市、家庭等领域，这些传感器设备是非常密集的，规模庞大，且大部分是静止的，对时延和移动性要求不高。 关键任务型物联网 主要应用于无人驾驶、车联网、自动工厂、远程医疗等领域，要求超低时延和高可靠性。 解决方案 这是不是意味着我们需要为每一个服务建设一个专用网络了？例如，一个服务移动宽带，一个服务大规模物联网，一个服务关键任务型物联网。 其实不需要，因为我们可以通过网络切片技术在一个独立的物理网络上切分出多个逻辑的网络，这是一个非常节省成本的做法！ 网络切片的构架与挑战 看完这张图，首先映入脑海的是什么？满满的复杂性和挑战性。 不过总结起来就是“多横三纵”：多横是指应用场景，例如自动驾驶、只能收集、宽带接入、VR以及AR等；三纵是指接入网、核心网、数据网和服务。 1） 在接入网面向用户侧的主要挑战是，某些终端设备（比如汽车）需要要同时接入多个切片网络，另外还涉及鉴权、用户识别等问题。 2） 接入网切片如何与核心网切片配对？接入网切片如何选择核心网切片？ 3） 编排与自动化是关键。 所谓编排与自动化包括： 3.1）具备快速创建新服务的能力 3.2）具备应用模板驱动(Template Driven)切片创建环境的能力 3.3）具备切片参数多样化的能力，以满足不同的商业用例需求 3.4）具备自动化切片部署的能力 3.5）具备弹性伸缩能力和还原能力 3.6）通过服务质量监控，具备动态优化切片性能的能力 参考资料  NGMN_5G_White_Paper   到底什么叫5G网络切片？   网络切片在5G中的应用   5G系列报告一：千倍容量毫秒时延，万物互联时代即将到来  ",
      "url"      : "https://y2p.cc/2017/12/21/5g-network-slice/",
      "keywords" : ""
    } ,
  
    {
      "title"    : "NFV 关键定义",
      "category" : "NFV",
      "content": "[WIP] NFV 关键定义 virtualisation container 在 ETSI GS NFV 003[1] 中定义如下：  virtualisation container: partition of a compute node that provides an isolated virtualized computation environment. NOTE: Examples of virtualization container includes virtual machine and OS container. 计算节点的分区，提供独立的虚拟计算环境。 注意： 虚拟化容器包含了虚机（virtual machine）和 操作系统级别的容器（OS container），当需要特别指明的时候，请使用 virtual machine 或者 OS container。 关于 OS container，在 ETSI GS NFV-EVE 004[2] 中解释如下：  Container-based virtualisation, also called operating system (OS)-level virtualisation, is an approach to virtualisation which allows multiple isolated user space instances on top of a kernel space within the OS. The isolated guests are called containers. 在 ETSI GS NFV-EVE 004[2] 中，将 DockerTM 划入了OS Container 的范畴：  DockerTM is an open-source project that provides an additional layer of abstraction and automation of operating-system-level virtualisation on LinuxTM. 如果在同一篇文章中出现了 OS container 和 Application container，请提高警惕，前方有雷！ 想要排雷，请看下图： 更多解读，请参考 operating-system-containers-vs-application-containers[7] VNFC 在 ETSI GS NFV 003[1] 中定义如下：  Virtualised Network Function Component (VNFC): internal component of a VNF providing a VNF Provider a defined sub-set of that VNF’s functionality, with the main characteristic that a single instance of this component maps 1:1 against a single Virtualisation Container. VNF 的内部组件，可为 VNF 提供商提供相关功能的子集，主要的特征是：该组件的单个实例被 1：1 地映射到单个的虚拟化容器。 注意： VNFC 的单个实例被 1：1 地映射到单个的虚机（virtual machine）或通常所指的容器（OS container）。 VNFC Instance 在 ETSI GS NFV 003[1] 中定义如下：  Virtualised Network Function Component (VNFC) Instance: instance of a VNFC deployed in a specific Virtualisation Container instance. It has a lifecycle dependency with its parent VNF instance 部署在特定虚拟化容器实例中的 VNFC 实例。 它与其父 VNF 实例具有生命周期依赖关系。 VNFD 定义 在 ETSI GS NFV 003[1] 中定义 如下：  Virtualised Network Function Descriptor (VNFD): configuration template that describes a VNF in terms of its deployment and operational behaviour, and is used in the process of VNF on-boarding and managing the lifecycle of a VNF instance 描述 VNF 的部署与操作行为的配置模板，其被用于 VNF 的运行过程，以及对于 VNF 实例的生命周期管理。 在 ETSI GS NFV-IFA 011[3] 中描述 如下： A VNFD is a deployment template which describes a VNF in terms of deployment and operational behaviour requirements. It also contains connectivity, interface and virtualised resource requirements. VNFD 是根据部署和操作行为要求描述 VNF 的部署模板。 它包含连接性，接口和虚拟化资源要求。 主要内容  VDU（虚拟化部署单元）：描述部署 VNFC 时 VNFC 的资源需求。   Connectivity：    外部连接点：描述 VNF 的外部连接点，其中 VDU 的内部连接点作为外部连接点公开，或者外部连接点直接连接到内部虚拟链路。     内部虚拟链路：描述一个或多个 VDU 的虚拟链路。     内部连接点：描述 VDU 的内部连接点。     部署风格：根据内部拓扑结构和资源需求，指定 VNF 的给定部署配置。    实例化级别：根据为每个 VDU 创建的 VNFC 实例的数量描述在部署风格内实例化的资源的级别。     VDU 配置文件：描述用于特定部署风格的给定 VDU 的附加实例化数据。     VL 配置文件：描述用于特定部署风格的给定 Virtual Links 的附加实例化数据。     VNF LCM 操作配置：表示配置生命周期管理操作的信息。     Scaling Aspect：描述用于水平缩放的细节。   更多详细信息可以参考 ETSI GS NFV-SOL 001[4] 的 A.1 tVNFD with deployment flavour modelling design example 章节 VNFD 的 TOSCA 描述 在 ETSI GS NFV-SOL 001[4] 的第6章 VNFD TOSCA model 模型中有具体描述，目前这份标准的版本是 0.5.0，处在早期草案阶段，还未对外公布。 但是在对外公布的 TOSCA-Simple-Profile-NFV-v1.0[6] 的第5章 VNF Descriptor Template for NFV 有具体描述，此描述与 ETSI GS NFV-SOL 001[4] 中的描述一致。 VNFD 的 YANG 描述 ETSI GS NFV-SOL 006[5] 的第6章 VNFD YANG module definitions 是有关的描述，但是目前只有目录结构，并无实质内容，目前这份标准的版本是 0.2.0，处在早期草案阶段，还未对外公布。 VNF 在 ETSI GS NFV 003[1] 中定义如下：  Virtualised Network Function (VNF): implementation of an NF that can be deployed on a Network Function Virtualisation Infrastructure (NFVI) 可以被部署于网络功能虚拟化基础设施 NFVI 的 NF（网络功能） VNF Instance 在 ETSI GS NFV 003[1] 中定义如下：  Virtualised Network Function Instance (VNF Instance): run-time instantiation of the VNF software, resulting from completing the instantiation of its components and of the connectivity between them, using the VNF deployment and operational information captured in the VNFD, as well as additional run-time instance-specific information and constraints VNF 软件的运行时实例，VNF 实例是各个组件及其相互间连接的实例化在完成之后的结果。在相关实例化的过程之中，使用了在 VNFD（Virtualised Network Function Descriptor，虚拟化的网络功能模块描述符）之中捕获的虚拟化网络功能部署及运行信息，以及额外的运行时实例特定信息与约束条件。 VNFM 在 ETSI GS NFV 003[1] 中定义如下：  Virtualised Network Function Manager (VNFM): functional block that is responsible for the lifecycle management of VNF 负责 VNF 生命周期管理的功能块 主要功能如下:  根据 VNF 类型和容量规划 VNF 所包含的 VNFC 类型、VNFC 数量、VNFC 冗余关系、VNFC 软件信息及软件间关联关系，VNFC 所需的 VM 资源、VNFC 间网络连接关系等。   根据 VNFD 生成对 VIM 的资源需求，规划网元需要的虚拟资源 根据网元容量、网元模型（例如虚拟网元包含哪些子功能模块及各模块之间的关系）以及虚拟机推导模板，推导出虚拟网网元所需的虚拟机资源描述（Resource Request Description，RRD），在资源预留池中，根据 RRD 向 VIM 申请需要的虚拟资源。   VNF生命周期管理，包括以下5个方面。 实例化： VNF 向N FVO 申请分配 VM 资源， NFVO 分配资源成功后， VNFM 向 VIM 申请创建并启动 VM ，加载相关软件。 查询：指查询有哪些 VNF 实例。 扩容和缩容：根据节能策略以及虚拟网元的资源负荷（包括虚拟机资源和物理服务器资源）状况，触发虚拟机扩容和缩容流程，并指示 VIM 申请/释放所需虚拟机资源。 自愈：指查询有哪些 VNF 实例。 终止。   VNF 所用 NFVI 资源的数据配置（如虚拟机 IP 地址）。   VNFI 性能、事件的采集并向 EMS 或 VNF 上报。   VNF 业务量、事件的采集。   VNFI 事件与 VNF 事件关系的对应。   运行过程中根据 NFVI / VNF 的故障/性能等情况决定 VM 是否迁移，如果判断需要迁移，则向 NFVO 发起迁移请求，由 NFVO 统一协调。   构建 VNF 实例描述 VID（VNF Instance Description）文件 根据虚拟资源申请结果生成 VID 文件，并传递给 OMU ，由 OMU 引导和监控虚拟网元中的各个虚拟机业务软件版本装载过程。   网络管理。 参考资料 [1] ETSI GS NFV 003 Terminology for Main Concepts in NFV [2] ETSI GS NFV-EVE 004 Report on the application of Different Virtualisation Technologies in the NFV Framework [3] ETSI GS NFV-IFA 011 VNF Packaging Specification [4] ETSI GS NFV-SOL 001 [5] ETSI GS NFV-SOL 006 [6] TOSCA-Simple-Profile-NFV-v1.0 [7] operating-system-containers-vs-application-containers ",
      "url"      : "https://y2p.cc/2018/01/16/nfv-key-definition/",
      "keywords" : ""
    } ,
  
    {
      "title"    : "5G",
      "category" : "5G",
      "content": "5G三大场景、三大组成部分、SA &amp; NSA 5G三大场景 5G的主要优点，总结而言，就三个：  1Gbps的用户体验速率   毫秒级的延迟   百万级/k㎡的终端接入 对应三大典型应用场景：  eMBB（enhanced Mobile Broadban，增强移动宽带）   uRLLC（Ultra－Reliable and Low Latency Communications，高可靠低时延通信）   mMTC（massive Machine Type Communications，海量机器类通信） 其中mMTC又包括eMTC（enhanced Machine Type Communications，增强型物联网）和NB－IoT（Narrow Band Internet of Things，窄带物联网）。 eMTC和NB－IoT两者同属3GPP标准内的LWPA（Low Power Wide Area，低功耗广域）技术，两者既相互竞争，又相互补充。 两者的标准化进程、产业发展、现网应用等有很多相似之处，但同时两者在移动性、速率以及成本等有一些不同之处。 若对语音、移动性、速率等有较高要求，则可选择eMTC技术。 若对这些要求不高，而对成本、覆盖等要求较高，则可选择NB－IoT技术。 与5G相关的所有行业应用，都是围绕这三个场景： 典型的行业应用如下：  VR（虚拟现实），超高分辨率显示，会涉及到巨大的数据传输带宽，和增强型移动宽带相关。   自动驾驶（或远程驾驶），高速行驶的车辆，需要极低的通信时延，和高可靠低时延通信相关。   智慧城市，例如远程抄表、智慧路灯，需要和海量的终端进行有效通信，和大规模机器通信相关。 5G三大组成部分 一般来说，移动通信网络，通常由 接入网、承载网、核心网 三大部分组成，如下图所示： 接入网 接入网，就是一个“窗口”，负责把用户手机的数据收集上来（无线基站就属于接入网）。 在5G网络中，接入网不再是由BBU（基带处理单元）、RRU（射频拉远单元）、天线这些东西组成了。而是被重构为以下3个功能实体：  CU（Centralized Unit，集中单元）   DU（Distribute Unit，分布单元）   AAU（Active Antenna Unit，有源天线单元）  CU：原BBU的非实时部分被分割出来，重新定义为CU，负责处理非实时协议和服务。 DU：BBU的剩余功能重新定义为DU，负责处理物理层协议和实时服务。 AAU：BBU的部分物理层处理功能与原RRU及无源天线合并为AAU。 简单来说，AAU=RRU+天线 再抛一张图给大家，应该能看得更明白一些： 注：图中，EPC（就是核心网部分）被分为New Core和MEC两部分，并且下沉（离基站更近）。 承载网 相比于4G来说，5G需求多样化，所以要网络多样化； 因为网络多样化，所以要切片； 因为要切片，所以网元要能灵活部署； 因为网元灵活部署，所以网元之间的连接（承载网：基站和基站之间、基站和核心网之间的连接系统）也要灵活变化。 所以，才有了DU和CU这样的新架构，才有了变来变去的回传、中传、前传，这三个概念，简单说，就是对不同实体之间的连接，这是承载网的主要组成部分。 依据5G提出的标准，CU、DU、AAU可以采取分离或合设的方式，所以，会出现多种部署形态： 上图所列的部署形态，依次为：  1 与传统4G宏站一致，CU与DU共硬件部署，构成BBU单元。   2 DU部署在4G BBU机房，CU集中部署。   3 DU集中部署，CU更高层次集中。   4 CU与DU共站集中部署，类似4G的C-RAN方式。 我们再来具体看看，对于前、中、回传，到底怎么个承载法。 前传 前传（AAU + DU）。主要有三种方式： 第一种，光纤直连方式。 每个AAU与DU全部采用光纤点到点直连组网，如下图： 这就属于典型的“土豪”方式了，实现起来很简单，但最大的问题是光纤资源占用很多。随着5G基站、载频数量的急剧增加，对光纤的使用量也是激增。 所以，光纤资源比较丰富的区域，可以采用此方案。 第二种，无源WDM方式。 将彩光模块安装到AAU和DU上，通过无源设备完成WDM功能，利用一对或者一根光纤提供多个AAU到DU的连接。如下图： 什么是彩光模块？ 光复用传输链路中的光电转换器，也称为WDM波分光模块。不同中心波长的光信号在同一根光纤中传输是不会互相干扰的，所以彩光模块实现将不同波长的光信号合成一路传输，大大减少了链路成本。 采用无源WDM方式，虽然节约了光纤资源，但是也存在着运维困难，不易管理，故障定位较难等问题。 第三种，有源WDM/OTN方式。 在AAU站点和DU机房中配置相应的WDM/OTN设备，多个前传信号通过WDM技术共享光纤资源。如下图： 这种方案相比无源WDM方案，组网更加灵活（支持点对点和组环网），同时光纤资源消耗并没有增加。 中传和回传 中传（DU + CU）和回传（CU以上），由于中传与回传对于承载网在带宽、组网灵活性、网络切片等方面需求是基本一致的，所以可以使用统一的承载方案。 主要有两种方案： 第一种，分组增强型OTN + IPRAN 利用分组增强型OTN设备组建中传网络，回传部分继续使用现有IPRAN架构。 第二种，端到端分组增强型OTN 中传与回传网络全部使用分组增强型OTN设备进行组网 核心网 重要性 接入网是一个“窗口”，负责把用户手机的数据收集上来；承载网，就是“卡车”，负责把数据送来送去；核心网呢，就是“管理中枢”，负责管理这些数据，对数据进行分拣，然后告诉它，该去何方。 说白了，核心网就是一个加强版的“路由器”，管理数据，分发数据，处理数据。 核心网的英文叫Core Network（核心网络），简称CN，和中国的英文简称一致。 既然是管理中枢，核心网的意义就非同小可了。所有的用户数据，最终都是要到核心网的。所有的业务功能（例如用户权限控制、业务开通或取消），也都是要靠核心网实现的。所以，核心网的组网很复杂，设备类型很多，随便数一数，都有几十种。这些不同类型的核心网设备，分工协作，对接联调，完成工作。 而且，一旦核心网出问题，就会导致大面积的故障，影响十万、百万甚至千万级的用户（单个基站出问题，一般只会影响几百个用户）。所以，核心网的重要性，在通信网络里当之无愧排名第一。 核心网虽然复杂而且重要，但是，在整个项目设备报价里面，它并不是大头。 谁是大头？基站。 为什么呢？因为基站的数量多啊。核心网的单价是很贵的，但是数量不多，几十套设备而已。但是基站一般都是几千套几万套，所以，基站总价很高，占了一个项目总金额的大头。 现在运营商买设备，砍价都很厉害。设备商干脆就买一送一，买基站送核心网。也就是说，核心网设备根本不赚钱，赔本送。靠无线那边赚钱，来补贴核心网产品线。 如果一个项目，无线设备和核心网设备都是自己的，那就会好做很多。如果部分是自己的，部分是别人的，那就意味着项目执行难度的大幅增加。 因为你要和别的厂家进行对接联调，会多了很多困难甚至是人为阻碍。 虽然通信设备都是遵循统一标准，各个设备之间的接口都是标准的，业务流程也是标准的，但是具体在对接的时候，还是会有很多技术细节问题。 万一对方不配合，某些参数下个套，或者存在未知的兼容问题，拖你的工期，能把你搞死。 核心网要搞无线其实是很好搞的。以前恶意竞争的时候，这种事情很常见，互相下套，互相推诿，然后拖对方工期。工期就意味着成本和客户满意度，甚至是项目的验收和回款。 除了增加项目执行难度之外，如果核心网被捏在别人手里，相当于你被“边缘化”了，接触不到很多业务功能层面的东西，你的价值也会随之失去很多。也就是说，更加沦为“管道”了。 卖设备其实只是一次性收入，卖服务卖维保才是源源不断的收入。后面开新功能，要扩容，要升级，要优化，都是服务项目，都是专门另行收费的。 更需要关注的是，5G核心网，基本上都是虚拟化设备，使用的是通用x86硬件，硬件价格是不会高的，主要是软件。基于SBA的服务架构，包括切片等，管理控制编排等，都是仰仗核心网的，如果核心网不是自己的，就相当于任人摆布了。 总而言之，没有核心网，就意味着丧失了大部分主动权。 这也是为什么，设备商哪怕是送，也一定会让自家的核心网设备先进去。核心网一旦进了，无线进去的概率，就大大增加了。 而且，一般来说，运营商不会轻易搬迁替换核心网。因为核心网太复杂了，搬迁替换的难度很大。 复杂性 核心网之所以复杂，其实是人为造成的。再具体一点说，就是因为市场的需要。用户产生欲望，市场满足欲望： 欲望越多 –&gt; 需求越多 –&gt; 业务越多 –&gt; 设备越多 –&gt; 接口越多 –&gt; 网络越复杂 最早的时候，固定电话网的核心网，说白了就是把电线两头的电话连接起来，这种交换，非常简单。 后来，用户数量越来越多，网络范围越来越大，开始有了分层。 网络架构也复杂了，有了网元。网元就是Net Element（网络单元），简称NE，是具有某种功能的网络单元实体。 同时，我们要识别和管理用户了——不是任何一个用户都允许用这个通信网络。只有被授权的合法用户，才能使用。 于是，多了一堆和用户有关的设备（网元）。简而言之，它们的核心任务只有三个：认证、授权和记账。 认证，就是看你是不是合法用户，有没有密钥。 授权，就是看你有权限做什么事，哪些服务可以用，哪些不能用。 记账，就是看你做了哪些事，然后记录下来，收你的钱。 再后来，有了无线通信，连接用户的方式变了，从电话线变成无线电波，无线接入网诞生。接入网变了，核心网也要跟着变，于是有了无线核心网。 再再后来，有了2G，3G，4G。 每一代通信标准，每一项具体制式，都有属于自己的网络架构，自己的硬件平台，自己的网元，自己的设备。 为什么每一代新通信标准出现，都要换新的设备呢？ 主要原因在于： 第一，因为用户数量激增，无线速率激增，所以网络设备的数据处理能力必须随之激增。处理器的运算速度激增，设备单板端口数量和带宽激增（电口变光口），内部线路（总线）的带宽激增。 就像我们的电脑一样，以前是586,1M网卡，后来是奔腾处理器，100M网卡，现在是酷睿处理器，1000M网卡。 这就是升级换代，速度和性能的提升。 第二，就是我前面说的，业务变得无比复杂了。 最开始是打电话，后来多了发短信，再后来，多了上网（数据业务）。 你以为就这三种简单业务？细究起来，远远不止啊：  以前是后付费，你打电话，我记账，月底给你账单。后来，有了预付费，你先存钱，你打电话的同时，我随时盯着你，只要你的余额一用完，我立刻掐掉你的电话。为了实现“预付费”这个功能，我们多了“智能网”设备。 以前发文本短信，后来要发彩信。 以前电话振铃就是嘟嘟嘟，后来有了彩铃。 以前电话号码是正常位数，现在多了短号码，多了集团用户，多了特服号码（不是你想的那种“特殊服务”啊，是110,119,120这种特殊情况服务号码）。更别说还有变态的一卡双号和一号双卡。 以前上网就是统一按流量收费，现在有了定向免流量（像腾讯大王卡这样）。 …… 所有这些特殊的业务，都带来了新网元，新设备，新功能，从而导致整个核心网，越来越庞大，越来越复杂。 核心网，就是路由交换，打包发包，围着协议栈、TCP/IP、OSI模型、报文转，没有空间波那些高深的理论，也不需要想破头去考虑如何突破速率瓶颈和对抗干扰。 但是，它涉及的功能性网元种类多，网元与网元之间的接口非常多。不同的接口，使用的是不同的协议。 而且，核心网有各种业务，例如短信、智能网、VoLTE等，业务的流程非常复杂，涉及到很多网元的配合，甚至跨网络类型（例如同时接入3GPP无线和Non-3GPP无线）。 5G SA &amp; NSA 由于部署新的网络投资巨大且要分别部署这两部分，所以3GPP（3rd Generation Partnership Project，一个标准化组织）分为了两种方式进行部署， SA（Standalone，独立组网）和NSA（Non－Standalone，非独立组网）。 独立组网指的是新建一个网络，包括新基站、回程链路以及核心网，非独立组网指的是使用现有的4G基础设施，进行5G网络的部署。 在2016年6月制定的标准中，3GPP共列举了Option1、Option2 、Option 3／3a、Option 4／4a、Option 5、Option 6、Option 7／7a、Option 8／8a等8种5G架构选项。 其中，Option1、Option 2、Option5和Option 6属于独立组网方式，其余属于非独立组网方式。 在2017年3月发布的版本中，优选了（并同时增加了2个子选项3x和7x）Option 2、Option 3／3a／3x、Option 4／4a、Option 5、Option 7／7a／7x等5种5G架构选项。 独立组网方式还剩下Option 2和Option 5两个选项。 独立组网 独立组网（Standalone, SA），在2016年6月制定的标准中，有1系、2系、5系和6系，在2017年3月发布的版本中，去掉了1系和6系，剩下2系和5系。 在以下图示中，实线叫做用户面，代表传输的数据，虚线叫做控制面，代表传输管理和调度数据的命令。 1系 4G 网络，目前的部署方式，由 4G 核心网和 4G 基站组成。 2系 这种方案下，从核心网到基站，全部新建，服务质量最好，成本也最高，也是最土豪的方案了。 5系 这种方案下，4G基站升级一下，变成增强型4G基站，然后把它们接入5G核心网，这样可以利旧，多少也能省点钱。 这种方案，可以理解为先部署5G的核心网，并在5G核心网中实现4G核心网的功能，先使用增强型4G基站，随后再逐步部署5G基站。 6系 这种方案，是先部署5G基站，采用4G核心网。但此选项会限制5G系统的部分功能，如网络切片，所以选项6已经被舍弃。 非独立组网 非独立组网（Non-Standalone, NSA），4G 基站和 5G 基站联合部署，参考的是LTE双连接架构。 什么是双连接架构？ 在LTE双连接构架中，UE（用户终端）在连接态下可同时使用至少两个不同基站的无线资源(分为主站和从站)。 3系 5G基站是无法直接连在4G核心网上面的，所以，它会通过4G基站接到4G核心网。 传统4G基站的处理能力有限，无法承载5G基站这个“拖油瓶”，所以，需要进行硬件改造，变成增强型4G基站。 有的运营商，不愿意花钱改造4G基站（毕竟都是旧设备，迟早要淘汰）。于是，想了别的办法。 第一种办法，5G基站的用户面直接通4G核心网，控制面继续锚定于4G基站。 什么叫用户面？什么叫控制面？ 简单来说，用户面就是用户具体的数据，控制面就是管理和调度的那些命令。 上面这种方式，叫做 “3a”。 第二种方法，就是把用户面数据分为两部分，会对 4G 基站造成瓶颈的那部分，迁移到 5G 基站。剩下的部分，继续走 4G 基站 这种方式，叫做”3x”。 我们把它们三个放在一起，可以对比看看： 需要注意的是，只有 “3” 是增强型 4G 基站 3/3a/3x 组网方式，是目前国外运营商最喜欢的方式，原因很简单： 利旧了4G基站，省钱。 部署起来很快很方便，有利于迅速推入市场，抢占用户。 4系 在”4系”组网里，4G 基站和 5G 基站共用 5G 核心网，5G 基站为主站，4G 基站为从站。 唯一不同的，选项4的用户面从 5G 基站走，选项 4a 的用户面直接连 5G 核心网。如下图所示： 7系 把”3系”组网方式里面的4G核心网替换成5G核心网，这就是”7系”组网方式。 需要注意的是，因为核心网是 5G 核心网，所以此类方式下，4G 基站都需要升级成增强型 4G 基站。 8系 8和8a使用的是4G核心网，运用5G基站将控制面命令和用户面数据传输至4G核心网中， 由于需要对4G核心网进行升级改造，成本更高，改造更加复杂，所以这个选项在2017年3月发布的版本中被舍弃，这里不做更多的介绍。 选择 5G标准落地后，运营商们采取何种方式组网是关注焦点。中国三大运营商均选择了5G独立组网（SA）的技术路线。 中国移动 打通全球首个全息视频通话 中国移动在上海全球移动通信大会上放出一颗重磅炸弹。昨日上午，中国移动联合大唐电信、爱立信、华为、英特尔和诺基亚等全球合作伙伴共同发布“5G SA（独立组网）起航行动”。 其间，中央网信办副主任杨小伟与上海移动技术人员现场进行了基于5G独立组网端到端系统的全息视频通话，首次采用5G独立组网端到端系统技术，包括5G CPE终端、新空口和新核心网。引人关注的是，这是全球首个5G独立组网端到端系统全息视频通话，在业界看来，这标志着5G独立组网技术取得突破进展，距离商用更进一层。 在同日发布的《5G SA（独立组网）核心网实现优化白皮书》里，中国移动展示了5G独立组网技术和产业发展的最新进展。其中电网差动保护系统利用了5G的网络切片特性，快速完成配网线路的故障判断及隔离；AR/VR直播系统利用了5G网络切片和边缘计算特性，实现高清视频信号的独立采集、跨域传输和本地分发。这些枯燥的技术术语背后，反映出中国移动对SA的高投入正迎来收获期。 6月14日，3GPP冻结了5G第一版本的独立组网标准，加上之前确定的非独立组网标准（NSA)，第一版本5G国际标准由此正式出炉。 此前，中国移动研究院院长黄宇红表示，在5G SA标准的制定过程中，中国移动做出了重要贡献，其中主导提出了5G独立组网的基础架构。 按照中国移动的时间表，2018年7月将展开5G非独立组网的外场测试，11月启动独立组网外场测试，明年10月启动友好用户测试；在产品计划方面，今年底首批5G芯片将面世，明年一季度推出首批5G终端，三季度推出5G智能手机。 中国联通 首次明确以独立组网方式建网 首次明确表态将以SA为目标架构。前期聚焦eMBB（增强移动宽带），持续保持中国联通在3G和4G时代的网络速率优势，为高清视频、VR/AR游戏娱乐、车载影音、智能家庭等大流量高带宽应用提供全方位的网络支持。未来将结合技术标准和生态系统的发展进程，引入uRLLC（低时延高可靠）和mMTC(海量机器类通信)技术，提供车联网、工业互联网等垂直行业的数字化转型支持。 根据时间表，2019年该公司将进行5G业务规模示范应用及试商用，计划在2020年正式商用。 中国联通5G战略最引人关注的是，与不少“小伙伴”联手打造5G产业合作新生态，合作对象包括腾讯、百度、华为等。 中国电信 已经率先发布5G技术白皮书，指出中国电信将在5G核心网采用SA组网方案。 设备商 亢奋的不只是电信运营商，产业链各方也密集发布5G战略。设备商们都在迎风热舞，狂刷存在感。 大唐发布了《5G业务应用白皮书》，围绕5G三大典型应用场景，选取了与5G结合点较强的十大应用领域进行研究，阐述该公司的5G技术储备能力，同时还与多家企业达成了5G应用方面的签约合作。 爱立信联合中国联通、驭势科技进行国内首个5G超远程智能驾驶实车演示，并与中国移动联手开展5G智能工厂改造应用试点。 诺基亚贝尔与腾讯签署战略合作框架协议，将合作建设5G联合实验室，推动5G新业务研究与验证。 新华三带着自家的5G小站加入5G的冲刺大战，将于今年底在运营商网络中进行试商用。华为则宣布将于2018年9月30日推出基于NSA的全套5G商用网络解决方案；2019年3月30日则会推出基于SA的5G商用系统。 参考资料 [1] 三大运营商5G路径全选SA [2] 5G独立组网和非独立组网的8种方式有何不同 [3] 鲜枣课堂：关于5G的NSA和SA，看完秒懂！ [4] 鲜枣课堂：关于5G接入网，看这一篇就够啦！ [5] 鲜枣课堂：核心网，你为什么这么难？！ [6] 鲜枣课堂：从2G到5G，核心网，你到底经历了什么？ [7] 鲜枣课堂：有史以来最强的5G入门科普！ [8] 专用5G网络的7种部署方案 ",
      "url"      : "https://y2p.cc/2018/12/12/5g/",
      "keywords" : ""
    } ,
  
    {
      "title"    : "Kubernetes HPA",
      "category" : "Kubernetes",
      "content": "Kubernetes HPA Kubernetes 设计理念 用户定义应用程序的规格，Kubernetes 按照定义的规格部署并运行应用程序。 其实现依赖于 声明式 API: 就是当用户向 Kubernetes 提交了一个 API 对象的描述之后，Kubernetes 会负责为你保证整个集群里各项资源的状态，都与你的 API 对象描述的需求相一致。更重要的是，这个保证是一项“无条件的”、“没有期限”的承诺：对于每个保存在 etcd 里的 API 对象，Kubernetes 都通过启动一种叫做“控制器模式”（Controller Pattern）的无限循环，不断检查，然后调谐，最后确保整个集群的状态与这个 API 对象的描述一致。 for { 实际状态 := 获取集群中对象X的实际状态（Actual State） 期望状态 := 获取集群中对象X的期望状态（Desired State） if 实际状态 == 期望状态{  什么都不做 } else {  执行编排动作，将实际状态调整为期望状态 } } HPA 1、HPA全称Horizontal Pod Autoscaling，即POD的水平自动伸缩。 Kubernetes 最大的特点是在用户定义的应用程序规格内进行自动伸缩，自动伸缩主要分为两种，其一为POD的水平伸缩，针对于POD数目的增减，也就是HPA；其二为垂直伸缩，即单个POD可以使用的资源的增减，也就是在POD的requests（最小资源）、limits（最大资源）范围内增减资源。 2、HPA是Kubernetes中实现POD水平自动伸缩的功能。 云计算具有水平弹性的特性，这是云计算区别于传统IT技术架构的主要特性。对于Kubernetes中的POD来说，HPA可以实现很多自动化功能，比如当POD中业务负载上升的时候，可以创建新的POD来保证业务系统稳定运行，当POD中业务负载下降的时候，可以销毁POD来提高资源利用率。 3、HPA控制器默认每隔30秒就会运行一次。 如果要修改间隔时间，可以设置horizontal-pod-autoscaler-sync-period参数。 4、HPA的操作对象是RC、RS或Deployment对应的POD 根据观察到的CPU等实际使用量与用户的期望值进行比对，做出是否需要增减POD实例数量的决策。 5、HPA的发展历程 在Kubernetes v1.1中首次引入了HPA特性。HPA第一个版本基于观察到的CPU利用率，后续版本支持基于内存使用。 在Kubernetes 1.6中引入了一个新的API，用来自定义指标，它允许HPA访问任意指标。 Kubernetes 1.7引入了聚合层，允许第三方应用程序通过注册为API附加组件来扩展Kubernetes API。自定义指标API以及聚合层使得像Prometheus这样的监控系统可以向HPA控制器公开特定于应用程序的指标。 部署监测工具 因为POD的metrics信息来源于其他第三方监控组件，所以在开始之前要保证监控组件的运行正常，我们拿metrics-server来举例。 部署metrics-server 可以按照以下方式部署： 测量 CPU/Memory HPA ，可以使用 https://github.com/kubernetes-incubator/metrics-server 中所示方式，如下： mkdir K8s-monitor cd K8s-monitor git clone https://github.com/kubernetes-incubator/metrics-server.git # 如果 Kubernetes &gt; 1.8 cd metrics-server/deploy/1.8+/ 修改 metrics-server-deployment.yaml 配置，在 containers 中添加 command 有关的内容  containers:  - name: metrics-server  image: k8s.gcr.io/metrics-server-amd64:v0.3.1  imagePullPolicy: Always  command:  - /metrics-server  - --kubelet-insecure-tls  - --kubelet-preferred-address-types=InternalIP,Hostname,InternalDNS,ExternalDNS,ExternalIP  volumeMounts:  - name: tmp-dir   mountPath: /tmp 部署 kubectl apply -f ./ 验证 可以通过运行 kubectl top node 来验证metrics-server是否运行正常。 运行不正常情况 kubectl top node error: metrics not available yet 运行正常情况 kubectl top node NAME  CPU(cores) CPU% MEMORY(bytes) MEMORY% master01 228m  5%  2551Mi  26% minion01 189m  4%  1991Mi  20% minion02 138m  3%  1594Mi  16% minion03 75m  1%  941Mi   9% 提示：刚部署完成后，需要稍等一会才会出现运行正常的情况。 部署prometheus 可以使用 https://github.com/coreos/prometheus-operator 中所示方式。 部署其他监控维度的监测工具 可以参考 https://github.com/heptiolabs/eventrouter 中所示方式。 制作镜像 新建文件夹hpa-test-app 在hpa-test-app文件夹下，新建hpa-test-app.go文件，内容如下： package main import (  fmt  log  net/http  strconv  strings ) func main() {  http.HandleFunc(/, compPiHandler)  log.Println(Server work at :8080 ...)  err := http.ListenAndServe(:8080, nil)  if err != nil {  log.Fatal(ListenAndServe: , err)  } } func compPiHandler(w http.ResponseWriter, r *http.Request) {  var (  lenI int  err error  ) lenS := r.URL.Query().Get(length)  if lenS != {  lenI, err = strconv.Atoi(lenS)  if err != nil {   lenI = 100  }  } else {  lenI = 100  } fmt.Fprintf(w, compPi(lenI)) } func compPi(L int) string {  var Pi []string N := (L)/4 + 1 s := make([]int, N+3)  w := make([]int, N+3)  v := make([]int, N+3)  q := make([]int, N+3)  n := (int)(float32(L)/1.39793 + 1) w[0] = 16 * 5  v[0] = 4 * 239 for k := 1; k &lt;= n; k++ {  div(w, 25, w, N)  div(v, 57121, v, N)  sub(w, v, q, N)  div(q, 2*k-1, q, N)  if k%2 != 0 {   add(s, q, s, N)  } else {   sub(s, q, s, N)  }  } Pi = append(Pi, fmt.Sprintf(%d., s[0])) for k := 1; k &lt; N; k++ {  Pi = append(Pi, fmt.Sprintf(%04d, s[k]))  } return strings.Join(Pi, ) } func add(a []int, b []int, c []int, N int) {  i, carry := 0, 0  for i = N + 1; i &gt;= 0; i-- {  c[i] = a[i] + b[i] + carry  if c[i] &lt; 10000 {   carry = 0  } else {   c[i] = c[i] - 10000   carry = 1  }  } } func sub(a []int, b []int, c []int, N int) {  i, borrow := 0, 0  for i = N + 1; i &gt;= 0; i-- {  c[i] = a[i] - b[i] - borrow  if c[i] &gt;= 0 {   borrow = 0  } else {   c[i] = c[i] + 10000   borrow = 1  }  } } func div(a []int, b int, c []int, N int) {  i, tmp, remain := 0, 0, 0  for i = 0; i &lt;= N+1; i++ {  tmp = a[i] + remain  c[i] = tmp / b  remain = (tmp % b) * 10000  } } 编译成linux下的程序包，我是在mac上开发的，需要编译到linux下运行，涉及到跨平台编译。 编译代码命令： sudo env GOOS=linux GOARCH=386 go build hpa-test-app.go 如果知道linux系统的准确信息（采用 uname -a 命令查看），例如amd64或者i386，将上述命令中的 GOARCH=386 替换为 GOARCH=amd64 或者 GOARCH=i386 编译完成后，本地会多出一个 hpa-test-app 程序，备用。 在hpa-test-app文件夹下，新建Dockerfile文件，内容如下： FROM alpine:latest MAINTAINER YP yu3peng@qq.com WORKDIR $GOPATH/src/github.com/yu3peng/hpa-test-app ADD hpa-test-app $GOPATH/src/github.com/yu3peng/hpa-test-app EXPOSE 8080 ENTRYPOINT [./hpa-test-app] 在当前目录下，执行docker build -t hpa-test-app . 输出 Sending build context to Docker daemon 5.12 kB Step 1/6 : FROM golang:latest ---&gt; 2422e4d43e15 Step 2/6 : MAINTAINER YP yu3peng@qq.com ---&gt; Using cache ---&gt; f4a9005be88b Step 3/6 : WORKDIR $GOPATH/src/github.com/yu3peng ---&gt; Using cache ---&gt; 6d5fec3b81a1 Step 4/6 : ADD . $GOPATH/src/github.com/yu3peng ---&gt; Using cache ---&gt; 24b3025ae163 Step 5/6 : EXPOSE 8080 ---&gt; Using cache ---&gt; c2e09786bf15 Step 6/6 : ENTRYPOINT ./hpa-test-app ---&gt; Using cache ---&gt; b9d6c77740b8 Successfully built b9d6c77740b8 执行docker image ls REPOSITORY  TAG    IMAGE ID  CREATED   SIZE hpa-test-app  latest   b9d6c77740b8  14 minutes ago 10 MB 服务测试 基于上面提供的hpa-test-app镜像, 我们创建一个hpa-test-app service, 然后为该service添加HPA机制。 创建Deployment 和 service ~$ kubectl get svc,deploy NAME    TYPE  CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/kubernetes ClusterIP 10.96.0.1 &lt;none&gt;  443/TCP 45d ~$ kubectl run hpa-test-app --image=yu3peng/hpa-test-app:latest --requests=cpu=6m,memory=32Mi --limits=cpu=60m,memory=128Mi --expose --port=8080 service hpa-test-app created deployment.apps hpa-test-app created ~$ kubectl get svc,deploy NAME    TYPE  CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/hpa-test-app ClusterIP 10.105.191.80 &lt;none&gt;  8080/TCP 4s service/kubernetes  ClusterIP 10.96.0.1  &lt;none&gt;  443/TCP 45d NAME      DESIRED CURRENT UP-TO-DATE AVAILABLE AGE deployment.extensions/hpa-test-app 1  1  1  0   4s 创建HPA 基于CPU利用率 可以直接使用命令创建HPA ~$ kubectl autoscale deployment hpa-test-app --cpu-percent=70 --min=1 --max=10 deployment.apps hpa-test-app autoscaled ~$ kubectl get hpa NAME   REFERENCE    TARGETS MINPODS MAXPODS REPLICAS AGE hpa-test-app Deployment/hpa-test-app 31%/70% 1  10  1  16s 基于Memory利用率 需要通过配置文件创建 cat &lt;&lt; EOF &gt; hpa-test-app-memory-metrics.yaml apiVersion: autoscaling/v2beta1 kind: HorizontalPodAutoscaler metadata: name: hpa-test-app-memory-metrics namespace: default spec: scaleTargetRef:  apiVersion: extensions/v1beta1  kind: Deployment  name: hpa-test-app minReplicas: 1 maxReplicas: 10 metrics: - type: Resource  resource:  name: memory  targetAverageValue: 100Mi EOF kubectl create -f hpa-test-app-memory-metrics.yaml 增压和减压测试 首先我们启动一个busybox的pod, 用来对hap-test-app服务进行压力测试 cat &lt;&lt; EOF &gt; pod-busybox.yaml apiVersion: v1 kind: Pod metadata: name: busybox namespace: default spec: containers: - image: busybox  command:  - sleep  - 3600  imagePullPolicy: IfNotPresent  name: busybox restartPolicy: Always EOF kubectl create -f pod-busybox.yaml 然后我们找到hpa-test-app服务的clusterIP: ~$ kubectl get svc hpa-test-app NAME   TYPE  CLUSTER-IP EXTERNAL-IP PORT(S) AGE hpa-test-app ClusterIP 10.105.191.80 &lt;none&gt;  8080/TCP 3m 应用扩张 进入容器, 持续访问hpa-test-app的API 默认计算100位的π，用以下命令实现 kubectl exec -it busybox /bin/sh / # while true; do wget -q -O- 10.105.191.80:8080; done 如果想加大计算（例如计算1000为的π）可以通过以下命令实现 kubectl exec -it busybox /bin/sh / # while true; do wget -q -O- 10.105.191.80:8080/?length=1000; done 然后我们持续关系HPA的扩张 ~$ kubectl get hpa NAME   REFERENCE    TARGETS MINPODS MAXPODS REPLICAS AGE hpa-test-app Deployment/hpa-test-app 100%/70% 1  10  1  3m ~$ kubectl get hpa NAME   REFERENCE    TARGETS MINPODS MAXPODS REPLICAS AGE hpa-test-app Deployment/hpa-test-app 100%/70% 1  10  2  3m ~$ kubectl get hpa NAME   REFERENCE    TARGETS MINPODS MAXPODS REPLICAS AGE hpa-test-app Deployment/hpa-test-app 331%/70% 1  10  2  4m ~$ kubectl get hpa NAME   REFERENCE    TARGETS MINPODS MAXPODS REPLICAS AGE hpa-test-app Deployment/hpa-test-app 331%/70% 1  10  5  4m ~$ kubectl get hpa NAME   REFERENCE    TARGETS MINPODS MAXPODS REPLICAS AGE hpa-test-app Deployment/hpa-test-app 185%/70% 1  10  8  5m ~$ kubectl get hpa NAME   REFERENCE    TARGETS MINPODS MAXPODS REPLICAS AGE hpa-test-app Deployment/hpa-test-app 130%/70% 1  10  10  7m pod数量的变化情况 1–&gt;2–&gt;5–&gt;8–&gt;10, 最终达到最大的扩展上线而停止。 应用收缩 中断对app的访问, 会发现容器又收缩为原来的1个: ~$ kubectl get hpa NAME   REFERENCE    TARGETS MINPODS MAXPODS REPLICAS AGE hpa-test-app Deployment/hpa-test-app 33%/70% 1  10  3  21m 参考资料 [1] 如何利用kubernetes实现应用的水平扩展(HPA) [2] k8s controller-manager之hpa源码分析 [3] k8s全栈监控之metrics-server和prometheus [4] K8S集群基于heapster的HPA测试 [5] kubectl run 背后到底发生了什么？ [6] Kubernetes之kubectl常用命令使用指南:2:故障排查 ",
      "url"      : "https://y2p.cc/2018/12/20/kubernetes-hpa/",
      "keywords" : ""
    } ,
  
    {
      "title"    : "Serverless CNCF Whitepaper",
      "category" : "Serverless",
      "content": "Serverless CNCF 白皮书 什么是serverless计算? Serverless计算是指构建和运行不需要服务器管理的应用程序的概念。它描述了一种更细粒度的部署模型，应用程序捆绑一个或多个function，上载到平台，然后执行，缩放和计费，以响应当前所需的确切需求。 Serverless计算并不意味着我们不再使用服务器来托管和运行代码;这也不意味着不再需要运维工程师。相反，它指的是serverless计算的消费者不再需要花费时间和资源来进行服务器配置，维护，更新，扩展和容量规划。相反，所有这些任务和功能都由serverless平台处理，并完全从开发人员和IT/运维团队中抽象出来。因此，开发人员专注于编写应用程序的业务逻辑。运维工程师能够将重点更多的放到关键业务任务上。 Serverless有两个主要角色：  Developer/开发人员：为serverless平台编写代码并从中获益，serverless平台提供了这样的视角：没有服务器，而代码始终在运行。 Provider/供应商：为外部或内部客户部署serverless平台。 运行serverless平台仍然需要服务器。供应商需要管理服务器（或虚拟机和容器）。即使在空闲时，供应商也会有一些运行平台的成本。自托管系统仍然可以被视为serverless：通常一个团队充当供应商，另一个团队充当开发人员。 Serverless计算平台可以提供以下中的一个或两个：  Functions-as-a-Service (FaaS)，通常提供事件驱动计算。开发人员使用由事件或HTTP请求触发的function来运行和管理应用程序代码。 开发人员将代码的小型单元部署到FaaS，这些代码根据需要作为离散动作执行，无需管理服务器或任何其他底层基础设施即可进行扩展。 Backend-as-a-Service (BaaS)，它是基于API的第三方服务，可替代应用程序中的核心功能子集。因为这些API是作为可以自动扩展和透明操作的服务而提供的，所以对于开发人员表现为是serverless。 Serverless产品或平台为开发人员带来以下好处：  零服务器运维：serverless通过消除维护服务器资源所涉及的开销，显着改变了运行软件应用程序的成本模型。    无需配置，更新和管理服务器基础设施。管理服务器，虚拟机和容器对于公司而言是一项重大费用，其中包括人员，工具，培训和时间。Serverless大大减少了这种费用。     灵活的可扩展性：serverless的FaaS或BaaS产品可以即时而精确地扩展，以处理每个单独的传入请求。对于开发人员来说，serverless平台没有“预先计划容量”的概念，也不需要配置“自动缩放”的触发器或规则。缩放可以在没有开发人员干预的情况下自动进行。完成请求处理后，serverless FaaS会自动收缩计算资源，因此不会有空闲容量。     闲置时无计算成本：从消费者的角度来看，serverless产品的最大好处之一是空闲容量不会产生任何成本。例如，serverless计算服务不对空闲虚拟机或容器收费; 换句话说，当代码没有运行或者没有进行有意义的工作时，不收费。 对于数据库，数据库引擎容量空闲等待请求时无需收费。当然，这不包括其他成本，例如有状态存储成本或添加的功能/功能/特性集。 Serverless技术简史 虽然按需或“花多少用多少”模式的概念可追溯到2006年和一个名为Zimki的平台，但serverless一词的第一次使用是2012年来自Iron.io的IronWorker产品 ，一个基于容器的分布式按需工作平台。 从那以后，公共云和私有云都出现了更多serverless实现。首先是BaaS产品，例如2011年的Parse和2012年的Firebase（分别由Facebook和谷歌收购）。2014年11月，AWS Lambda推出，2016年初在Bluemix上宣布了IBM OpenWhisk on Bluemix（现在是IBM Cloud Functions，其核心开源项目成为Apache OpenWhisk），Google Cloud Functions和Microsoft Azure Functions。华为Function Stage于2017年推出。还有许多开源serverless框架。每个框架（公共和私有）都具有独特的语言运行时和服务集，用于处理事件和数据。 这只是几个例子; 有关更完整和最新的列表，请参阅 Serverless Landscape 文档。Detail View: Serverless Processing Model 部分包含有关整个FaaS模型的更多详细信息。 Serverless用例 虽然serverless计算广泛使用，但它仍然相对较新。通常，当工作负载为以下情况时，应将serverless方法视为首选：  异步，并发，易于并行化为独立的工作单元 不经常或有零星的需求，在扩展要求方面存在巨大的，不可预测的差异 无状态，短暂的，对瞬间冷启动时间没有重大需求 在业务需求变更方面具有高度动态性，需要加快开发人员的速度 例如，对于常见的基于HTTP的应用程序，在自动扩展和更细粒度的成本模型方面有明显的优势。也就是说，使用serverless平台可能会有一些权衡。 例如，如果function的实例数下降到零，则在一段不活动时间后function启动可能会导致性能下降。因此，选择是否采用serverless架构需要仔细查看计算模型的功能性和非功能性方面。 不适合IaaS，PaaS或CaaS解决方案的非HTTP中心和非弹性规模工作负载现在可以利用serverless架构的按需性质和高效成本模型。其中一些工作负载包括：  执行逻辑以响应数据库更改（insert, update, trigger, delete） 对IoT传感器输入消息（例如MQTT消息）执行分析 流处理（分析或修改动态数据） 管理单次提取，转换和加载的作业，这些作业需要在短时间内进行大量处理（ETL） 通过聊天机器人界面提供认知计算（异步，但有关联） 调度执行时间很短的任务，例如cron或批处理样式调用 服务于机器学习和AI模型（检索一个或多个数据元素，如表格，NLP或图像，并与预先学习的数据模型匹配，以识别文本，面孔，异常等） 持续集成pipeline，按需为构建作业提供资源，而不是保留构建从属主机池等待作业分派 本节介绍serverless架构优秀的现有和新兴工作负载和用例。它还包括从早期成功案例中提取的早期结果，模式和最佳实践的详细信息。 这些场景中的每一个都显示了serverless架构如何解决技术问题，即Iaas，PaaS或CaaS效率低下或无法实现。这些例子是：  在没有按需模型的情况下，有效解决了一个全新的问题 更有效地解决了传统的云问题（性能，成本） 显示“大”的维度，无论是处理的数据大小还是处理的请求 通过低错误率的自动缩放（向上和向下）来显示弹性 以前所未有的速度（从天到小时）为市场带来了解决方案 多媒体处理 一个常见的用例，也是最早具体化的用例之一，是响应新文件上传执行一些转换过程的函数。 例如，如果将图像上载到诸如Amazon S3的对象存储服务，则该事件触发函数，用于创建图像的缩略图版本并将其存储回另一个对象存储桶或Database-as-a-Service。 这是一个相当原子化，可并行化的计算任务示例，该计算任务不经常运行并根据需求进行伸缩。 例子包括：  Santander 使用serverless function构建了一个概念验证，使用光学字符识别来处理移动支票存款。 这种类型的工作量变化很大，发薪日的处理需求 - 每两周一次 - 可能比支付期的大部分空闲时间大几倍。   通过将 每个视频帧通过图像识别服务 来自动分类电影，以提取演员，情感和对象; 或处理灾区的无人机镜头以估计损坏的程度。 数据库更改或更改数据捕获（CDC） 在此场景中，当从数据库插入，修改或删除数据时调用function。在这种情况下，它的功能类似于传统的SQL触发器，几乎就像是与主同步流并行的副作用或动作。其结果是执行一个异步逻辑，可以修改同一个数据库中的某些内容（例如记录到审计表），或者依次调用外部服务（例如发送电子邮件）或更新其他数据库，例如 DB CDC（更改数据捕获）用例的情况。 由于业务需要和处理变更的服务分布的原因，这些用例的频率以及对原子性和一致性的需要可能不同。 例子包括：  审核对数据库的更改，或确保它们满足特定质量或分析标准以进行可接受的更改。   在输入数据时或之后不久自动将数据翻译为其他语言。 IoT/物联网传感器输入消息 随着连接到网络的自主设备的爆炸式增加，额外的流量体积庞大，并且使用比HTTP更轻量级的协议。 高效的云服务必须能够快速响应消息并扩展以响应其扩散或突然涌入的消息。Serverless功能可以有效地管理和过滤来自IoT设备的MQTT消息。 它们既可以弹性扩展，也可以屏蔽负载下游的其他服务。 例子包括：  GreenQ的卫生用例（垃圾互联网），根据垃圾箱的相对饱满度来优化卡车取件路线。   在物联网设备（如AWS Greengrass）上使用serverless来收集本地传感器数据，对其进行规范化，与触发器进行比较，并将事件推送到聚合单元/云。 大规模流处理 另一种非事务性，非请求/响应类型的工作负载是在可能无限的消息流中处理数据。 函数可以连接到消息源，而消息必须从事件流中读取和处理。 鉴于高性能，高弹性和计算密集型处理工作负载，这对于serverless而言非常重要。 在许多情况下，流处理需要将数据与一组上下文对象（在NoSQL或in-mem DB中）进行比较，或者将数据从流聚合并存储到对象或数据库系统中。 例子包括：  Mike Roberts有一个很好的 Java/AWS Kinesis 示例 ，可以有效地处理数十亿条消息。   SnapChat 在Google AppEngine上使用serverless 处理邮件。 聊天机器人 与人类交互不一定需要毫秒级别的响应时间，并且在许多方面，稍微延迟让回复人类的机器人对话感觉更自然。因此，等待从冷启动加载function的初始等待时间可能是可接受的。当添加到Facebook，WhatsApp或Slack等流行的社交网络时，机器人可能还需要具有极高的可扩展性，因此在PaaS或IaaS模型中预先设置一个永远在线的守护程序，以预测突然或高峰需求，可能不会有作为serverless方法的高效或成本效益。 例子包括：  支持和销售机器人插入到大型社交媒体服务，如Facebook或其他高流量网站。   消息应用程序Wuu使用Google Cloud Functions使用户能够创建和共享在数小时或数秒内消失的内容。   另请参阅下面的HTTP REST API和Web应用程序。 批处理作业或计划任务 每天只需几分钟就能以异步方式进行强大的并行计算，IO或网络访问的作业非常适合serverless。作业可以在以弹性方式运行时有效地消费他们所需的资源，并且在不被使用的当天剩余时间内不会产生资源成本。 例子包括：  计划任务可以是每晚运行的备份作业。 并行发送许多电子邮件的作业会扩展function实例。 HTTP REST API和Web应用程序 传统的请求/响应工作负载仍然非常适合serverless，无论工作负载是静态网站还是使用JavaScript或Python等编程语言按需生成响应。即使它们可能会为第一个用户带来启动成本，但在其他计算模型中存在这种延迟的先例，例如将JavaServer Page初始编译为servlet，或者启动新的JVM来处理额外的负载。好处是单个REST调用（例如，微服务中的GET，POST，UPDATE和DELETE 4端点中的每一个）可以独立扩展并单独计费，即使它们共享公共数据后端。 例子包括：  移植到serverless架构的澳大利亚人口普查显示了开发速度，成本改进和自动扩展。 “如何通过无服务器将我的AWS账单削减90％。” AutoDesk示例：“成本只占传统云方法的一小部分（约1％）。” 在线编码/教育（考试，测试等）在事件驱动的环境中运行训练代码，并基于与该训练的结果和预期结果的比较向用户提供反馈。Serverless平台根据需要运行应答检查并根据需要进行扩展，仅在代码运行的时间内需要付费。 移动后端 使用serverless进行移动后端任务也很有吸引力。它允许开发人员在BaaS API之上构建REST API后端工作负载，因此他们可以花时间优化移动应用程序，而不是扩展后端。 例子包括：优化视频游戏的图形，而不是在游戏成为病毒式打击时投资服务器; 或者对于需要快速迭代以发现产品/市场适合性，或者上市时间至关重要的消费者业务应用程序。另一个示例是批量通知用户或程序其他异步任务以获得离线优先体验。 例子包括：  需要少量服务器端逻辑的移动应用程序; 开发人员可以将精力集中在原生代码开发上。 使用已配置的安全策略（例如Firebase身份验证/规则或Amazon Cognito）通过事件触发的serverless计算使用直接从移动设备访问BaaS的移动应用程序。 “Throwaway”或短期使用的移动应用程序，例如大型会议的调度应用程序，在会议前后的周末几乎没有需求，但需要极大的扩展和收缩; 在周一和周二早上的活动过程中根据时间表查看要求，然后在午夜时分回到主题演讲。 业务逻辑 当与管理和协调function一起部署时，在业务流程中执行一系列步骤的微服务工作负载的编排是serverless计算的另一个好用例。执行特定业务逻辑的function（例如订单请求和批准，股票交易处理等）可以与有状态管理器一起安排和协调。来自客户端门户的事件请求可以由这样的协调function提供服务，并传递给适当的serverless function。 例子包括： 交易台，处理股票市场交易并处理客户的交易订单和确认。协调器使用状态图管理交易。初始状态接受来自客户端门户的交易请求，并将请求传递给微服务function以解析请求并验证客户端。随后的状态根据买入或卖出交易指导工作流，验证基金余额，股票代码等，并向客户发送确认。在从客户端接收到确认请求事件时，后续状态调用管理交易执行的function，更新账户，并通知客户完成交易。 持续集成管道 传统的CI管道包括一个构建从属主机池，它们处于空闲等待以便分派作业。Serverless是一种很好的模式，可以消除对预配置主机的需求并降低成本。构建作业由新代码提交或PR合并触发。 调用function来运行构建和测试用例，仅在所需的时间内执行，并且在未使用时不会产生成本。这降低了成本，并可通过自动扩展来减少瓶颈以满足需求。 例子包括：  Serverless CI - Hyper.sh 的 Buildbot 集成 Serverless vs. 其他云原生技术 大多数应用程序开发人员在寻找托管其云原生应用程序的平台时可能会考虑三种主要的开发和部署模型。每个模型都有自己的一组不同的实现（无论是开源项目，托管平台还是本地产品）。这三种型号通常建立在容器技术的基础上，为了密度，性能，隔离和包装特性，但容器化并不是要求。 为了增加抽象，远离运行其代码的实际基础设施，并且更加关注开发的业务逻辑，它们是Container Orchestration（或Containers-as-a-Service），Platform-as-a-Service和Serverless（Functions-as-a-Service）。所有这些方法都提供了部署云原生应用程序的方法，但它们根据其预期的开发人员和工作负载类型确定了不同功能和非功能方面的优先级。以下部分列出了每个模型的一些关键特征。 请记住，没有任何银弹可以满足所有云原生开发和部署挑战。将特定工作负载的需求与每种常见的云原生开发技术的优缺点相匹配非常重要。同样重要的是要考虑应用程序的子组件可能更适合于一种方法而不是另一种方法，因此可以采用混合方式。 Container Orchestration/容器编排 Containers-as-a-Service（CaaS） - 保持对基础设施的完全控制并获得最大的可移植性。 示例：Kubernetes，Docker Swarm，Apache Mesos。 像Kubernetes，Swarm和Mesos这样的容器编排平台允许团队构建和部署可移植应用程序，具有灵活性和对配置的控制，可以在任何地方运行，而无需为不同的环境重新配置和部署。 优势包括最大限度的控制，灵活性，可重用性以及将现有的容器化应用程序引入云中的便利性，所有这些都是可能的，因为不太自由的应用程序部署模型提供了自由度。 CaaS的缺点包括显著地增加开发人员对操作系统（包括安全补丁），负载平衡，容量管理，扩展，日志记录和监控的责任。 目标受众  希望控制其应用程序及其所有依赖项的打包和版本控制的开发人员和运维团队，确保跨部署平台的可移植性和重用。 在一组相互依赖，独立扩展的微服务中寻求高性能的开发人员。 将容器移至云端，或跨私有/公共云部署，以及具有端到端群集部署经验的组织。 开发/运维人员经验  创建Kubernetes集群，Docker Swarm堆栈或Mesos资源池（完成一次）。 在本地迭代和构建容器镜像。 将打好标记的应用程序镜像推送到注册表。 将基于容器镜像的容器部署到集群。 测试并观察生产中的应用。 优点  对于正在部署的内容，开发人员拥有最大程度的控制权和责任。使用容器编排器，可以定义要部署的确切镜像版本，配置，以及管理其运行时的策略。 控制运行时环境（例如，运行时，版本，最小OS，网络配置）。 在系统外，容器镜像具有更高的可重用性和可移植性。 非常适合将容器化应用程序和系统引入云端。 缺点  对文件系统映像和执行环境负有更多责任，包括安全补丁和分发优化。 管理负载平衡和扩展行为的更多责任。 通常更多的责任是容量管理。 通常更长的启动时间，今天。 通常对应用程序结构的看法较少，因此指导意见较少。 通常对构建和部署机制负有更多责任。 通常，对监视，日志记录和其他常见服务的集成负有更多责任。 Platform-as-a-Service Platform-as-a-Service (PaaS) - 专注于应用程序，让平台处理其余的事务。 示例：Cloud Foundry，OpenShift，Deis，Heroku Platform-as-a-Service实现使团队能够部署和扩展应用程序，通过将配置信息注入到这些应用程序，可以使用大量运行时，绑定到数据目录，AI，IoT和安全服务，而无需手动配置和管理容器和操作系统。它非常适合具有稳定编程模型的现有Web应用程序。 优点包括更轻松地管理和部署应用程序，自动扩展和预配置服务，以满足最通用的应用程序需求。 缺点包括缺乏操作系统控制、粒度容器可移植性、负载平衡、应用程序优化，还有潜在的供应商锁定，以及需要在大多数PaaS平台上构建和管理监视和日志记录功能。 目标受众  需要部署平台的开发人员，使他们能够专注于应用程序源代码和文件（不打包它们），而不必担心操作系统。 默认情况下，使用可路由主机名创建更传统的基于HTTP的服务（应用程序和API）的开发人员。 但是，一些PaaS平台现在也支持通用TCP路由。 对更为成熟的云计算模型（与serverless相比）感到满意的组织，这些模型有综合文档和许多实例的支持。 开发/运维人员经验  迭代应用程序，在本地Web开发环境中构建和测试。 将应用程序推送到PaaS，在其中构建和运行。 测试并观察生产中的应用。 更新配置以确保高可用性和扩展以匹配需求。 优点  开发人员的参考框架在应用程序代码和它连接的数据服务上。对实际运行时的控制较少，但开发人员避免了构建步骤，也可以选择扩展和部署选项。 无需管理底层操作系统。 构建包提供对运行时的影响，根据需要提供尽可能多的控制（合理的默认值）。 非常适合具有稳定编程模型的许多现有Web应用程序。 缺点  失去对操作系统的控制，可能受到构建包版本的支配。 关于应用程序结构的更多见解，倾向于 12因素 微服务最佳实践，以及架构灵活性的潜在成本。 潜在的平台锁定。 Serverless Functions-as-a-Service (FaaS) 将逻辑编写为响应各种事件的小块代码。 示例：AWS Lambda，Azure Functions，基于Apache OpenWhisk的IBM Cloud Functions，Google Cloud Functions，华为Function Stage 和 Function Graph，Kubeless，iron.io，funktion，fission，nuclio Serverless使开发人员能够专注于由事件驱动的函数组成的应用程序，这些函数响应各种触发器并让平台负责其余的事情 - 例如触发器到函数逻辑，从一个函数传递到另一个函数的信息，自动设置容器和运行时间（时间，地点和内容），自动扩展，身份管理等。 其优势包括对任何云原生范例的基础设施管理的最低要求。无需考虑操作系统或文件系统，运行时甚至容器管理。Serverless享受自动扩展，弹性负载平衡和最细粒度的“即用即付”计算模型。 缺点包括不够全面和稳定的文档，示例，工具和最佳实践; 有挑战的调试; 响应时间可能较慢; 缺乏标准化和生态系统成熟度以及平台锁定的可能性。 目标受众  希望在单个函数中更多地关注业务逻辑的开发人员，这些函数可根据需求自动扩展并将交易与成本紧密联系起来。 希望更快地构建应用程序并且更少关注运维方面的开发人员。 创建事件驱动应用程序的开发人员和团队，例如响应数据库更改，物联网读数，人工输入等。 在标准和最佳实践尚未完全建立的领域，能够轻松采用尖端技术的组织。 开发/运维人员经验  迭代函数，在本地Web开发环境中构建和测试。 将单个函数上载到serverless平台。 声明事件触发器，函数及其运行时，以及事件到函数的关系。 测试并观察生产中的应用。 无需更新配置以确保高可用性和扩展以匹配需求。 优点  开发人员的观点已经远离运维问题，如管理高可用性函数的部署，更多地转向函数逻辑本身。 开发人员可根据需求/工作量自动扩展。 利用新的“即用即付”成本模型，仅对代码实际运行的时间收费。 操作系统，运行时甚至容器生命周期都是完全抽象的（serverless）。 更适合涉及物联网，数据和消息的新兴事件驱动和不可预测的工作负载。 通常是无状态，不可变和短暂的部署。每个函数都以指定的角色和明确定义/有限的资源访问权限运行。 中间件层将得到调整/优化，将随着时间的推移提高应用程序性能。 强烈推广微服务模型，因为大多数serverless运行时强制限制每个单独函数的大小或执行时间。 易于将第三方API集成为定制的serverless API，既可以扩展使用，又可以灵活地从客户端或服务器调用。 缺点  一种新兴的计算模型，快速创新，缺乏全面和稳定的文档，示例，工具和最佳实践。   由于运行时更具动态性，与IaaS和PaaS相比，调试可能更具挑战性。   由于按需结构，如果运行时在空闲时删除函数的所有实例，则某些serverless运行时的“冷启动”方面可能有性能问题。   在更复杂的情况下（例如，触发其他函数的函数），对于相同数量的逻辑，可以存在更多的运维表面区域。   缺乏标准化和生态系统成熟度。   由于平台的编程模型，事件/消息接口和BaaS产品，有平台锁定的可能性。 应该使用哪种云原生部署模型？ 为了确定哪种模型最适合您的特定需求，应对每种方法（以及若干模型实施）进行全面评估。本节将提供一些考虑因素的建议，因为没有一个放之四海而皆准的解决方案。 评估特性和能力 体验每种方法。 从功能和开发体验的角度找到最适合您需求的方法。你正试图找到问题的答案，例如：  基于前面部分中描述的工作负载，这是serverless证明其价值的地方，我的应用程序是否适合？ 与替代方案相比，我是否预期从serverless获得重大收益而值得改变？  在运行时及其运行环境中，我真正需要多少控制？ 小的运行时版本更改会影响我吗？ 我可以覆盖默认值吗？   我可以使用我选择的语言提供的全套功能和库吗？ 如果需要，我可以安装其他模块吗？ 我是否必须自己打补丁或升级？   我需要多少运维控制？ 我是否愿意放弃容器或执行环境的生命周期？   如果我需要更改服务代码怎么办？ 我可以多快部署它？ 我如何保护我的服务？ 我必须管理吗？ 或者我可以卸载到可以做得更好的服务吗？ 评估和衡量运维方面 使用PaaS和Container Orchestrator收集性能数据，例如恢复时间，以及使用Serverless平台进行冷启动。探索并量化应用程序的其他重要非功能特性对每个平台的影响，例如： 弹性：  如何使我的应用程序适应数据中心故障？ 在部署更新时如何确保服务的连续性？ 如果我的服务失败怎么办？ 平台会自动恢复吗？ 它对最终用户是不可见的吗？ 可扩展性：  如果有突然的变化，平台是否支持自动扩展？ 我的应用程序是否设计为有效地利用无状态扩展？ 我的serverless平台是否会压倒任何其他组件，例如数据库？ 我可以管理或限制背压吗？ 性能：  每个实例或每个HTTP客户端每秒有多少个函数调用？ 给定工作负载需要多少台服务器或实例？ 从调用到响应的延迟是多少（在冷启动和热启动中）？ 微服务之间的延迟，与单个部署中的共存功能相比，是问题吗？ CNCF Serverless工作组的潜在成果之一可能是何时选择特定模型的决策框架，以及如何在给定一组推荐工具的情况下进行测试。 有关详细信息，请参阅结论部分。 评估并考虑潜在成本的全部范围 这包括开发成本和运行时资源成本。  不是每个人都可以从头开始开展他们的开发活动。 因此，需要仔细考虑将现有应用程序迁移到其中一个云原生模型的成本。虽然对容器的升降式模型看起来最便宜，但从长远来看，它可能不是最具成本效益的。 同样，从成本角度来看，serverless的按需模型非常具有吸引力，但将单体应用程序拆分为函数所需的开发工作可能令人生畏。 与依赖服务集成的成本是多少？ Serverless计算最初可能看起来最经济，但它可能需要更昂贵的第三方服务成本，或者非常快速地自动缩放，这可能导致更高的使用费。 平台免费提供哪些功能/服务？ 我是否愿意以可移植性的潜在成本购买供应商的生态系统？ 基于多平台运行应用程序 在查看当前可用的各种云托管技术时，可能并不明显，但没有理由需要将单个解决方案用于所有部署。实际上，没有理由需要在单个应用程序中使用相同的解决方案。一旦将应用程序拆分为多个组件或微服务，您就可以自由地将每个组件分别部署在完全不同的基础设施上，如果这是最符合您需求的。 同样，每个用于其特定目的微服务也可以用最佳技术（即语言）开发。 “分解单体”带来的自由带来了新的挑战，以下部分重点介绍了在选择平台和开发微服务时应该考虑的一些方面。 跨部署目标拆分组件 考虑将正确的技术与正确的工作相匹配，例如，物联网演示可能同时使用PaaS应用程序处理对连接设备仪表板的请求，以及一组 serverless 函数来处理来自设备本身的MQTT消息事件。 Serverless不是一个银弹，而是在您的云原生架构中可以考虑的新选择。 设计多个部署目标 另一种设计选择是使您的代码尽可能通用，允许在本地进行测试，并依赖于上下文信息（如环境变量）来影响它在特定环境中的运行方式。 例如，一组普通的POJO可能能够在三个主要环境中的任何一个中运行，并且可以根据可用的环境变量，类库或绑定服务来定制精确的行为。 为任意方式继续使用DevOps管道 大多数容器编排平台，PaaS实现和serverless框架都可以由命令行工具驱动，并且相同的容器镜像可以构建一次并在每个平台上重用。 考虑抽象以简化模型之间的可移植性 有越来越多的第三方项目生态系统弥补了将当前在PaaS或CaaS上运行的基于HTTP的Web应用程序移植到serverless平台的差距。 其中包括来自Serverless, Inc.和Zappa Framework的几个工具。 Serverless框架提供的适配器使得使用流行的Web应用程序框架（如Python WSGi和JAX-RS REST API）编写的应用程序能够在Serverless平台上运行。这些框架还可以提供可移植性和多个Serverless平台之间差异性的抽象。 详细信息视图：Serverless处理模型 本节总结了serverless框架中当前的函数使用，并概括了serverless 函数需求，生命周期，调用类型和所需的抽象。 我们的目标是定义serverless 函数规范，以便相同的函数可以编码一次并在不同的serverless框架中使用。 本节未定义确切的函数配置和API。 我们可以将FaaS解决方案概括为具有下图中显示的几个关键元素：  Event sources/事件源 - 触发事件或流式传输触发到一个或多个函数实例中 Function instances/函数实例 - 单个函数/微服务，可以按需扩展 FaaS Controller/FaaS控制器- 部署，控制和监视函数实例及其来源 Platform services/平台服务 - FaaS解决方案使用的一般集群或云服务（有时称为Backend-as-a-Service） 让我们首先看一下serverless环境中函数的生命周期。 函数生命周期 以下部分描述了函数生命周期的各个方面以及serverless框架/运行时通常如何管理它们。 函数部署管道 函数的生命周期从编写代码并提供规范和元数据开始（参见下面的函数定义），“builder”实体将获取代码和规范，编译并将其转换为工件（代码二进制文件，包或容器镜像）。 然后将工件部署在具有控制器实体的集群上，该控制器实体负责基于事件流量和/或实例上的负载来扩展函数实例的数量。 函数操作 Serverless框架可以允许以下动作和方法来定义和控制函数生命周期：  Create - 创建新函数，包括其规格和代码 Publish - 创建可在群集上部署的函数新版本 Update Alias/Label (版本的) - 更新别名/标签（版本） - 更新版本别名 Execute/Invoke - 调用特定版本，不通过其事件源 Event Source association - 将特定版本的函数与事件源连接 Get - 返回函数元数据和规范 Update - 修改函数的最新版本 Delete - 删除函数，可以删除特定版本或其所有版本的函数 List - 显示函数列表及其元数据 Get Stats - 返回有关函数运行时使用情况的统计信息 Get Logs - 返回函数生成的日志  在创建函数时，提供其元数据（稍后在函数规范中描述）作为函数创建的一部分，函数将被编译并可能被发布。 稍后可以启动，禁用和启用函数。函数部署需要能够支持以下用例：  Event streaming/事件流，在此用例中，队列中可能始终存在事件，而处理的暂停/恢复可能需要通过显式请求   Warm startup/热启动 - 在任何时候具有最少实例数量的函数，在接收的“first”事件时进行热启动，因为该函数已经部署并准备好为事件服务（而不是冷启动，冷启动时指函数获得通过“incoming”事件在第一次调用时部署） 用户可以发布函数，这将创建新版本（“latest”版本的副本），发布的版本可能被标记或具有别名，请参阅下文。 用户可能希望直接执行/调用函数（绕过事件源或API gateway）以进行调试和开发过程。用户可以指定调用参数，例如所需版本，同步/异步操作，详细级别等。 用户可能想要获得函数统计（例如调用次数，平均运行时间，平均延迟，失败，重试等），统计可以是当前度量值或时间序列值（例如存储在Prometheus或云提供者设施中例如AWS Cloud Watch）。 用户可能希望检索函数日志数据。这可以通过严重性级别和/或时间范围和/或内容来进行过滤。 Log数据是每个函数都有的，它包括诸如函数创建和删除，显式错误，警告或调试消息之类的事件，以及可选的函数Stdout或Stderr。倾向每次调用有一个日志条目或者将日志条目与特定调用相关联的方式（以允许更简单地跟踪函数执行流程）。 函数版本控制和别名 一个函数可能有多个版本，使用户能够运行不同版本的代码，如beta / production，A / B测试等。使用版本控制时，函数版本默认为“最新”，“最新”版本可以更新和修改，可能会触发每个此类更改的新构建过程。 一旦用户想要冻结版本，他将使用发布操作，该操作将创建具有潜在标签或别名的新版本（例如“beta”，“production”），以便在配置事件源时使用，因此事件或API调用可以路由到特定的功能版本。非最新函数版本是不可变的（它们的代码和全部或部分函数规范），并且一旦发布就不能更改;函数不能“未发布”，而应删除它们。 请注意，今天的大多数实现都不允许函数分支/ fork（更新旧版本代码），因为它使实现和使用变得复杂，但是将来可能需要这样做。 当存在相同功能的多个版本时，用户必须指定他想要操作的功能的版本以及如何在不同版本之间划分事件的流量（例如，用户可以决定路由90％的事件流量到一个稳定的版本和10％的流量到测试版又名“金丝雀发布”）。这可以通过指定确切版本或指定版本别名来实现。版本别名通常会引用特定的函数版本。 当用户创建或更新某个功能时，它可能会根据更改的性质推动新的构建和部署。 函数关联的事件源 由事件源触发的事件调用函数。函数和事件源之间存在n：m映射。每个事件源可用于调用多个函数，函数可由多个事件源触发。事件源可以映射到函数的特定版本或函数的别名，后者提供了更改函数和部署新版本的方法，而无需更改事件关联。事件源也可以定义为使用相同函数的不同版本，并定义应为每个版本分配多少流量。 在创建函数之后，或者在稍后的某个时间点，需要关联应该触发函数调用的事件源作为该事件的结果。这需要一组操作和方法，例如：  创建事件源关联   更新事件源关联   列出事件源关联 事件来源 不同类型的事件源包括：  事件和消息服务，例如：RabbitMQ，MQTT，SES，SNS，Google Pub / Sub   存储服务，例如：S3，DynamoDB，Kinesis，Cognito，Google云存储，Azure Blob，iguazio V3IO（对象/流/数据库）   端点服务，例如：物联网，HTTP网关，移动设备，Alexa，Google Cloud端点   配置存储库，例如：Git，CodeCommit   使用特定于语言的SDK的用户应用程序   计划事件 - 定期启用函数调用 虽然每个事件提供的数据可能在不同的事件源之间有所不同，但事件结构应该是通用的，能够封装关于事件源的特定信息（事件数据和元数据下的详细信息）。 函数要求 以下列表描述了函数和无服务器运行时应基于当前技术水平满足的常见需求集：  函数必须与不同事件类的底层实现分离   可以从多个事件源调用Function   每个调用方法不需要不同的函数   事件源可以调用多个函数   函数可能需要一种机制来与底层平台服务进行持久绑定，这可能是跨函数调用。函数可能是短暂的，但如果需要在每次调用时执行，例如在记录，连接和安装外部数据源的情况下，引导程序可能很昂贵。   每个函数都可以使用与在同一应用程序中使用的其他函数不同的代码语言编写   函数运行时应尽可能减少事件序列化和反序列化开销（例如，使用本机语言结构或高效编码方案） 工作流程相关要求：  函数可以作为工作流的一部分调用，其中函数的结果是另一个函数的触发器   函数可以由事件或“和/或事件组合”触发   一个事件可以触发按顺序或并行执行的多个函数   “和/或事件组合”可以触发按顺序或并行或分支运行的m个函数   在工作流程的中间，可能会收到不同的事件或函数结果，这会触发分支到不同的函数   部分或全部函数的结果需要作为输入传递给另一个函数   函数可能需要一种与底层平台服务进行持久绑定的机制，这可能是跨函数调用或函数可能是短暂的。 函数调用类型 可以根据不同的用例从不同的事件源调用函数，例如：  同步请求（Req / Rep），例如， HTTP请求，gRPC调用   客户发出请求并等待立即响应。这是一个阻止电话。   异步消息队列请求（发布/订阅），例如， RabbitMQ，AWS SNS，MQTT，电子邮件，对象（S3）更改，计划事件（如CRON job）    消息发布到交换机并分发给订户     没有严格的消息排序。完全一次处理     消息/记录流：例如Kafka，AWS Kinesis，AWS DynamoDB Streams，数据库CDC    一组有序的消息/记录（必须按顺序处理）     通常，每个分片使用单个工作程序（分片消费者）将流分片为多个分区/分片     可以从消息，数据库更新（日志）或文件（例如CSV，Json，Parquet）生成流     事件可以推送到函数运行时或由函数运行时拉动     批量作业例如ETL工作，分布式深度学习，HPC仿真    作业被调度或提交到队列，并在运行时使用并行的多个函数实例进行处理，每个函数实例处理工作集的一个或多个部分（任务）     当所有并行工作程序成功完成所有计算任务时，作业完成    函数代码 函数代码和依赖关系和/或二进制文件可以驻留在外部存储库中，例如S3对象桶或Git存储库，或者由用户直接提供。 如果代码位于外部存储库中，则用户需要指定路径和凭据。 无服务器框架还可以允许用户观察代码库以进行更改（例如，使用web钩子）并在每次提交时自动构建函数镜像/二进制文件。 函数可能依赖于外部库或二进制文件，需要由用户提供，包括描述其构建过程的方法（例如，使用Dockerfile，Zip）。 另外，可以通过一些二进制包装（例如OCI图像）将该函数提供给框架。 函数定义 无服务器函数定义可能包含以下规范和元数据，函数定义是特定于版本的：  Unique ID   Name   Description   Labels (or tags)   Version ID (and/or Version Aliases)   Version creation time   Last Modified Time (of function definition)   Function Handler   Runtime language   Code + Dependencies or Code path and credentials   Environment Variables   Execution Role and Secret   Resources (Required CPU, Memory)   Execution Timeout   Log Failure (Dead Letter Queue)   Network Policy / VPC   Data Bindings 元数据详细信息 函数框架可以包括以下函数元数据：  版本 - 每个函数版本应该具有唯一标识符，此外版本可以使用一个或多个别名标记（例如“最新”，“生产”，“测试版”）。 API网关和事件源会将流量/事件路由到特定的函数版本。   环境变量 - 用户可以指定将在运行时提供给函数的环境变量。环境变量也可以从秘密和加密内容中导出，或者从平台变量中导出（例如像Kubernetes EnvVar定义）。环境变量使开发人员能够控制函数行为和参数，而无需修改代码和/或重建函数，从而允许更好的开发人员体验和函数重用。   执行角色 - 该函数应在特定用户或角色身份下运行，以授予和审核其对平台资源的访问权限。   资源 - 定义所需或最大的硬件资源，例如函数使用的内存和CPU。   超时 - 指定函数调用可以运行的最长时间，直到平台终止。   失败日志（死信队列） - 一个队列或流的路径，它将存储具有适当详细信息的失败函数执行列表。   网络策略 - 分配给该函数的网络域和策略（用于与外部服务/资源通信的函数）。   执行语义 - 指定如何执行函数（例如，每个事件至少一次，最多一次，恰好一次）。 数据绑定 一些无服务器框架允许用户指定函数使用的输入/输出数据资源，这使开发人员简化，性能（在执行之间保留数据连接，可以预取数据等），以及更好的安全性（数据资源）凭证是上下文的一部分，而不是代码）。 绑定数据可以是文件，对象，记录，消息等形式。函数规范可以包括数据绑定定义的数组，每个数据绑定定义指定数据资源，其凭证和使用参数。数据绑定可以引用事件数据（例如，数据库密钥是从事件“用户名”字段派生的），请参阅以下内容：https：//docs.microsoft.com/azure/azure-functions/functions-triggers-bindings。 函数输入 函数输入包括事件数据和元数据，并且可以包括上下文对象。 事件数据和元数据 事件细节应该传递给函数处理程序，不同的事件可能具有不同的元数据，因此希望函数能够确定事件的类型并轻松地解析公共和特定于事件的元数据。 可能需要将事件类与实现分离，例如：无论流存储是Kafka还是Kinesis，处理消息流的函数都将起作用。在这两种情况下，它将接收消息正文和事件元数据，该消息可以在不同框架之间路由。 事件可以包括单个记录（例如，在请求/响应模型中），或者接受多个记录或微批（例如，在流模式中）。 FaaS解决方案使用的常见事件数据和元数据的示例：  活动类/种类   版本   活动ID   事件源   来源身份   内容类型   消息正文   时间戳 事件/记录特定元数据的示例  HTTP：路径，方法，标题，查询参数   消息队列：主题，标题   记录流：表，键，操作，修改时间，旧字段，新字段 事件源结构的示例： http://docs.aws.amazon.com/lambda/latest/dg/eventsources.html https://docs.microsoft.com/azure/azure-functions/functions-triggers-bindings https://cloud.google.com/functions/docs/concepts/events-triggers 一些实现关注于JSON作为向事件传递事件信息的机制。这可能为更高速度的函数（例如流处理）或低能量设备（IoT）增加大量的序列化/解串行化开销。在这些情况下，可能需要考虑本机语言结构或其他序列化机制作为选项。 函数上下文 调用函数时，框架可能希望提供对跨多个函数调用的平台资源或常规属性的访问，而不是将所有静态数据放在事件中或强制函数在每次调用时初始化平台服务。 函数上下文作为一组输入属性，环境变量或全局变量提供。一些实现使用所有三种的组合。 上下文的示例：  函数名称，版本，ARN   内存限制   请求ID   云区   环境变量   安全密钥/令牌   运行时/ Bin路径   日志   数据绑定 某些实现初始化日志对象（例如，作为AWS中的全局变量或Azure中的部分上下文），使用日志对象，用户可以使用集成平台工具跟踪函数执行。除了传统的日志记录之外，未来的实现可以将计数器/监视和跟踪活动抽象为平台上下文的一部分，以进一步提高函数的可用性。 数据绑定是函数上下文的一部分，平台基于用户配置启动与外部数据资源的连接，并且这些连接可以跨多个函数调用重用。 函数输出 当函数退出时，它可以：  将值返回给调用者（例如，在HTTP请求/响应示例中）   将结果传递到工作流中的下一个执行阶段   将输出写入日志 应该有一种确定的方法来通过返回的错误值或退出代码来了解函数是成功还是失败。 函数输出可以是结构化的（例如HTTP响应对象）或非结构化的（例如某些输出字符串）。 无服务器函数工作流程 在无服务器域中，用例属于以下类别之一：  一个事件触发一个函数   事件和/或事件组合触发一个函数   一个事件触发按顺序或并行执行的多个函数   该函数的结果可能是另一个函数的触发器   N个事件（在和/或中）触发m个函数，即例如事件 - 函数交错的工作流。 event1触发function1，完成function1以及event2和event3触发function2，然后function2的不同结果触发分支到function3或function4。 用户需要一种方法来指定其无服务器用例或工作流。例如，一个用例可能是“当照片上传到云存储上时，对照片进行面部识别（照片存储事件发生）。”当接收到运动检测事件时，另一个物联网用例可以是“进行运动分析”，然后根据分析函数的结果，“触发房屋警报加上对警察部门的呼叫”或者只是“发送运动图像到房主。“有关详细信息，请参阅用例部分。 AWS为用户提供“步骤函数”原语（基于状态机的原语）以指定其工作流程，但步骤函数不允许指定触发工作流中的哪些函数的事件/事件。请参阅https://aws.amazon.com/step-functions/。 下图是用户工作流程的示例，涉及事件和函数。使用这样的函数图，用户可以轻松指定事件和函数之间的交互，以及如何在工作流中的函数之间传递信息。 函数图状态包括以下内容： 事件状态此状态允许等待事件源中的事件，然后触发函数运行或多个函数按顺序或并行或在分支中运行。 操作/任务状态此状态允许按顺序或并行运行一个或多个函数，而无需等待任何事件。 切换/选择状态该状态允许转换到多个其他状态（例如，先前的函数结果触发分支/转换到不同的下一状态）。 结束/停止状态此状态使用“失败/成功”终止工作流程。 通过状态此状态在两个状态之间注入事件数据。 延迟/等待状态此状态使工作流程执行延迟指定的持续时间或直到指定的时间/日期。 需要将状态和相关信息保存在某些持久存储中以进行故障恢复。在一些用例中，用户可能希望将来自一个州的信息传递到下一个状态。该信息可以是函数执行结果的一部分或与事件触发器相关联的输入数据的一部分。需要在每个状态定义信息过滤器以过滤掉需要在状态之间传递的信息。 结论 无服务器架构为云原生工作负载提供了令人兴奋的新部署选项。正如我们在无服务器工作负载部分中看到的，某些用例中无服务器技术提供了超过其他云托管技术的主要优势。 但是，无服务器技术并不适合所有情况，应该在适当的时候仔细考虑。短期的，事件驱动的处理正在推动早期采用和使用案例，这些企业预计会出现具有不可预测的容量和基础架构需求的高变化率。有关无服务器计算的更多阅读材料和见解，请参阅其他参考资料部分。 CNCF无服务器工作组与Redpoint Ventures合作，最近发布了无服务器景观。它说明了生态系统中可用的一些主要的无服务器项目，工具和服务。它无意代表一个全面的，完全包容的无服务器生态系统，也不是一种认可，而只是对景观的概述。预计每个人的所有者将提供更新以试图使其保持最新。 CNCF的后续步骤 关于CNCF应该考虑在这个领域做什么，为技术监督委员会的考虑提供以下建议：  鼓励更多无服务器技术供应商和开源开发人员加入CNCF，分享想法，并在彼此的创新基础上再接再厉。例如，更新无服务器横向文档中列出的开源项目并维护函数矩阵。   通过建立可互操作的API，确保开放式生态系统，确保与供应商承诺和开源工具的可互操作实施。在平台提供商和第三方开发人员库创建者的帮助下，类似于CSI和CNI的新互操作性和可移植性工作。其中一些可能值得他们自己的CNCF工作组，或者可能继续作为无服务器工作组的一项倡议。例如：    事件：定义公共事件格式和API以及元数据。一些初始提案可以在Serverless WG github repo中找到。     部署：利用现有的CNCF成员（也是无服务器提供商），启动一个新的工作组，探索可用于协调一组通用函数定义元数据的可能小步骤。例如：    应用程序定义清单，例如AWS SAM和OpenWhisk Packaging Specification。      跨不同提供商的无服务器平台的函数工作流。有许多使用场景超出触发单个函数的单个事件，并且将涉及按顺序或并行执行的多个函数的工作流程，并且由事件的不同组合+工作流的前一步骤中的函数的返回值触发。如果我们可以定义开发人员可以用来定义其用例工作流的一组通用构造，那么他们将能够创建可以在不同的无服务器平台上使用的工具。这些构造指定事件和函数之间的关系/交互，工作流中函数之间的关系/交互以及如何将信息从一个函数传递到下一个步骤函数等。一些示例是AWS步骤函数构造和华为函数图/工作流程构造。     培养开源工具生态系统，加快开发人员的采用和速度，探索关注的领域，例如：    仪表     可调试     教育：为新用户提供一套设计模式，参考架构和通用词汇表。    术语表：以公布的形式保留术语表（附录A），并确保工作组文档始终如一地使用这些术语     用例：维护用例列表，按常用模式分组，创建共享的高级词汇表。支持以下目标：      对于不熟悉无服务器平台的开发人员：增加对常见用例的理解，确定好的入口点       对于无服务器提供商和库/框架作者，便于考虑共同需求        CNCF GitHub仓库中的示例应用程序和开源工具，优先突出互操作性方面或链接到每个提供商的外部资源。     提供有关如何评估无服务器架构相对于CaaS或PaaS的函数和非函数特性的指导。这可以采用决策树的形式，也可以从CNCF项目系列中推荐一套工具。   提供有关无服务器安全主题的指导，例如：安全无服务器开发指南，强化无服务器部署，充分的安全日志记录和监视以及相关工具和过程（请参阅无服务器体系结构中十大最关键的安全风险）。   开始CNCF输出的过程（对于上面提到的建议文档），例如来自无服务器工作组和存储工作组，在GitHub中作为Markdown文件存在，可以随着时间的推移进行协作维护，这一点尤为重要 这个领域的创新速度。 参考资料 [1] CNCF Serverless Whitepaper ",
      "url"      : "https://y2p.cc/2019/03/05/serverless-cncf-whitepaper/",
      "keywords" : "Serverless"
    } ,
  
    {
      "title"    : "Serverless 典型使用场景",
      "category" : "Serverless",
      "content": " serverless典型使用场景 后端 AWS Lambda：您可以使用 AWS Lambda 构建无服务器后端，以处理 Web、移动、物联网 (IoT) 和第 3 方 API 请求。 Web 应用程序后端 Azure的描述 从队列中获取在线订单，并进行处理，生成的数据保存在数据库中。 分析:  需要通过 Service Bus ，处理结果存储到Azure自家的 Cosmos DB。 Aws Lambda的描述 通过将 AWS Lambda 与其他 AWS 服务相结合，开发人员可以构建功能强大的 Web 应用程序，从而可自动扩展和收缩，并跨多个数据中心在高可用配置中运行，而无需在可扩展性、备份或多数据中心冗余方面执行任何管理工作。 实际案例：Bustle 使用 AWS Lambda 和 Amazon API Gateway 为其 Bustle iOS 应用程序和网站运行了一个无服务器后端。借助无服务器架构，Bustle 无需管理基础设施，因此每位工程师都能够集中精力构建新功能并进行创新。 场景分析：  web请求通过REST走API Gateway进来，触发Lambda Lambda返回数据给请求 标准的对BaaS后端的请求 移动应用程序后端 Azure的描述 同事之间通过使用手机银行支付午餐费用：垫付午餐费用的人通过移动应用请求付款，这将在其他同事的手机上触发通知。 分析：  跳过了Service Bus，应该只是图上忽略了吧？ 同样处理结果进Cosmos DB 存Cosmos DB时再次出发其他function 继续通过使用 Notifications Hub 发送通知 Aws Lambda的描述 您可以使用 AWS Lambda 和 Amazon API Gateway 构建后端，以验证和处理 API 请求。Lambda 能够轻松创造丰富、个性化的应用程序体验。 实际案例：Bustle 使用 AWS Lambda 和 Amazon API Gateway 为其 Bustle iOS 应用程序和网站运行了一个无服务器后端。借助无服务器架构，Bustle 无需管理基础设施，因此每位工程师都能够集中精力构建新功能并进行创新。 场景分析：  移动应用程序更新状态，通过REST请求走API Gateway进来，触发Lambda Lambda处理并发起状态更新通知，push给Amazon SNS IOT 后端 Aws Lambda的描述 您可以使用 AWS Lambda 构建无服务器后端，以处理 Web、移动、物联网 (IoT) 和第 3 方 API 请求。 场景分析：  传感器发送数据给Kinesis流数据处理平台 Kinesis捕获传感器数据并流式传输传感器数据，触发Lambda 数据处理 Aws lambda：您可以使用 AWS Lambda 执行代码以响应数据更改、系统状态变化或用户操作等触发器。Lambda 可以由 S3、DynamoDB、Kinesis、SNS 和 CloudWatch 等 AWS 产品直接触发，也可以通过 AWS Step Functions 编排到工作流程中。您可借此构建各种实时的无服务器数据处理系统。 数据库更改或更改数据捕获（CDC） CNCF白皮书：在此场景中，当从数据库插入，修改或删除数据时调用function。在这种情况下，它的功能类似于传统的SQL触发器，几乎就像是与主同步流并行的副作用或动作。其结果是执行一个异步逻辑，可以修改同一个数据库中的某些内容（例如记录到审计表），或者依次调用外部服务（例如发送电子邮件）或更新其他数据库，例如 DB CDC（更改数据捕获）用例的情况。 由于业务需要和处理变更的服务分布的原因，这些用例的频率以及对原子性和一致性的需要可能不同。 例子包括：  审核对数据库的更改，或确保它们满足特定质量或分析标准以进行可接受的更改。 在输入数据时或之后不久自动将数据翻译为其他语言。 实时文件处理 Azure的描述 病历以 PDF 文件形式安全上传。然后使用 OCR 检测分解和处理该数据，并将其添加到数据库以便查询。 分析：  触发来自Blob Storage function进行处理，转交给OCR处理的服务 再将结构化信息存储在数据库中 Aws Lambda的描述 您可以使用 Amazon S3 触发 AWS Lambda，以便在上传数据后立即对它们进行处理。例如，您可以使用 Lambda 实时创建缩略图、转换视频代码、建立文件索引、处理日志、验证内容以及聚合和筛选数据。 实际案例：西雅图时报利用 AWS Lambda 来调整图像大小，以便于在不同的设备（如台式机、平板电脑和智能手机）上查看。 场景分析：  相机拍照，图片上传，触发Lambda （同样是Blob Storage触发） 对图片进行处理如修改大小 但这里没有说处理之后的图片如何存储，应该也是类似的存储在s3中 实时流式处理 Azure的描述 从大量云应用收集到大量遥测数据。近乎实时地处理该数据，并将其存储到 DB 中，供分析仪表板使用。 分析：  应用或者设备产生数据，作为遥测信息收集 function处理遥测数据，将结果发送给 Cosmos DB 按照实际经验，这里的function应该会是多次流处理，不会简单的一个function Aws Lambda的描述 您可以使用 AWS Lambda 和 Amazon Kinesis 处理实时流数据，从而跟踪应用程序活动、处理事务处理顺序、分析单击数据流、整理数据、生成指标、筛选日志、建立索引、分析社交媒体以及遥测和计量 IoT 设备数据。 实际案例：Localytics 可实时处理数十亿个数据点，并使用 Lambda 来处理存储在 S3 中或从 Kinesis 进行流式处理的历史和活动数据。 场景分析：  社交媒体数据实时上载，触发Lambda Lambda生成趋势数据，存储到 DynamoDb 趋势数据可以被立即查看 提取、转换、加载 Aws Lambda的描述 您可以使用 AWS Lambda 针对 DynamoDB 表中的每个数据更改执行数据验证、筛选、排序或其他转换，并将转换后的数据加载到其他数据存储。 实际案例：Zillow 使用 Lambda 和 Kinesis 实时跟踪移动指标子集。凭借 Kinesis 和 Lambda，我们得以在两周内开发和部署一套成本高效的解决方案。 场景分析：  订单操作更新数据库，触发Lambda Lambda进行转换，将数据存储到数据仓库 计划任务的自动化 Azure的描述 每 15 分钟对客户数据库执行一次分析，检查是否有重复条目，避免将多个通信发送给同一个客户。 分析：  function定时调度，定期清理数据 CNCF白皮书的描述 批处理作业或计划任务: 每天只需几分钟就能以异步方式进行强大的并行计算，IO或网络访问的作业非常适合serverless。作业可以在以弹性方式运行时有效地消费他们所需的资源，并且在不被使用的当天剩余时间内不会产生资源成本。 例子包括：  计划任务可以是每晚运行的备份作业。 并行发送许多电子邮件的作业会扩展function实例。 扩展 SaaS 应用程序 Azure的描述 SaaS 解决方案通过 webhook（可通过 Functions 实现）提供扩展性，以自动执行某些工作流。 分析：  外部活动，通过web hook触发function function进行处理，或者通过事件继续触发其他function 持续集成管道 CNCF白皮书：传统的CI管道包括一个构建从属主机池，它们处于空闲等待以便分派作业。Serverless是一种很好的模式，可以消除对预配置主机的需求并降低成本。构建作业由新代码提交或PR合并触发。 调用function来运行构建和测试用例，仅在所需的时间内执行，并且在未使用时不会产生成本。这降低了成本，并可通过自动扩展来减少瓶颈以满足需求。 业务逻辑 CNCF白皮书：当与管理和协调function一起部署时，在业务流程中执行一系列步骤的微服务工作负载的编排是serverless计算的另一个好用例。执行特定业务逻辑的function（例如订单请求和批准，股票交易处理等）可以与有状态管理器一起安排和协调。来自客户端门户的事件请求可以由这样的协调function提供服务，并传递给适当的serverless function。 例子包括： 交易台，处理股票市场交易并处理客户的交易订单和确认。协调器使用状态图管理交易。初始状态接受来自客户端门户的交易请求，并将请求传递给微服务function以解析请求并验证客户端。随后的状态根据买入或卖出交易指导工作流，验证基金余额，股票代码等，并向客户发送确认。在从客户端接收到确认请求事件时，后续状态调用管理交易执行的function，更新账户，并通知客户完成交易。 聊天机器人 CNCF白皮书：与人类交互不一定需要毫秒级别的响应时间，并且在许多方面，稍微延迟让回复人类的机器人对话感觉更自然。因此，等待从冷启动加载function的初始等待时间可能是可接受的。当添加到Facebook，WhatsApp或Slack等流行的社交网络时，机器人可能还需要具有极高的可扩展性，因此在PaaS或IaaS模型中预先设置一个永远在线的守护程序，以预测突然或高峰需求，可能不会有作为serverless方法的高效或成本效益。 例子包括：  支持和销售机器人插入到大型社交媒体服务，如Facebook或其他高流量网站。 消息应用程序Wuu使用Google Cloud Functions使用户能够创建和共享在数小时或数秒内消失的内容。 另请参阅下面的HTTP REST API和Web应用程序。 参考资料 [1] Serverless Architectures: 非常经典，中文翻译版本见 Serverless架构综述 [2] Serverless architecture@technology radar: ThoughtWorks在技术雷达中对serverless的定义 [3] 所谓Serverless，你理解对了吗？ [4] InfoQ虚拟研讨会：无服务器计算的实践方法 [5] 姗姗来迟的Serverless如何助力微服务和DevOps [6] Serverless云函数架构精解 [7] What makes serverless architectures so attractive? [8] 基于Kubernetes的原生 Serverless 框架 Kubeless实践 ",
      "url"      : "https://y2p.cc/2019/03/06/serverless-usage/",
      "keywords" : "Serverless"
    } ,
  
    {
      "title"    : "UPF",
      "category" : "UPF",
      "content": "User Plane Function UPF 在 5G 中的位置 UPF（User Plane Function，用户面功能），是 3GPP 定义的 5G Core（核心网）基础设施系统架构的基本组成部分。 UPF 从 4G EPC CUPS 演进而来，4G EPC CUPS 将 PGW 分离为 PGW-C 和 PGW-U。使 PGW-U 在更接近网络边缘的地方执行数据包处理以及流量聚合，即提高带宽效率，同时减少网络阻塞。 而处理信令业务的 PGW-C 则依旧位于 MME 的北向。CUPS 的主要目标是支持 5G 实现新的无线接入（NR）实现，满足物联网应用和更高的数据传输速率需求。 然而要彻底实现 CP 和 UP 的分离是一项复杂的工程，所以 5G Core 利用 UPF 的优势来实现了网络切片（Network Slicing）的设计。 3GPP 23.501 是这样定义 UPF 的：  作为移动基础设施（Mobile Infrastructure，例如：RAN）和 DN（Data Network）之间的互连点，完成 UP 上 GTP-U（GRPS 隧道协议）协议的封装和解封装。   用于 RAT（Radio Access Technologies，无线接入技术）内部、或 RAT 之间移动性的 PDU（Protocol Data Unit，协议数据单元）会话锚点（Session Anchor Point），包括向 gNB（NG-RAN 节点）发送一个或多个 End 标记包（End Marker Packets）。   分组路由和转发：当 UPF 在多个 PSA（PDU session anchor）之间时，即 UPF 作为 I-UPF（Intermediate UPF，中间 UPF）时，会充当上行链路分类器（Uplink Classifier，UL-CL）和分支点的角色以支持多宿主 PDU 会话。作为前者时，会基于 traffic matching filters 将数据流导向到特定 DN。   数据包检查：使用 SDF（Service Data Flow，服务数据流）的 traffic filter templates 或者接收从 SMF 发出的 3-tuple（协议、以及服务端的 IP 地址和端口）PDF（Packet Flow Description）来对应用流程进行检测。   用户平面部分策略规则实施，例如门控，重定向，流量转向。   提供 UP 的 Pre-Flow QoS 功能，对包括上行链路（UL）和下行链路（DL）的传输层数据段实施速率限制（Rate Limiting）以及分组标记，以及在 DL 上处理反射 QoS（DSCP）标记，在 UL 上处理流量验证（SDF 到 QoS 流量映射）。   为计费（Billing）以及合法拦截（LI，Lawful Intercept）提供 UP 流量收集接口以及流量使用报告。  NOTE： 并非所有上述的 UPF 功能清单都需要在网络切片的 UPF 的实例中得到支持。 与 UPF 相关的由 4 个标准参考点：  N3：RAN（gNB）和 Intermediate UPF 之间的接口 N9：两个 UPF 之间的接口，例如：I-UPF 和 UPF of PDU Session Anchor（简称：锚 UPF）之间 N6：DN 和 UPF 之间的接口，即连接外部 PDU 和 DN N4：SMF 和 UPF 之间的接口  在 3GPP R16 系统架构方面增加了 I-SMF，解决了用户移动出当前 SMF 服务区的时候，如何保证业务连续性。 流程方面增加了 I-SMF 插入、修改和删除过程，以及 I-SMF 如何建立用户面的分流路径。 该功能对于国内运营商来说比较重要，能够保证用户在跨省移动的时候保证业务连续性。在该功能中也增强了 UPF 分配用户 IP 地址。   其中，N3 和 N9 参考点上采用的协议是带 5G 扩展协议头的 GTP 协议、分段路由协议（SRV6 或 NSH）以及 ICN（Information Centric Networking）协议。 此外，还有 GTP 协议之上（Over）的 LISP-DP（Locator/ID Separation data plane protocol，位置/身份分离数据平面协议）和 ILA（Identifier Locator Addressing，标识符定位寻址）。 由 I-UPF 来完成中继，并在锚 UPF 之上终止这些协议。 UPF 通过 N4 参考点从 SMF 接收 user plane traffic flow，并最终根据 rules 实现数据分流。 N4 采用了 PFCP（Packet Forwarding Control Protocol，分组交换控制协议），该协议类似于 OpenFlow，也可以用于 CUPS SX 参考点。 外部可以使用 UPF 建立的 PFCP Session 来识别（PDRs，Packet Detection Rules，数据包发现规则）、转发（FARs，Forwarding Action Rules）、处理（BARs，Buffering Action Rules）、标记（QERs，QoS Enforcement Rules）、报告（URRs，Usage Reporting Rules）数据包。 为了满足 5G 实例化以及上/下缩放的基本要求，UPF 必须使用现代微服务方法实现为纯云原生网络功能，并且可以在无服务器框架内部署。 为了通过复杂的流水线和低延迟实现高数据包吞吐量，从历史上看，使用专用硬件和定制芯片就已经实现。 在共享资源上交付云原生 UPF 需要解决复杂的实时数据处理问题，并在管道中提供高度的灵活性，以支持新兴的流量隧道，移动性和基础架构覆盖。 UPF实例化还必须高度自动化和精心安排。这意味着开发或增强用户空间数据平面加速，运行时可编程性技术，并与众多云编排系统（例如Kubernetes）紧密集成。 用户平面功能是支持新一代基于服务的体系结构的重要新组件。但是，在这一领域的创新速度也应使大多数有远见的 CUPS 采用者能够在过渡 PGW / SGW-U 解决方案上采用UPF，从而平滑了从4G到5G的迁移。 Open UPF 为了不断拓展 5G 垂直行业新兴市场，促进 UPF 朝着更加开放、灵活的方向发展，共同推动 N4 接口解耦，中国移动启动 OpenUPF ——面向行业的 5G UPF 及 N4 接口开放合作伙伴计划，致力于打造可靠、可管、灵活、开放的UPF，构建开放产业生态，助力5G赋能垂直行业。 产业界积极响应，华为、中兴、爱立信、诺基亚和英特尔等25个行业伙伴共同参与该计划。 在合作伙伴的支持下，中国移动按照既定目标发布《面向垂直行业的 N4 解耦 UPF 设备规范》和《面向垂直行业的 N4 接口规范》两项规范，同时发布《5G OpenUPF 白皮书》,向开放行业 UPF、助力 5G 赋能垂直行业迈出重要一步。 统一架构 有序推进 N4 解耦 N4 解耦是 Open UPF 计划的第一步，主要面向垂直行业边缘侧部署的 UPF 设备。 此次发布的《面向垂直行业的 N4 解耦 UPF 设备规范》，制定功能分级分类，要求 UPF 必须支持的最小功能集（14类），保证 UPF 基本业务正常进行，推荐支持 ULCL 分流等 12 类功能，满足不同垂直行业的定制化需求，后续根据需求不断迭代增加。设备需集软硬件于一体，具备虚拟化能力，分档分级，同时满足核心网设备可管可靠可控的要求。 《面向垂直行业的 N4 接口规范》基于功能的明晰和分级分类，消除理解差异，统一接口流程和接口信源。以 3GPP R15 版本为基础，同时引入 3GPP R16 个别重要信元，要求开发时以设备规范为标准，按需支持推荐功能对应 IE。将接口 IE 细分为 118 项标准 IE 和 7 项定制 IE，尽量重用并保留扩展性。定制 IE 兼容现有流程和标准 IE，标准 IE 以扩展取值为主，并尽量减少对控制面（SMF）的影响。 面向行业应用场景，需要轻量化、低成本、灵活部署的 UPF。 当前，UPF 与 SMF 的接口（N4）尚未完全开放，服务化能力尚未完全实现，一定程度上影响了 5G 响应行业客户需求的能力。运营商网络核心侧的 UPF 需要承载面向全网的业务、用户数为百万级以上，业务功能要求全、容量和性能要求高。 作为核心网的关键设备，系统级的 UPF 部署和维护成本相对较高。 N4 接口的非标准化，造成 UPF 与 SMF 同厂商的绑定，无法满足边缘用户侧 UPF 轻量化、低成本和灵活的部署需求。 中国移动提出的 OpenUPF 合作伙伴计划从开放接口、开放设备、开放服务和开放智能四个方面定义可靠、可管、可信、简洁、灵活的UPF。 计划通过构建完整的技术体系推动产业成熟、增强网络能力、助力 5G 服务垂直行业用户。 垂直赋能 加速 5G 融入百业 UPF 是连接运营商和垂直行业的桥梁，担负着数据流量处理、路由等核心功能。随着 5G 边缘计算的拓展，UPF 已逐渐从运营商的核心层走向行业客户的接入层，成为运营商服务垂直行业的第一窗口。 据了解，0penUPF 计划的一个重要目标是以“全集” UP F为基础、定义简单高效的“最简” UPF，通过最简 UPF 的功能满足高效灵活的部署，降低 5G 进入千行百业的门槛。 OpenUPF计划的第二个重要目标是满足行业差异化需求、探索功能定制的“增量” UPF，通过增量提升产业价值，同时避免碎片化的定制需求带来研发和维护成本的上升。 从不同场景的差异化需求可以看出，仅仅满足基础功能的 UPF 是不够的，UPF 还需进一步增强定制化能力，这一方面有赖于标准的演进，另一方面有赖于新的开放服务模式。 参考资料  SMF/UPF拓扑增强   What is the 5G User Plane Function (UPF)?   推进N4解耦,完善UPF规范:中国移动加速5G垂直行业落地   推进N4接口开放：中国电信在下一盘什么大棋?   中国移动研究院：5G OpenUPF 白皮书 （第一版）         ",
      "url"      : "https://y2p.cc/2019/11/11/upf/",
      "keywords" : ""
    } ,
  
    {
      "title"    : "MEC",
      "category" : "MEC",
      "content": "五个边缘计算开源框架功能对比       EdgeX Foundry  K3S  KubeEdge  StarlingX  Baetyl     云边协同  不支持  不支持  支持  支持  支持    原生支持K8S  不支持  支持  支持  不支持  不支持    边缘组件资源占用  中  小  最小（内存256M）  较大  较大    部署复杂度  复杂  简单  简单  复杂  复杂    是否去中心化  否  否  是  否  否    是否支持MQTT  支持  支持  支持  支持  支持    容器化编排  不支持  支持  支持  支持  不支持   五个边缘计算开源框架的简介： EdgeX Foundry Linux基金组织的开源项目。 github 开源地址 偏重于端侧设备的管理，定位是通用工业IOT边缘计算通用框架，提供了一些设备接入、边缘数据传输等场景的实现，但不具备云上对边缘端的应用和设备的管控、云边协同等智能边缘系统的能力，架构组件之间依赖复杂。 K3s Rancher Labs的开源产品。 github 开源地址 K3s是在边缘运行整个K8s集群的方案，不具备云边协同的能力；其次K3s虽然对K8s做了轻量化，但整体资源要求仍然较高，无法运行在IOT Hub、工业网关等小型设备中。 K3S是CNCF官方认证的Kubernetes发行版，开源时间较KubeEdge稍晚。K3S专为在资源有限的环境中运行Kubernetes的研发和运维人员设计，目的是为了在x86、ARM64和ARMv7D架构的边缘节点上运行小型的Kubernetes集群。 事实上，K3S就是基于一个特定版本Kubernetes（例如：1.13）直接做了代码修改。K3S分Server和Agent，Server就是Kubernetes管理面组件 + SQLite和Tunnel Proxy，Agent即Kubernetes的数据面 + Tunnel Proxy。 为了减少运行Kubernetes所需的资源，K3S对原生Kubernetes代码做了以下几个方面的修改： 删除旧的、非必须的代码。K3S不包括任何非默认的、Alpha或者过时的Kubernetes功能。除此之外，K3S还删除了所有非默认的Admission Controller，in-tree的cloud provider和存储插件； 整合打包进程。为了节省内存，K3S将原本以多进程方式运行的Kubernetes管理面和数据面的多个进程分别合并成一个来运行； 使用containderd替换Docker，显著减少运行时占用空间； 引入SQLite代替etcd作为管理面数据存储，并用SQLite实现了list/watch接口，即Tunnel Proxy； 加了一个简单的安装程序。 K3S的所有组件（包括Server和Agent）都运行在边缘，因此不涉及云边协同。如果K3S要落到生产，在K3S之上应该还有一个集群管理方案负责跨集群的应用管理、监控、告警、日志、安全和策略等，遗憾的是Rancher尚未开源这部分能力。 KubeEdge 华为开源产品 github 开源地址 打通了云、边、端的整体流程：  · 用户能够在云上统一管理边缘节点上的应用、设备  · 提供了云边协同的能力，能够同步云边的应用、设备的数据  · 针对复杂多样的边缘设备，KubeEdge定义了一套通用的设备管理API（K8s CRD）以及设备协议解耦层，用户可以方便地使用KubeEdge在云上管理各种边缘设备  · 针对云边网络不稳定的情况，提供了云边数据协同的可靠性传输、边缘元数据持久化  · 针对边缘资源不足的情况，轻量化裁剪了Kubelet，支持在256MB的小型设备上运行 1.关于部署： kubeEdge 包括 cloud 和 edge 部分，在 kubernetes 构建，在 cloud 与 edge 端提供核心的基础支持，比如网络，应用，部署以及元数据的同步等。 安装kubeEdge 需要安装 kubernetes 集群，cloud 与 edge 部分 cloud side： docker, kubernetes cluster and cloudcore. edge side：docker, mqtt and edgecore. 2.kubeedge 组件： KubeEdge的边缘进程包含以下5个组件： edged是个重新开发的轻量化Kubelet，实现Pod，Volume，Node等Kubernetes资源对象的生命周期管理； metamanager负责本地元数据的持久化，是边缘节点自治能力的关键； edgehub是多路复用的消息通道，提供可靠和高效的云边信息同步； devicetwin用于抽象物理设备并在云端生成一个设备状态的映射； eventbus订阅来自于MQTT Broker的设备数据。 KubeEdge的云端进程包含以下2个组件： cloudhub部署在云端，接收edgehub同步到云端的信息； edgecontroller部署在云端，用于控制Kubernetes API Server与边缘的节点、应用和配置的状态同步。 3.支持的特性： • Replace data exchange format between cloud and edge from json to protobuf. • Support reliable message delivery from cloud to edge. • Evaluate gRPC for cloud to edge communication. • Support CSI for persistent storage (using PV/PVC/StorageClass) at edge. • Support ingress at edge. • Add admission-webhook based validation for device CRDs. • Enhance performance and reliability of KubeEdge infrastructure. • Upgrade Kubernetes dependencies in vendor to v1.15. • Migrate to Go module for dependency management. • Improve contributor experience by defining project governance policies, release process, membership rules etc. • Improve the performance and e2e tests with more metrics and scenarios. 4.未来版本将支持的特性： • Support edge-cloud communication using edgemesh. • Add Layer 4 proxy support in edgemesh. • Istio-based service mesh across Edge and Cloud where micro-services can communicate freely in the mesh. • Enable function as a service at the Edge. • Support more types of device protocols such as OPC-UA, Zigbee. • Evaluate and enable much larger scale Edge clusters with thousands of Edge nodes and millions of devices. • Enable intelligent scheduling of applications to large scale Edge clusters. • Data management with support for ingestion of telemetry data and analytics at the edge. • Security at the edge. • Support for monitoring at the edge. 5.功能原理介绍： 1）KubeEdge的云边协同通信测试过包括Grpc、WebSocket、Quic，最后发现WebSocket是性能最好的，所以默认采用了WebSocket。Quic作为备选项，在网络频繁断开等很不稳定场景有优势。KubeEdge云边消息传递是通过EdgeHub跟CloudHub间的Websocket或Quic协议的长连接传递的。 2）KubeEdge会将边缘收到的应用、设备元数据都进行本地持久化。相比Kubelet在内存中缓存对象的方式，可以有效保证节点离线、故障恢复时的业务自治和快速自愈。 3）edgemesh组件实现边缘节点之间的pod通信和边缘pod到云端pod的通信，但是目前还不支持云端pod到边缘侧pod的通信。 StarlingX Intel和WindRiver开源的边缘计算项目 github 开源地址 StarlingX是一个软件栈，他包含了打包，编译，安装配置，openstack本身，WindRiver的MTCE平台，以及WindRiver针对电信云开发的VIM等等。基于OpenStack的大规模边缘计算方案，集成了OpenStack的核心服务用于实现计算，网络，存储等能力。目标是实现边缘端的无人值守，虚拟机级别的管理。边缘端组成边缘云互相协同，以及和中心云实现协同。 Baetyl 百度开源的面向端的工业互联网智能边缘计算方案 github 开源地址 它需要和百度的云端管理套件BIE结合实现云边协同 KubeEdge与K3S全方位对比 部署模型 KubeEdge遵循的是以下部署模型: KubeEdge是一种完全去中心化的部署模式，管理面部署在云端，边缘节点无需太多资源就能运行Kubernetes的agent，云边通过消息协同。从Kubernetes的角度看，边缘节点 + 云端才是一个完整的Kubernetes集群。这种部署模型能够同时满足“设备边缘”和“基础设施边缘”场景的部署要求。 K3S的部署模型如下所示： K3S会在边缘运行完整的Kubernetes集群，这意味着K3S并不是一个去中心化的部署模型，每个边缘都需要额外部署Kubernetes管理面。这种部署模型带来的问题有： 在边缘安装Kubernetes管理面将消耗较多资源，因此该部署模型只适合资源充足的“基础设施边缘”场景，并不适用于资源较少的“设备边缘”的场景； 集群之间网络需要打通； 为了管理边缘Kubernetes集群还需要在上面叠加一层多集群管理组件（遗憾的是该组件未开源）。 云边协同 云边协同是KubeEdge的一大亮点。KubeEdge通过Kubernetes标准API在云端管理边缘节点、设备和工作负载的增删改查。边缘节点的系统升级和应用程序更新都可以直接从云端下发，提升边缘的运维效率。另外，KubeEdge底层优化的多路复用消息通道相对于Kubernetes基于HTTP长连接的list/watch机制扩展性更好，允许海量边缘节点和设备的接入。KubeEdge云端组件完全开源，用户可以在任何公有云/私有云上部署KubeEdge而不用担心厂商锁定，并且自由集成公有云的其他服务。 K3S并不提供云边协同的能力。 边缘节点离线自治 与Kubernetes集群的节点不同，边缘节点需要在完全断开连接的模式下自主工作，并不会定期进行状态同步，只有在重连时才会与控制面通信。此模式与Kubernetes管理面和工作节点通过心跳和list/watch保持状态更新的原始设计非常不同。 KubeEdge通过消息总线和元数据本地存储实现了节点的离线自治。用户期望的控制面配置和设备实时状态更新都通过消息同步到本地存储，这样节点在离线情况下即使重启也不会丢失管理元数据，并保持对本节点设备和应用的管理能力。 K3S也不涉及这方面能力。 设备管理 KubeEdge提供了可插拔式的设备统一管理框架，允许用户在此框架上根据不同的协议或实际需求开发设备接入驱动。当前已经支持和计划支持的协议有：MQTT，BlueTooth，OPC UA，Modbus等，随着越来越多社区合作伙伴的加入，KubeEdge未来会支持更多的设备通信协议。KubeEdge通过device twins/digital twins实现设备状态的更新和同步，并在云端提供Kubernetes的扩展API抽象设备对象，用户可以在云端使用kubectl操作Kubernetes资源对象的方式管理边缘设备。 K3S并不涉及这方面能力。 轻量化 为了将Kubernetes部署在边缘，KubeEdge和K3S都进行了轻量化的改造。区别在于K3S的方向是基于社区版Kubernetes不断做减法（包括管理面和控制面），而KubeEdge则是保留了Kubernetes管理面，重新开发了节点agent。 需要注意的是，K3S在裁剪Kubernetes的过程中导致部分管理面能力的缺失，例如：一些Admission Controller。而KubeEdge则完整地保留了Kubernetes管理面，没有修改过一行代码。 下面我们将从二进制大小、内存和CPU三个维度对比KubeEdge和K3S的资源消耗情况。由于KubeEdge的管理面部署在云端，用户不太关心云端资源的消耗，而K3S的server和agent均运行在边缘，因此下面将对比KubeEdge agent，K3S agent和K3S server这三个不同的进程的CPU和内存的资源消耗。 测试机规格为4 vCPU，8GB RAM。 内存消耗对比 分别用KubeEdge和K3S部署0~100个应用，分别观测两者的内存消耗，对比如下所示： 从上图可以看出，内存消耗：KubeEdge agent   u0026lt; K3S agent   u0026lt; K3S Server。有意思的是，K3S agent即使不运行应用也消耗100+MB的内存，而K3S server在空跑的情况下内存消耗也在300MB左右。 CPU使用对比 分别用KubeEdge和K3S部署0~100个应用，分别观测两者的CPU使用情况，对比如下所示： 从上图可以看出，KubeEdge agent CPU消耗要比K3S agent和server都要少。 二进制大小 KubeEdge agent二进制大小为62MB，K3S二进制大小为36MB。 大规模 Kubernetes原生的可扩展性受制于list/watch的长连接消耗，生产环境能够稳定支持的节点规模是1000左右。KubeEdge作为华为云智能边缘服务IEF的内核，通过多路复用的消息通道优化了云边的通信的性能，压测发现可以轻松支持5000+节点。 而K3S的集群管理技术尚未开源，因为无法得知K3S管理大规模集群的能力。 小结 K3S最让人印象深刻的创新在于其对Kubernetes小型化做的尝试，通过剪裁了Kubernetes一些不常用功能并且合并多个组件到一个进程运行的方式，使得一些资源较充足的边缘节点上能够运行Kubernetes，让边缘场景下的用户也能获得一致的Kubernetes体验。然而通过上面的性能对比数据发现，K3S的资源消耗还是比KubeEdge要高出好几倍，而且动辄几百MB的内存也不是大多数设备边缘节点所能提供的。最重要的是，Kubernetes最初是为云数据中心而设计的，很多边缘计算场景特殊的问题原生Kubernetes无法很好地解决， K3S直接修改Kubernetes的代码甚至基础实现机制（例如，使用SQLite替换etcd）的做法仍值得商榷。关于什么能改，什么不能改以及怎么改？每个用户根据自己的实际需求有各自的观点，而且也很难达成一致。另外， K3S这种侵入式的修改能否持续跟进Kubernetes上游的发展也是一个未知数。 KubeEdge和K3S走的是另一条道路，KubeEdge是一个从云到边缘再到设备的完整边缘云平台，它与Kubernetes的耦合仅仅是100%兼容Kubernetes原生API。KubeEdge提供了K3S所不具备的云边协同能力，开发了更轻量的边缘容器管理agent，解决了原生Kubernetes在边缘场景下的离线自治问题，并且支持海量异构边缘设备的接入等。KubeEdge最近捐献给CNCF，成为CNCF边缘计算领域的第一个正式项目，就是为了和社区合作伙伴一起制定云和边缘计算协同的标准，结束边缘计算没有统一标准和参考架构的混沌状态，共同推动边缘计算的产业发展。 参考资料  边缘计算开源方案对比   KubeEdge向左，K3S向右  ",
      "url"      : "https://y2p.cc/2020/02/02/mec/",
      "keywords" : ""
    } ,
  
    {
      "title"    : "GPU",
      "category" : "GPU",
      "content": "Awesome GPU Awesome GPU 硬件厂家共享方案 NVIDIA GPU 共享方案 GPU 的算力很强，GPU 硬件很贵，为了节省固定资产的投入，需要将多个推理服务部署在同一张 GPU 卡上，在保证服务质量的前提下通过 GPU 共享来提升 GPU 的利用率。目前英伟达官方的 GPU 共享技术主要有两种方案：  MIG vGPU MPS NVIDIA MIG 方案 VIDIA MIG 方案，多实例 GPU (MIG) 扩展了每个 NVIDIA H100、A100 及 A30 Tensor Core GPU 的性能和价值。MIG 可将 GPU 划分为多达七个实例，每个实例均完全独立于各自的高带宽显存、缓存和计算核心。如此一来，管理员便能支持所有大小的工作负载，且服务质量 (QoS) 稳定可靠，让每位用户都能享用加速计算资源。 MIG，即是一种 Hardware Partition。硬件资源隔离、故障隔离都是硬件实现的 —— 这是无可争议的隔离性最好的方案。它的问题是不灵活: 只有高端 GPU 支持；只支持 CUDA 计算；只支持 7 个 MIG 实例。 NVIDIA vGPU 方案 NVIDIA vGPU 方案采用虚拟化的技术，基于 SR-IOV 进行 GPU 设备虚拟化管理，在驱动层提供了时间分片执行的逻辑，并做了一定的显存隔离，这样在对显卡进行初始化设置的时候就可以根据需求将显卡进行划分。其中时间分片调度的逻辑可以是按实例均分，或者是自定义比例，显卡的显存需要按照预设的比例进行划分。Nvdia的vGPU方案在实施中有下面两点限制：  vGPU划分完成之后，如果要改变这种预定义的划分，需要重启显卡才能生效，无法做到不重启更改配置。 其方案基于虚机，需要先对 GPU 物理机进行虚拟化之后，再在虚拟机内部署容器，无法直接基于物理机进行容器化的调度，另外 vGPU 方案需要收取 license 费用，增加了使用成本。 NVIDIA MPS 方案 NVIDIA MPS 方案是一种算力分割的软件虚拟化方案。它通过将多个进程的 CUDA Context，合并到一个 CUDA Context 中，省去了 Context Switch 的开销，也在 Context 内部实现了算力隔离。如前所述，MPS 的致命缺陷，是 把许多进程的 CUDA Context 合并成一个，从而导致了额外的故障传播。所以尽管它的算力隔离效果极好，但长期以来工业界使用不多，多租户场景尤其如此。 寒武纪 GPU 共享方案 寒武纪 SR-IOV 方案 SR-IOV 是 PCI-SIG 在 2007 年推出的规范，目的就是 PCIe 设备的虚拟化。SR-IOV 的本质是什么？考虑我们说过的 2 种资源和 2 种能力，来看看一个 VF 有什么:  配置空间是虚拟的（特权资源） MMIO 是物理的 中断和 DMA，因为 VF 有自己的 PCIe 协议层的标识（Routing ID，就是 BDF），从而拥有独立的地址空间。 那么，什么设备适合实现 SR-IOV？其实无非是要满足两点:  硬件资源要容易 partition 无状态（至少要接近无状态） 常见 PCIe 设备中，最适合 SR-IOV 的就是网卡了: 一或多对 TX/RX queue + 一或多个中断，结合上一个 Routing ID，就可以抽象为一个 VF。而且它是近乎无状态的。 试考虑 NVMe 设备，它的资源也很容易 partition，但是它有存储数据，因此在实现 SR-IOV 方面，就会有更多的顾虑。 回到 GPU 虚拟化：为什么 2007 年就出现 SR-IOV 规范、直到 2015 业界才出现第一个「表面上的」SRIOV-capable GPU ？这是因为，虽然 GPU 基本也是无状态的，但是它的硬件复杂度极高，远远超出 NIC、NVMe 这些，导致硬件资源的 partition 很难实现。 寒武纪虚拟化技术——vMLU，该虚拟化技术允许多个操作系统和应用程序共存于一个物理计算平台上，共享同一个芯片的计算资源。它为用户提供良好的安全性和隔离性，还支持如热迁移等高灵活特性。vMLU 帮助提高云计算密度，也使数据中心的 IT 资产管理更灵活。 除了虚拟化基本的资源共享特性，思元 270 首推的 SR-IOV 虚拟化技术，支持运行在云服务器上的多个实例直接共享智能芯片的硬件资源。传统虚拟化系统中大量的资源和时间损耗在 Hypervisor 或 VMM 软件层面，PCIe 设备的性能优势无法彻底发挥。而 SR-IOV 的价值在于消除这一软件瓶颈，助力多个虚拟机实现高效物理资源共享。 与传统图形加速卡的 vGPU 所采用的虚拟化技术不同，思元 270 采用「非基于时间片的共享」方式，因为其没有因时间片切换上下文带来的性能损失，能充分保证各 VF 独立的服务质量，彼此完全独立运行互不影响。 另外，SR-IOV 还可以避免因分时复用切换应用带来的性能开销。vMLU 搭配 Docker 或 VM 运行时，单个 VF 业务性能保持在硬件性能的 91% 以上。这使得用户在多模型并行时，对各 VF 可以做出更准确的服务质量 (QoS) 预期，而不必考虑多模型时的拥塞或切换带来的性能开销。 基于 SR-IOV 的 vMLU：更好的租户隔离性 虚拟化技术被数据中心广泛采用，除了因为其提供了对资源共享的能力（提供了更好的密度性能），也因为相对于其它技术 (如 docker), 虚拟化提供了更好的隔离性和安全性。寒武纪 vMLU 基于 SR-IOV 的虚拟化技术可以帮助云用户实现更好的隔离特性，具体优势如下：  资源独立，互不干扰，能确保服务质量（QoS）； 多任务时，没有无队列阻塞的烦恼； 其具备独立内存资源，各 VF 之间互不可见； 它的部署相对简单，不需要对开源软件成分进行修改。 软件厂家共享方案 方案评估点  不会使用超过其被分配的算力大小 隔离本身不应该对于 GPU 算力有过多损耗 多个进程同时共享的时候，与其单独运行时相比，不应有太大的性能偏差，即共享可以有效避免进程之间的干扰。 截获 CUDA API 实现显存及算力隔离 显存隔离 对于深度学习应用来说，对于显存的需求来自于三个方面。  第一是模型的 CUDA kernel context，可类比于 CPU 程序中的 text 段，提供给 CUDA kernel 执行的环境，这是一项刚需，没有充足的显存，kernel 将无法启动，且 context 的大小随着 kernel 的复杂程度有增长，但在整体模型显存需求中是最小的一部分。   第二部分来自于模型训练得出的一些参数，如卷积中的 weight 和 bias。   第三部分来自于模型在推理过程中的临时存储，用于储存中间的计算结果。 对于一般的模型来说，基本都不需要占用整个GPU的显存。但是这里有一个例外，Tensorflow 框架默认分配所有 GPU 的显存来进行自己的显存管理。当然 Tensorflow 框架有相应的选项可以屏蔽该行为，但是对于平台来说，要让每个用户修改 TF 的配置为屏蔽该行为，就不太可行。 为应对这一问题，一个巧妙的方法可以在不需要应用开发者参与的情况下，让 Tensorflow 的部署应用只分配它所需的显存大小而不出现问题。该方法即 API 动态拦截。Tensorflow 之所以可以知道当前 GPU 的剩余显存，是通过 cuDeviceTotalMem/cuMemGetInfo 这两个 CUDA library API。通过 LD_PRELOAD 的方式，在钩子 so 中实现这两个 API，那么 Tensorflow 执行的时候，link 首先会调用的是的 API 实现，而不是 CUDA 的，这样就可以动态的修改这两个 API 的返回结果，如这里想做的，将特定 Tensorflow 应用的显存配额限制在其申请数值。 在系统实现的过程中，还对 cuMemAlloc/cuMemFree 做了同样的拦截，目的是为了能够对同容器中的多个 GPU 进程进程统一管理。当多个 GPU 进程分配显存之和超过其配额时，可以通过 cuMalloc 来返回显存不足的错误。容器内显存配额管理是通过 share mem 来做的。 相关实现方式可以参考：vcuMemGetInfo 算力隔离 GPU程序的执行，是通过kernel的片段来具体实施，在CPU侧launch了 kernel之后，具体的kernel及其调用参数随即交由GPU的硬件调度器来在某个未来的时间点真正运行起来。在默认的情况下，kernel是被派发给GPU上所有的SM，且执行过程中不能被中断。 CUDA中用来区分thread，来判断代码应该处理数据的偏移量的方法，是通过CUDA中的blockIdx/threadIdx这两个内嵌变量。这两个变量在机器码上是只读的，在thread由硬件调度器派发的时候所指定。通过硬件调度器，就完成了抽象的blockIdx/threadIdx和具体的SM/SP的绑定。 为了能够精确的控制算力，我们就不能再依赖硬件调度器来控制内核启动。在这里用了一个取巧的方法，就是让内核启动之后被“困”在固定数目的SM上面，这个数目的值和GPU整体SM个数的比例就是给这个内核算力配比。 为了形象化来阐述思路，这里我们对GPU做了一个抽象化的改动，SM的个数被定义为10个。然后有一个启动参数为«&lt;15,1»&gt;的内核，即CUDA block size为15，thread size为1。它正常启动的时候，硬件调度器会给每一个SM上分配一个内核的副本。这样在第一时间就消耗了10个block的副本，随后每个SM上内核执行完毕之后会退出，硬件调度器会进一步分配剩下的5个block副本，在这个也执行完毕之后就完成了整个内核的执行。 算力切分之后，我们会在内核启动时，动态的修改其启动参数，将其CUDA block size从15变为5。这样硬件调度器就会将内核副本分配到GPU上一半数目的SM上，空闲的一半可以为其他内核所使用。 我们虽然通过动态修改启动参数的方法，避免了内核占满全部SM资源，但此时还没完成“困”这一动作。所以此时的内核行为是其完成预定逻辑之后，会退出，导致此时内核不能覆盖block size为15时的数据空间。为了将其“困“住，我们在内核的汇编EXIT处，替换成了BRANCH操作。这样内核完成本身的逻辑之后，会跳转到我们预设的一段逻辑中。这个逻辑完成虚拟blockIdx/threadIdx的自增操作，随后再跳转到内核开始位置，来基于更新的blockIdx/threadIdx来进行新一轮计算。 这次需要指出的是blockIdx/threadIdx为只读寄存器，所以没办法直接更改它的值。作为一个替代的解决方案时，将内核中的blockIdx/threadIdx进行整体替换为可写的寄存器，这样我们就可以在预设的跳转逻辑中做更改操作。 参考资料  怎样节省 2/3 的 GPU？爱奇艺 vGPU 的探索与实践   寒武纪vMLU技术面世，首推SR-IOV虚拟化功能   GPU虚拟化，算力隔离，和qGPU  ",
      "url"      : "https://y2p.cc/2021/10/10/gpu/",
      "keywords" : "GPU"
    } ,
  
    {
      "title"    : "base64",
      "category" : "Base64",
      "content": " 目录  Base64编码原理分析 总结 参考资料 Base64编码原理分析 Base64是网络上最常见的用于传输8Bit字节代码的编码方式之一，在了解Base64编码之前，先了解几个基本概念：位、字节。 位：”位(bit)”是计算机中最小的数据单位。每一位的状态只能是0或1； 字节：8个二进制位构成1个”字节(Byte)”，字节是存储空间的基本计量单位。1个字节可以储存1个英文字母，2个字节可以存储1个汉字； Base64编码的作用 因为有些网络传送渠道并不支持所有的字节，例如传统的邮件只支持可见字符的传送，像ASCII码的控制字符就不能通过邮件传送。这样就受到了很大的限制，比如图片二进制流的每个字节不可能全部是可见字符，所以就传送不了。最好的方法就是在不改变传统协议的情况下，开辟一种新的方案来支持二进制文件的传送。把不可见字符用可见字符来表示。而Base64就是一种基于64个可见字符来表示二进制数据的表示方法。 扩展：不可见字符其实并不是不显示，只是这些字符在屏幕上显示不出来，比如：换行符、回车、退格……字符。 Base64编码的原理 Base64可以将ASCII字符串或者是二进制编码成只包含A—Z，a—z，0—9，+，/ 这64个字符（ 26个大写字母，26个小写字母，10个数字，1个+，一个 / 刚好64个字符）。这64个字符用6个bit位就可以全部表示出来，一个字节有8个bit 位，那么还剩下两个bit位，这两个bit位用0来补充。其实，一个Base64字符仍然是8个bit位，但是有效部分只有右边的6个 bit，左边两个永远是0。 Base64的编码规则是将3个8位字节(3×8=24位)编码成4个6位的字节(4×6=24位)，之后在每个6位字节前面，补充两个0，形成4个8位字节的形式，那么取值范围就变成了0~63。又因为2的6次方等于64，所以每6个位组成一个单元。 总结 Base64编码并不是真正的加密方式，它只是从二进制到字符的转换过程，说Base64编码是加密方法，只是因为经过Base64编码之后，让人一眼看上去不知道什么内容而已。 参考资料 [1] # 参考资料 ",
      "url"      : "https://y2p.cc/notes/base64/",
      "keywords" : "base64"
    } ,
  
    {
      "title"    : "中文文案排版指北（简体中文版）",
      "category" : "Copywriting",
      "content": "统一中文文案、排版的相关用法，降低团队成员之间的沟通成本，增强网站气质。 Other languages:  Chinese Traditional English  目录  空格   中英文之间需要增加空格  中文与数字之间需要增加空格  数字与单位之间需要增加空格  全角标点与其他字符之间不加空格  -ms-text-autospace to the rescue?   标点符号   不重复使用标点符号   全角和半角   使用全角中文标点  数字使用半角字符  遇到完整的英文整句、特殊名词，其內容使用半角标点   名词   专有名词使用正确的大小写  不要使用不地道的缩写   争议   链接之间增加空格  简体中文使用直角引号   工具 谁在这样做？ 参考文献 空格 「有研究显示，打字的时候不喜欢在中文和英文之间加空格的人，感情路都走得很辛苦，有七成的比例会在 34 岁的时候跟自己不爱的人结婚，而其余三成的人最后只能把遗产留给自己的猫。毕竟爱情跟书写都需要适时地留白。 与大家共勉之。」——vinta/paranoid-auto-spacing 中英文之间需要增加空格 正确：  在 LeanCloud 上，数据存储是围绕 AVObject 进行的。 错误：  在LeanCloud上，数据存储是围绕AVObject进行的。 在 LeanCloud上，数据存储是围绕AVObject 进行的。 完整的正确用法：  在 LeanCloud 上，数据存储是围绕 AVObject 进行的。每个 AVObject 都包含了与 JSON 兼容的 key-value 对应的数据。数据是 schema-free 的，你不需要在每个 AVObject 上提前指定存在哪些键，只要直接设定对应的 key-value 即可。 例外：「豆瓣FM」等产品名词，按照官方所定义的格式书写。 中文与数字之间需要增加空格 正确：  今天出去买菜花了 5000 元。 错误：  今天出去买菜花了 5000元。 今天出去买菜花了5000元。 数字与单位之间需要增加空格 正确：  我家的光纤入户宽带有 10 Gbps，SSD 一共有 20 TB。 错误：  我家的光纤入户宽带有 10Gbps，SSD 一共有 10TB。 例外：度／百分比与数字之间不需要增加空格： 正确：  今天是 233° 的高温。 新 MacBook Pro 有 15% 的 CPU 性能提升。 错误：  今天是 233 ° 的高温。 新 MacBook Pro 有 15 % 的 CPU 性能提升。 全角标点与其他字符之间不加空格 正确：  刚刚买了一部 iPhone，好开心！ 错误：  刚刚买了一部 iPhone ，好开心！ -ms-text-autospace to the rescue? Microsoft 有个 -ms-text-autospace 的 CSS 属性可以实现自动为中英文之间增加空白。不过目前并未普及，另外在其他应用场景，例如 OS X、iOS 的用户界面目前并不存在这个特性，所以请继续保持随手加空格的习惯。 标点符号 不重复使用标点符号 正确：  德国队竟然战胜了巴西队！ 她竟然对你说「喵」？！ 错误：  德国队竟然战胜了巴西队！！ 德国队竟然战胜了巴西队！！！！！！！！ 她竟然对你说「喵」？？！！ 她竟然对你说「喵」？！？！？？！！ 全角和半角 不明白什么是全角（全形）与半角（半形）符号？请查看维基百科词条『全角和半角』。 使用全角中文标点 正确：  嗨！你知道嘛？今天前台的小妹跟我说「喵」了哎！ 核磁共振成像（NMRI）是什么原理都不知道？JFGI！ 错误：  嗨! 你知道嘛? 今天前台的小妹跟我说 “喵” 了哎! 嗨!你知道嘛?今天前台的小妹跟我说”喵”了哎! 核磁共振成像 (NMRI) 是什么原理都不知道? JFGI! 核磁共振成像(NMRI)是什么原理都不知道?JFGI! 数字使用半角字符 正确：  这件蛋糕只卖 1000 元。 错误：  这件蛋糕只卖 １０００ 元。 例外：在设计稿、宣传海报中如出现极少量数字的情形时，为方便文字对齐，是可以使用全角数字的。 遇到完整的英文整句、特殊名词，其內容使用半角标点 正确：  乔布斯那句话是怎么说的？「Stay hungry, stay foolish.」 推荐你阅读《Hackers &amp; Painters: Big Ideas from the Computer Age》，非常的有趣。 错误：  乔布斯那句话是怎么说的？「Stay hungry，stay foolish。」 推荐你阅读《Hackers＆Painters：Big Ideas from the Computer Age》，非常的有趣。 名词 专有名词使用正确的大小写 大小写相关用法原属于英文书写范畴，不属于本 wiki 讨论內容，在这里只对部分易错用法进行简述。 正确：  使用 GitHub 登录 我们的客户有 GitHub、Foursquare、Microsoft Corporation、Google、Facebook, Inc.。 错误：  使用 github 登录 使用 GITHUB 登录 使用 Github 登录 使用 gitHub 登录 使用 gｲんĤЦ8 登录 我们的客户有 github、foursquare、microsoft corporation、google、facebook, inc.。 我们的客户有 GITHUB、FOURSQUARE、MICROSOFT CORPORATION、GOOGLE、FACEBOOK, INC.。 我们的客户有 Github、FourSquare、MicroSoft Corporation、Google、FaceBook, Inc.。 我们的客户有 gitHub、fourSquare、microSoft Corporation、google、faceBook, Inc.。 我们的客户有 gｲんĤЦ8、ｷouЯƧquﾑгє、๓เςг๏ร๏Ŧt ς๏гק๏гคtเ๏ภn、900913、ƒ4ᄃëв๏๏к, IПᄃ.。 注意：当网页中需要配合整体视觉风格而出现全部大写／小写的情形，HTML 中请使用标准的大小写规范进行书写；并通过 text-transform: uppercase;／text-transform: lowercase; 对表现形式进行定义。 不要使用不地道的缩写 正确：  我们需要一位熟悉 JavaScript、HTML5，至少理解一种框架（如 Backbone.js、AngularJS、React 等）的前端开发者。 错误：  我们需要一位熟悉 Js、h5，至少理解一种框架（如 backbone、angular、RJS 等）的 FED。 争议 以下用法略带有个人色彩，既：无论是否遵循下述规则，从语法的角度来讲都是正确的。 链接之间增加空格 用法：  请 提交一个 issue 并分配给相关同事。 访问我们网站的最新动态，请 点击这里 进行订阅！ 对比用法：  请提交一个 issue 并分配给相关同事。 访问我们网站的最新动态，请点击这里进行订阅！ 简体中文使用直角引号 用法：  「老师，『有条不紊』的『紊』是什么意思？」 对比用法：  “老师，‘有条不紊’的‘紊’是什么意思？” 工具    仓库  语言     vinta/paranoid-auto-spacing  JavaScript    huei90/pangu.node  Node.js    huacnlee/auto-correct  Ruby    sparanoid/space-lover  PHP (WordPress)    nauxliu/auto-correct  PHP    hotoo/pangu.vim  Vim    sparanoid/grunt-auto-spacing  Node.js (Grunt)    hjiang/scripts/add-space-between-latin-and-cjk  Python   谁在这样做？    网站  文案  UGC     Apple 中国  Yes  N/A    Apple 香港  Yes  N/A    Apple 台湾  Yes  N/A    Microsoft 中国  Yes  N/A    Microsoft 香港  Yes  N/A    Microsoft 台湾  Yes  N/A    LeanCloud  Yes  N/A    知乎  Yes  部分用户达成    V2EX  Yes  Yes    SegmentFault  Yes  部分用户达成    Apple4us  Yes  N/A    豌豆荚  Yes  N/A    Ruby China  Yes  标题达成    PHPHub  Yes  标题达成   参考文献  Guidelines for Using Capital Letters Letter case - Wikipedia Punctuation - Oxford Dictionaries Punctuation - The Purdue OWL How to Use English Punctuation Corrently - wikiHow 格式 - openSUSE 全角和半角 - 维基百科 引号 - 维基百科 疑问惊叹号 - 维基百科 ",
      "url"      : "https://y2p.cc/notes/chinese-copywriting-guidelines/",
      "keywords" : "中文文案排版指北"
    } ,
  
    {
      "title"    : "cilium",
      "category" : "Cilium",
      "content": " 目录  Cilium简介   BPF   为什么选择Cilium？ Cilium简介 Cilium是一个纯开源软件，没有哪家公司提供商业化支持，也不是由某一公司开源，该软件用于透明地保护使用Linux容器管理平台（如Docker和Kubernetes）部署的应用程序服务之间的网络连接。 Cilium的基础是一种名为BPF的新Linux内核技术，它可以在Linux本身动态插入强大的安全可见性和控制逻辑。由于BPF在Linux内核中运行，因此可以应用和更新Cilium安全策略，而无需对应用程序代码或容器配置进行任何更改。 基于微服务的应用程序分为小型独立服务，这些服务使用HTTP、gRPC、Kafka等轻量级协议通过API相互通信。但是，现有的Linux网络安全机制（例如iptables）仅在网络和传输层（即IP地址和端口）上运行，并且缺乏对微服务层的可见性。 Cilium为Linux容器框架（如Docker和Kubernetes）带来了API感知网络安全过滤。使用名为BPF的新Linux内核技术，Cilium提供了一种基于容器/容器标识定义和实施网络层和应用层安全策略的简单而有效的方法。 注：Cilium中文意思是“纤毛“，它十分细小而又无处不在。 BPF 柏克莱封包过滤器（Berkeley Packet Filter，缩写 BPF），是类Unix系统上数据链路层的一种原始接口，提供原始链路层封包的收发，除此之外，如果网卡驱动支持洪泛模式，那么它可以让网卡处于此种模式，这样可以收到网络上的所有包，不管他们的目的地是不是所在主机。参考维基百科和eBPF简史。 为什么选择Cilium？ 现代数据中心应用程序的开发已经转向面向服务的体系结构（SOA），通常称为微服务，其中大型应用程序被分成小型独立服务，这些服务使用HTTP等轻量级协议通过API相互通信。微服务应用程序往往是高度动态的，作为持续交付的一部分部署的滚动更新期间单个容器启动或销毁，应用程序扩展/缩小以适应负载变化。 这种向高度动态的微服务的转变过程，给确保微服务之间的连接方面提出了挑战和机遇。传统的Linux网络安全方法（例如iptables）过滤IP地址和TCP/UDP端口，但IP地址经常在动态微服务环境中流失。容器的高度不稳定的生命周期导致这些方法难以与应用程序并排扩展，因为负载均衡表和访问控制列表要不断更新，可能增长成包含数十万条规则。出于安全目的，协议端口（例如，用于HTTP流量的TCP端口80）不能再用于区分应用流量，因为该端口用于跨服务的各种消息。 另一个挑战是提供准确的可见性，因为传统系统使用IP地址作为主要识别工具，其在微服务架构中的寿命可能才仅仅几秒钟，被大大缩短。 利用Linux BPF，Cilium保留了透明地插入安全可视性+强制执行的能力，但这种方式基于服务/pod/容器标识（与传统系统中的IP地址识别相反），并且可以根据应用层进行过滤 （例如HTTP）。因此，通过将安全性与寻址分离，Cilium不仅可以在高度动态的环境中应用安全策略，而且除了提供传统的第3层和第4层分割之外，还可以通过在HTTP层运行来提供更强的安全隔离。 。 BPF的使用使得Cilium能够以高度可扩展的方式实现以上功能，即使对于大规模环境也不例外。 ",
      "url"      : "https://y2p.cc/notes/cilium/",
      "keywords" : "cilium"
    } ,
  
    {
      "title"    : "Git",
      "category" : "Git",
      "content": "常用命令    功能  命令     添加文件/更改到暂存区  git add filename    添加所有文件/更改到暂存区  git add .    提交  git commit -m msg    从远程仓库拉取最新代码  git pull origin master    推送到远程仓库  git push origin master    查看配置信息  git config –list    查看文件列表  git ls-files    比较工作区和暂存区  git diff    比较暂存区和版本库  git diff –cached    比较工作区和版本库  git diff HEAD    从暂存区移除文件  git reset HEAD filename    查看本地远程仓库配置  git remote -v    回滚  git reset –hard 提交SHA    强制推送到远程仓库  git push -f origin master    修改上次 commit  git commit –amend    推送 tags 到远程仓库  git push –tags    推送单个 tag 到远程仓库  git push origin [tagname]    删除远程分支  git push origin –delete [branchName]    远程空分支（等同于删除）  git push origin :[branchName]   Q&amp;A 如何解决gitk中文乱码，git ls-files 中文文件名乱码问题？ 在~/.gitconfig中添加如下内容 [core] quotepath = false [gui] encoding = utf-8 [i18n] commitencoding = utf-8 [svn] pathnameencoding = utf-8 参考 http://zengrong.net/post/1249.htm 如何处理本地有更改需要从服务器合入新代码的情况？ git stash git pull git stash pop stash 查看 stash 列表： git stash list 查看某一次 stash 的改动文件列表（不传最后一个参数默认显示最近一次）： git stash show stash@{0} 以 patch 方式显示改动内容 git stash show -p stash@{0} 如何合并 fork 的仓库的上游更新？ git remote add upstream https://upstream-repo-url git fetch upstream git merge upstream/master 如何通过 TortoiseSVN 带的 TortoiseMerge.exe 处理 git 产生的 conflict？ 将 TortoiseMerge.exe 所在路径添加到 path 环境变量。 运行命令 git config --global merge.tool tortoisemerge 将 TortoiseMerge.exe 设置为默认的 merge tool。  在产生 conflict 的目录运行 git mergetool，TortoiseMerge.exe 会跳出来供你 resolve conflict。   也可以运行 git mergetool -t vimdiff 使用 -t 参数临时指定一个想要使用的 merge tool。   不想跟踪的文件已经被提交了，如何不再跟踪而保留本地文件？ git rm --cached /path/to/file，然后正常 add 和 commit 即可。 如何不建立一个没有 parent 的 branch？ git checkout --orphan newbranch 此时 git branch 是不会显示该 branch 的，直到你做完更改首次 commit。比如你可能会想建立一个空的 gh-pages branch，那么： git checkout --orphan gh-pages git rm -rf . // add your gh-pages branch files git add . git commit -m init commit submodule 的常用命令 添加 submodule git submodule add git@github.com:philsquared/Catch.git Catch 这会在仓库根目录下生成如下 .gitmodules 文件并 clone 该 submodule 到本地。 [submodule Catch] path = Catch url = git@github.com:philsquared/Catch.git 更新 submodule git submodule update 当 submodule 的 remote 有更新的时候，需要 git submodule update --remote 删除 submodule 在 .gitmodules 中删除对应 submodule 的信息，然后使用如下命令删除子模块所有文件： git rm --cached Catch clone 仓库时拉取 submodule git submodule update --init --recursive 删除远程 tag git tag -d v0.0.9 git push origin :refs/tags/v0.0.9 或 git push origin --delete tag [tagname] 清除未跟踪文件 git clean 可选项：    选项  含义     -q, –quiet  不显示删除文件名称    -n, –dry-run  试运行    -f, –force  强制删除    -i, –interactive  交互式删除    -d  删除文件夹    -e, –exclude  忽略符合 的文件    -x  清除包括 .gitignore 里忽略的文件    -X  只清除 .gitignore 里忽略的文件   忽略文件属性更改 因为临时需求对某个文件 chmod 了一下，结果这个就被记为了更改，有时候这是想要的，有时候这会造成困扰。 git config --global core.filemode false 参考：How do I make Git ignore file mode (chmod) changes? patch 将未添加到暂存区的更改生成 patch 文件： git diff &gt; demo.patch 将已添加到暂存区的更改生成 patch 文件： git diff --cached &gt; demo.patch 合并上面两条命令生成的 patch 文件包含的更改： git apply demo.patch 将从 HEAD 之前的 3 次 commit 生成 3 个 patch 文件： （HEAD 可以换成 sha1 码） git format-patch -3 HEAD 生成 af8e2 与 eaf8e 之间的 commits 的 patch 文件： （注意 af8e2 比 eaf8e 早） git format-patch af8e2..eaf8e 合并 format-patch 命令生成的 patch 文件： git am 0001-Update.patch 与 git apply 不同，这会直接 add 和 commit。 只下载最新代码 git clone --depth 1 git://xxxxxx 这样 clone 出来的仓库会是一个 shallow 的状态，要让它变成一个完整的版本： git fetch --unshallow 或 git pull --unshallow 基于某次 commit 创建分支 git checkout -b test 5234ab 表示以 commit hash 为 5234ab 的代码为基础创建分支 test。 恢复单个文件到指定版本 git reset 5234ab MainActivity.java 恢复 MainActivity.java 文件到 commit hash 为 5234ab 时的状态。 设置全局 hooks git config --global core.hooksPath C:/Users/mazhuang/git-hooks 然后把对应的 hooks 文件放在最后一个参数指定的目录即可。 比如想要设置在 commit 之前如果检测到没有从服务器同步则不允许 commit，那在以上目录下建立文件 pre-commit，内容如下： #!/bin/sh CURRENT_BRANCH=$(git rev-parse --abbrev-ref HEAD) git fetch origin $CURRENT_BRANCH HEAD=$(git rev-parse HEAD) FETCH_HEAD=$(git rev-parse FETCH_HEAD) if [ $FETCH_HEAD = $HEAD ]; then  echo Pre-commit check passed  exit 0 fi echo Error: you need to update from remote first exit 1 ",
      "url"      : "https://y2p.cc/notes/git/",
      "keywords" : "Git, 版本控制"
    } ,
  
    {
      "title"    : "GPU",
      "category" : "GPU",
      "content": " ",
      "url"      : "https://y2p.cc/notes/gpu/",
      "keywords" : "gpu"
    } ,
  
    {
      "title"    : "Kubernetes Cloud Provider",
      "category" : "Kubernetes",
      "content": " ",
      "url"      : "https://y2p.cc/notes/kubernetes-cloud-provider/",
      "keywords" : "Kubernetes Cloud Provider"
    } ,
  
    {
      "title"    : "Kubernetes Dashboard",
      "category" : "Kubernetes",
      "content": " 目录  Kuberctl create -f kubernetes-dashboard.yaml kubectl create -f maxright.yaml kubectl proxy acess url kubectl describe secret dashboard -n kube-system copy token to web Kuberctl create -f kubernetes-dashboard.yaml # ------------------- Dashboard Secret ------------------- # apiVersion: v1 kind: Secret metadata: labels:  k8s-app: kubernetes-dashboard name: kubernetes-dashboard-certs namespace: kube-system type: Opaque --- # ------------------- Dashboard Service Account ------------------- # apiVersion: v1 kind: ServiceAccount metadata: labels:  k8s-app: kubernetes-dashboard name: kubernetes-dashboard namespace: kube-system --- # ------------------- Dashboard Role &amp; Role Binding ------------------- # kind: Role apiVersion: rbac.authorization.k8s.io/v1 metadata: name: kubernetes-dashboard-minimal namespace: kube-system rules: # Allow Dashboard to create 'kubernetes-dashboard-key-holder' secret. - apiGroups: [] resources: [secrets] verbs: [create] # Allow Dashboard to create 'kubernetes-dashboard-settings' config map. - apiGroups: [] resources: [configmaps] verbs: [create] # Allow Dashboard to get, update and delete Dashboard exclusive secrets. - apiGroups: [] resources: [secrets] resourceNames: [kubernetes-dashboard-key-holder, kubernetes-dashboard-certs] verbs: [get, update, delete] # Allow Dashboard to get and update 'kubernetes-dashboard-settings' config map. - apiGroups: [] resources: [configmaps] resourceNames: [kubernetes-dashboard-settings] verbs: [get, update] # Allow Dashboard to get metrics from heapster. - apiGroups: [] resources: [services] resourceNames: [heapster] verbs: [proxy] - apiGroups: [] resources: [services/proxy] resourceNames: [heapster, http:heapster:, https:heapster:] verbs: [get] --- apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: kubernetes-dashboard-minimal namespace: kube-system roleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: kubernetes-dashboard-minimal subjects: - kind: ServiceAccount name: kubernetes-dashboard namespace: kube-system --- # ------------------- Dashboard Deployment ------------------- # kind: Deployment apiVersion: apps/v1beta2 metadata: labels:  k8s-app: kubernetes-dashboard name: kubernetes-dashboard namespace: kube-system spec: replicas: 1 revisionHistoryLimit: 10 selector:  matchLabels:  k8s-app: kubernetes-dashboard template:  metadata:  labels:  k8s-app: kubernetes-dashboard  spec:  containers:  - name: kubernetes-dashboard  image: siriuszg/kubernetes-dashboard-amd64  ports:  - containerPort: 8443   protocol: TCP  args:   - --auto-generate-certificates   # Uncomment the following line to manually specify Kubernetes API server Host   # If not specified, Dashboard will attempt to auto discover the API server and connect   # to it. Uncomment only if the default does not work.   # - --apiserver-host=http://my-address:port  volumeMounts:  - name: kubernetes-dashboard-certs   mountPath: /certs   # Create on-disk volume to store exec logs  - mountPath: /tmp   name: tmp-volume  livenessProbe:   httpGet:   scheme: HTTPS   path: /   port: 8443   initialDelaySeconds: 30   timeoutSeconds: 30  volumes:  - name: kubernetes-dashboard-certs  secret:   secretName: kubernetes-dashboard-certs  - name: tmp-volume  emptyDir: {}  serviceAccountName: kubernetes-dashboard  # Comment the following tolerations if Dashboard must not be deployed on master  tolerations:  - key: node-role.kubernetes.io/master  effect: NoSchedule --- # ------------------- Dashboard Service ------------------- # kind: Service apiVersion: v1 metadata: labels:  k8s-app: kubernetes-dashboard name: kubernetes-dashboard namespace: kube-system spec: ports:  - port: 443  targetPort: 8443 selector:  k8s-app: kubernetes-dashboard kubectl create -f maxright.yaml apiVersion: v1 kind: ServiceAccount metadata: name: dashboard namespace: kube-system --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: dashboard subjects: - kind: ServiceAccount  name: dashboard  namespace: kube-system roleRef: kind: ClusterRole name: cluster-admin apiGroup: rbac.authorization.k8s.io kubectl proxy acess url http://localhost:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/ kubectl describe secret dashboard -n kube-system copy token to web ",
      "url"      : "https://y2p.cc/notes/kubernetes-dashboard/",
      "keywords" : ""
    } ,
  
    {
      "title"    : "Linux/Unix",
      "category" : "Linux",
      "content": "类 Unix 系统下的一些常用命令和用法。 实用命令 fuser 查看文件被谁占用。 fuser -u .linux.md.swp id 查看当前用户、组 id。 lsof 查看打开的文件列表。  An open file may be a regular file, a directory, a block special file, a character special file, an executing text reference, a library, a stream or a network file (Internet socket, NFS file or UNIX domain socket.) A specific file or all the files in a file system may be selected by path. 查看网络相关的文件占用 lsof -i 查看端口占用 lsof -i tcp:5037 查看某个文件被谁占用 lsof .linux.md.swp 查看某个用户占用的文件信息 lsof -u mazhuang -u 后面可以跟 uid 或 login name。 查看某个程序占用的文件信息 lsof -c Vim 注意程序名区分大小写。 ",
      "url"      : "https://y2p.cc/notes/linux/",
      "keywords" : "Linux"
    } ,
  
    {
      "title"    : "Mac OS X",
      "category" : "Mac",
      "content": "快捷键约定： C –&gt; Ctrl S –&gt; Shift M –&gt; Alt/Option Cmd –&gt; Command Mac 键盘快捷键官方参考 窗口    功能  快捷键     显示桌面  F11    切换窗口全屏状态  C-Cmd-F    隐藏当前程序的所有窗口  Cmd-H    最小化窗口  Cmd-M    关闭窗口  Cmd-W    关闭当前程序  Cmd-Q    新建标签  Cmd-T    新建窗口  Cmd-N   程序    功能  快捷键     打开 emoji 表情窗口  C-Cmd-空格    打开 Spotlight  C-空格    切换输入法  Cmd-空格    打开 Alfred  M-空格    打开 Finder 并查找  C-M-空格    打开 Launchpad  四指合拢   搜索  使用 find 命令，例如： find ~ -iname aapt   使用 mdfind 命令，例如： 全局搜索 mdfind -name aapt   或搜索指定文件夹 mdfind -onlyin ~/Library aapt   使用 locate 命令，例如： locate aapt   复制文件路径  在 Finder 下 先按键 Cmd-i，然后从弹出的窗口里复制。   在 Terminal 下 pwd|pbcopy   Safari    功能  快捷键     定位到地址栏  Cmd-L    切换标签  Cmd-S-Left/Right    收藏页面  Cmd-D   保存 Safari 里正在播放的视频 $ su # cd /private/var/folders # ls nk zz # cd nk # ls zy3770994vqg83xvmbc9pd0m0000gn # cd zy3770994vqg83xvmbc9pd0m0000gn/T # open . 然后复制里面叫 FlashTmp.xxx 的文件，改名为 FlashTmp.flv。（操作过程中保持视频在播放状态） Terminal    功能  快捷键     新建标签  Cmd-T    上/下个标签  Cmd-{/}    删除光标前的输入  C-U   WireShark 使用 WireShark 1.99 开发版，可以不依赖于 X11，界面基于 Qt，更加美观，符合 Mac 界面风格。 截图    功能  快捷键     全屏截图保存到桌面  Cmd-S-3    选区截图保存到桌面  Cmd-S-4    窗口截图保存到桌面  Cmd-S-4 空格    QQ 截图  Cmd-S-A   去除窗口截图时的阴影： defaults write com.apple.screencapture disable-shadow -bool TRUE Killall SystemUIServer 如果要保留窗口截图时的阴影，则将 TRUE 改为 FALSE。 iBooks 里的电子书保存路径 /Users/&lt;username&gt;/Library/Containers/com.apple.BKAgentService/Data/Documents/iBooks/Books 安装 mpv 没有图形界面 使用 brew options mpv 可以看到有个 --with-bundle 是安装时创建 .app 文件。 brew install mpv --with-bundle brew linkapps mpv ",
      "url"      : "https://y2p.cc/notes/mac/",
      "keywords" : "Mac"
    } ,
  
    {
      "title"    : "Markdown",
      "category" : "Markdown",
      "content": "目录  列表 强调 标题 表格 代码块 图片 锚点 Emoji Footnotes 列表 1. 有序列表项 1 2. 有序列表项 2 3. 有序列表项 3   有序列表项 1   有序列表项 2   有序列表项 3 * 无序列表项 1 * 无序列表项 2 * 无序列表项 3   无序列表项 1   无序列表项 2   无序列表项 3 任务列表 1 任务列表 2 强调 ~~删除线~~ **加黑** *斜体* 删除线 加黑 斜体 标题 # 一级标题 ## 二级标题 ### 三级标题 #### 四级标题 ##### 五级标题 ###### 六级标题 Tips: # 与标题中间要加空格。 表格 | HEADER1 | HEADER2 | HEADER3 | HEADER4 | | ------- | :------ | :-----: | ------: | | content | content | content | content |   HEADER1  HEADER2  HEADER3  HEADER4     content  content  content  content   :—– 表示左对齐 :—-: 表示中对齐 —–: 表示右对齐 代码块 print 'Hello, World!'   list item1   list item2 print 'hello'   图片 ![本站favicon](/favicon.ico)  锚点 * [目录](#目录) 目录 Emoji :camel: :blush: :smile: Footnotes This is a text with footnote1.    Here is the footnote 1 definition. &#8617;  ",
      "url"      : "https://y2p.cc/notes/markdown/",
      "keywords" : "Markdown"
    } ,
  
    {
      "title"    : "Markmap",
      "category" : "Markmap",
      "content": " 目录  语法 效果 试试 语法 # markmap ## Links - &lt;https://markmap.js.org/&gt; - [GitHub](https://github.com/gera2ld/markmap) ## Related - [coc-markmap](https://github.com/gera2ld/coc-markmap) - [gatsby-remark-markmap](https://github.com/gera2ld/gatsby-remark-markmap) ## Features - links - **strong** ~~del~~ *italic* ==highlight== ----- - multiline text - `inline code` -  ```js  console.log('code block');  ``` - Katex - $x = {-b   pm   sqrt{b^2-4ac}   over 2a}$ - Now we can wrap very very very very long text based on `maxWidth` option 效果  Markmap  试试 markmap try it out ",
      "url"      : "https://y2p.cc/notes/markmap/",
      "keywords" : "Markmap"
    } ,
  
    {
      "title"    : "MPV",
      "category" : "Tools",
      "content": "Mac OS X 下最好用的播放器，没有之一。 快捷键 参考：mpv keybindings    按键  功能     RIGHT  前进 5 秒    LEFT  后退 5 秒    UP  前进 60 秒    DOWN  后退 60 秒    [  0.9091 倍速播放    ]  1.1 倍速播放    {  0.5 倍速播放    }  2.0 倍速播放    Backspace  还原到 1.0 倍速    Space 或 p  播放/暂停    .  下一帧    ,  上一帧    9 或 /  音量 -2    0 或 *  音量 +2    f  切换是否全屏    v  显示/隐藏字幕    T  切换是否前端显示    s  截屏，有字幕    S  截屏，无字幕    o  显示进度条与时间，2 秒后消失    I (大写 i)  显示当前文件名    1  对比度 -1    2  对比度 +1    3  亮度 -1    4  亮度 +1    5  Gamma 值 -1    6  Gamma 值 +1    7  饱和度 -1    8  饱和度 +1    l (小写 L)  设置/清除 A-B 循环点    j/J  选择字幕    #  切换声道    q  退出   命令行参数 参考：OPTIONS 参考  MPV使用小记 ",
      "url"      : "https://y2p.cc/notes/mpv/",
      "keywords" : "mpv"
    } ,
  
    {
      "title"    : "点点和滴滴",
      "category" : "note",
      "content": " 记录点点滴滴 ",
      "url"      : "https://y2p.cc/notes/note/",
      "keywords" : "note"
    } ,
  
    {
      "title"    : "Python",
      "category" : "Python",
      "content": "requests 优雅简单的 HTTP 模块。 BeautifulSoup 很好用的 HTML/XML 解析器。 json JSON 编码解码器。 应用举例：  格式化 JSON 文件 python -m json.tool src.json &gt; dst.json   在 Vim 里格式化 JSON： :%!python -m json.tool   CGIHTTPServer 简单实用的 HTTP 服务器。 应用举例：  运行一个简易的 HTTP 服务器 python -m CGIHTTPServer 80   base64 方便地进行 base64 编解码的模块。 应用举例：  解码 base64 echo aGVsbG93b3JsZA== | python -m base64 -d   则能看到输出 helloworld  ",
      "url"      : "https://y2p.cc/notes/python/",
      "keywords" : "Python"
    } ,
  
    {
      "title"    : "Qt Creator",
      "category" : "Qt",
      "content": "快捷键（for mac） 参考：http://doc.qt.io/qtcreator/creator-keyboard-shortcuts.html C –&gt; Ctrl S –&gt; Shift M –&gt; Alt Cmd –&gt; Command    功能  快捷键     自动完成  C-Space    显示/隐藏侧边栏  Cmd-0    切换已打开的文件  M-Tab    上/下一行  C-p/C-n    前进/后退一个字符  C-f/C-b    删除一个单词  M-Del    构建  Cmd-b    运行  Cmd-r    调试  Cmd-y    注释  Cmd-/    换行  Cmd-Return    跳到定义  F2    切换头文件与源文件  F4    前进/后退  M-Cmd-Left/Right    打开定位器  Cmd-k  ",
      "url"      : "https://y2p.cc/notes/qt-creator/",
      "keywords" : "Qt, Qt Creator"
    } ,
  
    {
      "title"    : "Swimming",
      "category" : "Hobbies",
      "content": "蛙泳 我目前能游出最远距离的泳姿。 视频  蛙泳教学完整版 动作时序图 要领  先划手，后收腿，先伸胳膊后蹬腿。 腿部动作时一定要勾脚，大腿不要收太多。 收腿放松，蹬腿用力，蹬完一定要漂一会，把握好节奏，不要快蹬快收。 手部划水动作不要过大，不要超过肩部。 两手开始外分时就抬头吸气。 自由泳 视频  值得一看的自由泳教学视频 ",
      "url"      : "https://y2p.cc/notes/swimming/",
      "keywords" : "游泳, 蛙泳"
    } ,
  
    {
      "title"    : "Note Template",
      "category" : "cate1",
      "content": "Content here ",
      "url"      : "https://y2p.cc/notes/template/",
      "keywords" : "keyword1, keyword2"
    } ,
  
    {
      "title"    : "Vim",
      "category" : "Vim",
      "content": "移动 以字（符）为单位    功能  按键     上  k    下  j    左  h    右  l   以单词为单位    功能  按键     前一个单词尾  ge    后一个单词首  w    本单词首（已在本词首则跳到前一单词首）  b    本单词尾（已在本词尾则跳到后一单词尾）  e   以屏幕为单位    功能  按键     向下翻页  CTRL-f    向上翻页  CTRL-b    向下翻半页  CTRL-d    向上翻半页  CTRL-u    向上一行  CTRL-y    向下一行  CTRL-e    光标移到屏幕上方  H    光标移到屏幕中间  M    光标移到屏幕下方  L    光标所在位置移到屏幕上方  zt    光标所在位置移到屏幕中间  zz    光标所在位置移到屏幕下方  zb   行号    功能  按键     跳到第 num 行  :num 或 numG 或 numgg   文件    功能  按键     跳到文件头  gg    跳到文件尾  G   编辑 复制    功能  按键     复制光标所在单词  yiw    复制光标所在行  yy   粘贴    功能  按键     在光标之后粘贴  p    在光标之前粘贴  P   剪切    功能  按键     剪切选中区域  d    剪切光标所在行  dd   替换    功能  按键     将全文中的 str1 替换为 str1  :%s/str1/str2/g    将 1 到 5 行中的 str1 替换为 str2  :1,5/str1/str2/g   大小写    功能  按键     将选中内容大小写互换  ~    将选中内容全转为小写  gu    将选中内容全转为大写  gU    将当前行变成小写  guu    将当前行变成大写  gUU   选择    功能  按键     选中上一次选择的区域  gv    选中括号内区域  vi{、vi[、vi(   搜索    功能  按键     向下查找字符串  /str    向上查找字符串  ?str    查找下一个  n    查找上一个  N    向下查找光标所在单词  *    向下查找光标所在单词  #   正则表达式    功能  按键     匹配单词左边界    &lt;    匹配单词右边界    &gt;    去重  :g/^  (.*  )$    1/d   常用    功能  按键     删除空行  :g/^$/d    撤销/UNDO  u    重做/REDO  C-r    统计行/单词/字符/字节数  g C-g   全局    功能  按键     退出  :q    强制执行  !    执行外部命令  :!   文件操作    功能  按键     打开  :e    打开文件对话框  :bro e    保存  :w    另存为对话框  :bro w    查看历史文件列表  :ol    查看并打开历史文件  :bro ol    重命名当前文件  :f filename   vimdiff    功能  按键     移动到上一个不同处  [c    移动到下一个不同处  ]c    该差异点使用当前文件的  dp    该差异点使用其它文件的  do    手动刷新重新比较  :diffupdate   Buffer    功能  按键     查看 Buffer 列表  :ls    转到 Buffer 列表中的下一个 Buffer  :bn    转到 Buffer 列表中的上一个 Buffer  :bp    转到 Buffer 列表中的 num 号 Buffer  :bnum    你之前待过的一个 Buffer  :b#    从 Buffer 列表中删除 num 号 Buffer  :bdnum   组合命令 可以使用 | 来组合命令，比如 cmd1 | cmd2。 代码    功能  按键     格式化代码  gg=G    去除 1-20 行首的行号  :1,20s/^    s  *[0-9]  *    s  *//g    展开全部折叠  zR    展开当前层级折叠  zr    全部折叠  zM    当前层级折叠  zm    切换折叠/展开  za    递归折叠/展开当前大区块  zA    折叠当前区块  zc    递归折叠当前大区块  zC    展开当前区块  zo    递归展开当前大区块  zO    格式化 json 数据  :%!python -m json.tool    缩进当前行  &gt;&gt;    反缩进当前行  &lt;&lt;   插件 CtrlP 基础按键 C-p    功能  按键     刷新列表  F5    切换文件/缓冲区/MRU  C-f/b    切换全路径搜索/文件名搜索  C-d    切换正则表达式模式  C-r    上/下一个选项  C-k/j    在新标签/垂直分割/水平分割打开文件  C-t/v/x    历史选择记录的上/下一条  C-p/n    创建文件和它的父路径  C-y    标记并打开多个文件  C-z C-o    退出 CtrlP  C-c   LeaderF    功能  按键     打开文件  Leader-f    打开缓冲区  Leader-b    打开 MRU  Leader-m（自定义的）    退出  C-c    切换模糊查找和正则查找  C-r    粘贴  C-v    清空输入  C-u    上/下一个选项  C-k/j    在新标签/垂直分割/水平分割打开文件  C-t/]/v    刷新列表  F5  ",
      "url"      : "https://y2p.cc/notes/vim/",
      "keywords" : "Vim"
    } 
  
]

