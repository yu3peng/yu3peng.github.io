---
layout: post
title: GPU
categories: GPU
description: GPU
keywords: GPU
---

# Awesome GPU

<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">
<title>Markmap</title>
<style>
* {
  margin: 0;
  padding: 0;
}
#mindmap {
  display: block;
  width: 100vw;
  height: 100vh;
}
</style>
</head>
<body>

<svg id="mindmap"></svg>
<script src="https://cdn.jsdelivr.net/npm/d3@6.7.0"></script><script src="https://cdn.jsdelivr.net/npm/markmap-view@0.13.2"></script><script>((w,x,k,M)=>{const _=w();window.mm=_.Markmap.create("svg#mindmap",x==null?void 0:x(_,M),k)})(()=>window.markmap,(e,t)=>e.deriveOptions(t),{"type":"heading","depth":1,"payload":{"lines":[0,2]},"content":"Awesome-GPU","children":[{"type":"heading","depth":2,"payload":{"lines":[3,4]},"content":"Architecture","children":[{"type":"heading","depth":3,"payload":{"lines":[5,6]},"content":"Resources Management","children":[{"type":"list_item","depth":5,"payload":{"lines":[7,8]},"content":"<strong>TECS'21</strong>-<a href=\"https://dl.acm.org/doi/10.1145/3429440\">Reducing Energy in GPGPUs through Approximate Trivial Bypassing</a>"},{"type":"list_item","depth":5,"payload":{"lines":[8,9]},"content":"<strong>ASPLOS'17</strong>-<a href=\"http://dl.acm.org/citation.cfm?id=3037709\">Locality-Aware CTA Clustering for Modern GPUs</a>"},{"type":"list_item","depth":5,"payload":{"lines":[9,10]},"content":"<strong>ASPLOS'17</strong>-<a href=\"http://dl.acm.org/citation.cfm?id=3037707\">Dynamic Resource Management for Efficient Utilization of Multitasking GPUs</a>"},{"type":"list_item","depth":5,"payload":{"lines":[10,11]},"content":"<strong>HPCA'17</strong>-<a href=\"http://ieeexplore.ieee.org/document/7920860/\">Dynamic GPGPU Power Management Using Adaptive Model Predictive Control</a>"},{"type":"list_item","depth":5,"payload":{"lines":[11,12]},"content":"<strong>ISCA'16</strong>-<a href=\"http://ieeexplore.ieee.org/document/7551394/\">Transparent Offloading and Mapping (TOM): Enabling Programmer-Transparent Near-Data Processing in GPU Systems</a>"}]},{"type":"heading","depth":3,"payload":{"lines":[13,14]},"content":"Parallelism","children":[{"type":"list_item","depth":5,"payload":{"lines":[15,16]},"content":"<strong>HPCA'18</strong>-<a href=\"https://ieeexplore.ieee.org/abstract/document/8327010\">Accelerate GPU Concurrent Kernel Execution by Mitigating Memory Pipeline Stalls</a>"},{"type":"list_item","depth":5,"payload":{"lines":[16,17]},"content":"<strong>HPCA'17</strong>-<a href=\"http://ieeexplore.ieee.org/document/7920863/\">Controlled Kernel Launch for Dynamic Parallelism in GPUs</a>"},{"type":"list_item","depth":5,"payload":{"lines":[17,18]},"content":"<strong>GTC'17</strong>-<a href=\"http://on-demand.gputechconf.com/gtc/2017/presentation/s7622-Kyrylo-perelygin-robust-and-scalable-cuda.pdf\">COOPERATIVE GROUPS</a>"},{"type":"list_item","depth":5,"payload":{"lines":[18,19]},"content":"<strong>ISCA'16</strong>-<a href=\"http://ieeexplore.ieee.org/document/7551424/\">LaPerm: Locality Aware Scheduler for Dynamic Parallelism on GPUs</a>"},{"type":"list_item","depth":5,"payload":{"lines":[19,20]},"content":"<strong>ISCA'16</strong>-<a href=\"http://ieeexplore.ieee.org/document/7551426/\">Virtual Thread Maximizing Thread-Level Parallelism beyond GPU Scheduling Limit</a>"},{"type":"list_item","depth":5,"payload":{"lines":[20,21]},"content":"<strong>Berkeley TechRpts'16</strong>-<a href=\"https://www2.eecs.berkeley.edu/Pubs/TechRpts/2016/EECS-2016-143.html\">Understanding Latency Hiding on GPUs</a>"}]},{"type":"heading","depth":3,"payload":{"lines":[22,23]},"content":"Cache","children":[{"type":"list_item","depth":5,"payload":{"lines":[24,25]},"content":"<strong>ISCA'16</strong>-<a href=\"http://ieeexplore.ieee.org/document/7551393/\">APRES: Improving Cache Efficiency by Exploiting Load Characteristics on GPUs</a>"},{"type":"list_item","depth":5,"payload":{"lines":[25,26]},"content":"<strong>SC'15</strong>-<a href=\"https://ieeexplore.ieee.org/document/7832791\">Adaptive and Transparent Cache Bypassing for GPUs</a>"}]},{"type":"heading","depth":3,"payload":{"lines":[27,28]},"content":"Memory","children":[{"type":"list_item","depth":5,"payload":{"lines":[29,30]},"content":"<strong>ICCAD'21</strong>-<a href=\"https://ieeexplore.ieee.org/document/9643535\">Improving Inter-kernel Data Reuse With CTA-Page Coordination in GPGPU</a>"},{"type":"list_item","depth":5,"payload":{"lines":[30,31]},"content":"<strong>SC'21</strong>-<a href=\"https://dl.acm.org/doi/10.1145/3458817.3480855\">In-Depth Analyses of Unified Virtual Memory System for GPU Accelerated Computing</a>"},{"type":"list_item","depth":5,"payload":{"lines":[31,32]},"content":"<strong>IBM'20</strong>-<a href=\"https://ieeexplore.ieee.org/document/8907404\">Umpire: Application-Focused Management and Coordination of Complex Hierarchical Memory</a>"},{"type":"list_item","depth":5,"payload":{"lines":[32,33]},"content":"<strong>HPCA'13</strong>-<a href=\"https://ieeexplore.ieee.org/document/6522332\">Reducing GPU Offload Latency via Fine-Grained CPU-GPU Synchronization</a>"}]},{"type":"heading","depth":3,"payload":{"lines":[34,35]},"content":"White Papers","children":[{"type":"list_item","depth":5,"payload":{"lines":[36,37]},"content":"<strong>NVIDIA Ampere</strong>-<a href=\"https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/nvidia-ampere-architecture-whitepaper.pdf\">NVIDIA A100 Tensor Core GPU Architecture</a>"},{"type":"list_item","depth":5,"payload":{"lines":[37,38]},"content":"<strong>NVIDIA Turing</strong>-<a href=\"https://www.nvidia.com/en-us/design-visualization/technologies/turing-architecture/\">NVIDIA TURING GPU ARCHITECTURE</a>"},{"type":"list_item","depth":5,"payload":{"lines":[38,39]},"content":"<strong>NVIDIA Volta</strong>-<a href=\"http://www.nvidia.com/object/volta-architecture-whitepaper.html\">NVIDIA TESLA V100</a>"},{"type":"list_item","depth":5,"payload":{"lines":[39,40]},"content":"<strong>NVIDIA Pascal</strong>-<a href=\"http://www.nvidia.com/object/gpu-architecture.html\">NVIDIA TESLA P100</a>"},{"type":"list_item","depth":5,"payload":{"lines":[40,41]},"content":"<strong>NVIDIA Kepler</strong>-<a href=\"https://www.nvidia.com/content/PDF/kepler/NVIDIA-Kepler-GK110-Architecture-Whitepaper.pdf\">NVIDIA’s Next Generation CUDA Compute Architecture: Kepler</a>"},{"type":"list_item","depth":5,"payload":{"lines":[41,42]},"content":"<strong>NVIDIA Fermi</strong>-<a href=\"https://www.nvidia.com/content/PDF/fermi_white_papers/NVIDIA_Fermi_Compute_Architecture_Whitepaper.pdf\">NVIDIA’s Next Generation CUDA Compute Architecture: Fermi</a>"},{"type":"list_item","depth":5,"payload":{"lines":[42,43]},"content":"<strong>AMD CDNA 2</strong>-<a href=\"https://www.amd.com/system/files/documents/amd-cdna2-white-paper.pdf\">INTRODUCING AMD CDNA 2 ARCHITECTURE</a>"},{"type":"list_item","depth":5,"payload":{"lines":[43,44]},"content":"<strong>AMD CDNA</strong>-<a href=\"https://www.amd.com/system/files/documents/amd-cdna-whitepaper.pdf\">INTRODUCING AMD CDNA ARCHITECTURE</a>"}]}]},{"type":"heading","depth":2,"payload":{"lines":[45,46]},"content":"Algorithms","children":[{"type":"heading","depth":3,"payload":{"lines":[47,48]},"content":"BLAS","children":[{"type":"list_item","depth":5,"payload":{"lines":[49,50]},"content":"<strong>IPDPS'20</strong>-<a href=\"https://ieeexplore.ieee.org/abstract/document/9139835\">Demystifying Tensor Cores to Optimize Half-Precision Matrix Multiply</a>"},{"type":"list_item","depth":5,"payload":{"lines":[50,51]},"content":"<strong>PPoPP'19</strong>-<a href=\"https://dl.acm.org/doi/10.1145/3293883.3295734\">A Coordinated Tiling and Batching Framework for Efficient GEMM on GPU</a>"},{"type":"list_item","depth":5,"payload":{"lines":[51,52]},"content":"<strong>GTC'18</strong>-<a href=\"http://on-demand.gputechconf.com/gtc/2018/presentation/s8854-cutlass-software-primitives-for-dense-linear-algebra-at-all-levels-and-scales-within-cuda.pdf\">CUTLASS: CUDA TEMPLATE LIBRARY FOR DENSE LINEAR ALGEBRA AT ALL LEVELS AND SCALES</a>"}]},{"type":"heading","depth":3,"payload":{"lines":[53,54]},"content":"Stencils","children":[{"type":"list_item","depth":5,"payload":{"lines":[55,56]},"content":"<strong>CGO'20</strong>-<a href=\"https://dl.acm.org/doi/10.1145/3368826.3377904\">AN5D: Automated Stencil Framework for High-Degree Temporal Blocking on GPUs</a>"},{"type":"list_item","depth":5,"payload":{"lines":[56,57]},"content":"<strong>IPDPS'20</strong>-<a href=\"https://ieeexplore.ieee.org/document/8820786\">On Optimizing Complex Stencils on GPUs</a>"},{"type":"list_item","depth":5,"payload":{"lines":[57,58]},"content":"<strong>PPoPP'18</strong>-<a href=\"https://dl.acm.org/doi/abs/10.1145/3178487.3178500\">Register Optimizations for Stencils on GPUs</a>"}]},{"type":"heading","depth":3,"payload":{"lines":[59,60]},"content":"Scans","children":[{"type":"list_item","depth":5,"payload":{"lines":[61,62]},"content":"<strong>NVResearch TechRpts'16</strong>-<a href=\"https://research.nvidia.com/publication/single-pass-parallel-prefix-scan-decoupled-look-back\">Single-pass Parallel Prefix Scan with Decoupled Look-back</a>"}]}]},{"type":"heading","depth":2,"payload":{"lines":[63,64]},"content":"Applications","children":[{"type":"heading","depth":3,"payload":{"lines":[65,66]},"content":"Deep Learning","children":[{"type":"list_item","depth":5,"payload":{"lines":[67,68]},"content":"<strong>PPoPP'21</strong>-<a href=\"https://dl.acm.org/doi/10.1145/3437801.3441585\">Understanding and bridging the gaps in current GNN performance optimizations</a>"},{"type":"list_item","depth":5,"payload":{"lines":[68,69]},"content":"<strong>SC'21</strong>-<a href=\"https://dl.acm.org/doi/abs/10.1145/3458817.3476138\">E.T.: re-thinking self-attention for transformer models on GPUs</a>"},{"type":"list_item","depth":5,"payload":{"lines":[69,70]},"content":"<strong>OSDI'21</strong>-<a href=\"https://www.usenix.org/system/files/osdi21-wang-yuke.pdf\">GNNAdvisor: An Adaptive and Efficient Runtime System for GNN Acceleration on GPUs</a>"},{"type":"list_item","depth":5,"payload":{"lines":[70,71]},"content":"<strong>SC'20</strong>-<a href=\"https://arxiv.org/abs/2006.10901\">Sparse GPU Kernels for Deep Learning</a>"},{"type":"list_item","depth":5,"payload":{"lines":[71,72]},"content":"<strong>PPoPP'18</strong>-<a href=\"https://arxiv.org/abs/1801.04380\">SuperNeurons: Dynamic GPU Memory Management for Training Deep Neural Networks</a>"},{"type":"list_item","depth":5,"payload":{"lines":[72,73]},"content":"<strong>HPCA'17</strong>-<a href=\"http://ieeexplore.ieee.org/document/7920809/\">Towards Pervasive and User Satisfactory CNN across GPU Microarchitectures</a>"}]}]},{"type":"heading","depth":2,"payload":{"lines":[74,75]},"content":"Tools","children":[{"type":"heading","depth":3,"payload":{"lines":[76,77]},"content":"Benchmarks","children":[{"type":"list_item","depth":5,"payload":{"lines":[78,79]},"content":"<strong>GTC'18</strong>-<a href=\"https://arxiv.org/pdf/1804.06826.pdf\">Dissecting the NVIDIA Volta GPU Architecture via Microbenchmarking</a>"},{"type":"list_item","depth":5,"payload":{"lines":[79,80]},"content":"<strong>ISPASS'10</strong>-<a href=\"http://ieeexplore.ieee.org/document/5452013/\">Demystifying GPU Microarchitecture through Microbenchmarking</a>"}]},{"type":"heading","depth":3,"payload":{"lines":[81,82]},"content":"Models","children":[{"type":"list_item","depth":5,"payload":{"lines":[83,84]},"content":"<strong>PMBS'19</strong>-<a href=\"https://ieeexplore.ieee.org/document/9059264\">Instruction Roofline An insightful visual performance model for GPUs</a>"},{"type":"list_item","depth":5,"payload":{"lines":[84,85]},"content":"<strong>ECP'19</strong>-<a href=\"https://crd.lbl.gov/assets/Uploads/ECP19-Roofline-1-intro.pdf\">Performance Tuning of Scientific Codes with the Roofline Model</a>"},{"type":"list_item","depth":5,"payload":{"lines":[85,86]},"content":"<strong>GTC'18</strong>-<a href=\"http://on-demand.gputechconf.com/gtc/2018/presentation/s81006-volta-architecture-and-performance-optimization.pdf\">VOLTA Architecture and performance optimization</a>"},{"type":"list_item","depth":5,"payload":{"lines":[86,87]},"content":"<strong>Synthesis Lectures on Computer Architecture'12</strong>-<a href=\"http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=6812836&amp;newsearch=true&amp;queryText=Performance%20Analysis%20and%20Tuning%20for%20General%20Purpose%20Graphics%20Processing%20Units%2038%20.LB.GPGPU.RB.\">Performance Analysis and Tuning for General Purpose Graphics Processing Units (GPGPU)</a>"},{"type":"list_item","depth":5,"payload":{"lines":[87,88]},"content":"<strong>SC'10</strong>-<a href=\"https://www.nvidia.com/content/PDF/sc_2010/CUDA_Tutorial/SC10_Fundamental_Optimizations.pdf\">Fundamental_Optimizations</a>"}]},{"type":"heading","depth":3,"payload":{"lines":[89,90]},"content":"Simulators","children":[{"type":"list_item","depth":5,"payload":{"lines":[91,92]},"content":"<strong>ISPASS'10</strong>-<a href=\"http://ieeexplore.ieee.org/document/5452029/\">Visualizing Complex Dynamics in Many-Core Accelerator Architectures</a>"},{"type":"list_item","depth":5,"payload":{"lines":[92,93]},"content":"<strong>ISPASS'09</strong>-<a href=\"http://ieeexplore.ieee.org/abstract/document/4919648/\">Analyzing CUDA Workloads Using a Detailed GPU Simulator</a>"}]},{"type":"heading","depth":3,"payload":{"lines":[94,95]},"content":"Profilers","children":[{"type":"list_item","depth":5,"payload":{"lines":[96,97]},"content":"<strong>PLDI'18</strong>-<a href=\"https://dl.acm.org/citation.cfm?id=3192397\">GPU Code Optimization using Abstract Kernel Emulation and Sensitivity Analysis</a>"},{"type":"list_item","depth":5,"payload":{"lines":[97,98]},"content":"<strong>CGO'18</strong>-<a href=\"https://dl.acm.org/citation.cfm?id=3168831\">CUDAAdvisor: LLVM-based runtime profiling for modern GPUs</a>"},{"type":"list_item","depth":5,"payload":{"lines":[98,99]},"content":"<strong>CCGRID'18</strong>-<a href=\"https://ieeexplore.ieee.org/document/8411034\">Exposing Hidden Performance Opportunities in High Performance GPU Applications </a>"},{"type":"list_item","depth":5,"payload":{"lines":[99,100]},"content":"<strong>THPC'16</strong>-<a href=\"https://link.springer.com/chapter/10.1007/978-3-319-56702-0_3\">Monitoring Heterogeneous Applications with the OpenMP Tools Interface</a>"},{"type":"list_item","depth":5,"payload":{"lines":[100,101]},"content":"<strong>Euro-Par'15</strong>-<a href=\"https://link.springer.com/chapter/10.1007/978-3-319-27308-2_16\">Identifying Optimization Opportunities Within Kernel Execution in GPU Codes</a>"},{"type":"list_item","depth":5,"payload":{"lines":[101,102]},"content":"<strong>SC'13</strong>-<a href=\"https://dl.acm.org/citation.cfm?id=2503299\">Effective sampling-driven performance tools for GPU-accelerated supercomputers</a>"},{"type":"list_item","depth":5,"payload":{"lines":[102,103]},"content":"<strong>ISPASS'12</strong>-<a href=\"https://ieeexplore.ieee.org/document/6189206\">Lynx: A dynamic instrumentation system for data-parallel applications on GPGPU architectures </a>"},{"type":"list_item","depth":5,"payload":{"lines":[103,104]},"content":"<strong>ICPP'11</strong>-<a href=\"https://dl.acm.org/citation.cfm?id=2066951\">Parallel Performance Measurement of Heterogeneous Parallel Systems with GPUs</a>"},{"type":"list_item","depth":5,"payload":{"lines":[104,105]},"content":"<a href=\"http://www.vi-hps.org/projects/score-p/\"><strong>Vampir|Score-P</strong></a>"},{"type":"list_item","depth":5,"payload":{"lines":[105,106]},"content":"<a href=\"https://www.cs.uoregon.edu/research/tau/home.php\"><strong>TAU</strong></a>"},{"type":"list_item","depth":5,"payload":{"lines":[106,107]},"content":"<a href=\"http://icl.utk.edu/papi/\"><strong>PAPI</strong></a>"},{"type":"list_item","depth":5,"payload":{"lines":[107,108]},"content":"<a href=\"https://www.allinea.com/products/map/\"><strong>Allinea MAP</strong></a>"},{"type":"list_item","depth":5,"payload":{"lines":[108,109]},"content":"<a href=\"https://openspeedshop.org/\"><strong>Open|SpeedShop</strong></a>"},{"type":"list_item","depth":5,"payload":{"lines":[109,110]},"content":"<a href=\"http://hpctoolkit.org/\"><strong>HPCToolkit</strong></a>"},{"type":"list_item","depth":5,"payload":{"lines":[110,111]},"content":"<a href=\"https://developer.nvidia.com/nsight-systems\"><strong>NVIDIA Nsight Systems</strong></a>"},{"type":"list_item","depth":5,"payload":{"lines":[111,112]},"content":"<a href=\"https://developer.nvidia.com/nsight-compute\"><strong>NVIDIA Nsight Compute</strong></a>"},{"type":"list_item","depth":5,"payload":{"lines":[112,113]},"content":"<a href=\"https://github.com/NVlabs/SASSI/blob/master/doc/SASSI-Tutorial-Micro2015.pptx\"><strong>SASSI</strong></a>"},{"type":"list_item","depth":5,"payload":{"lines":[113,114]},"content":"<a href=\"https://github.com/NVlabs/NVBit/releases\"><strong>NVBit</strong></a>"}]}]},{"type":"heading","depth":2,"payload":{"lines":[115,116]},"content":"Runtime","children":[{"type":"heading","depth":3,"payload":{"lines":[117,118]},"content":"Scheduling","children":[{"type":"list_item","depth":5,"payload":{"lines":[119,120]},"content":"<strong>PPoPP'22</strong>-<a href=\"https://arxiv.org/abs/2107.08538\">CASE: A Compiler-Assisted SchEduling Framework for Multi-GPU Systems</a>"},{"type":"list_item","depth":5,"payload":{"lines":[120,121]},"content":"<strong>TPDS'20</strong>-<a href=\"https://www.computer.org/csdl/journal/td/2020/04/08853389/1dKnnndWFwY\">cCUDA: Effective Co-Scheduling of Concurrent Kernels on GPUs</a>"}]}]},{"type":"heading","depth":2,"payload":{"lines":[122,123]},"content":"Code Generation","children":[{"type":"heading","depth":3,"payload":{"lines":[124,125]},"content":"Compilers","children":[{"type":"list_item","depth":5,"payload":{"lines":[126,127]},"content":"<strong>AMD'21</strong>-<a href=\"https://arxiv.org/abs/2111.12055\">Generating GPU Compiler Heuristics using Reinforcement Learning</a>"},{"type":"list_item","depth":5,"payload":{"lines":[127,128]},"content":"<strong>TACO'21</strong>-<a href=\"https://dl.acm.org/doi/10.1145/3469030\">Domain-Specific Multi-Level IR Rewriting for GPU: The Open Earth Compiler for GPU-accelerated Climate Simulation</a>"},{"type":"list_item","depth":5,"payload":{"lines":[128,129]},"content":"<strong>LLVM'17</strong>-<a href=\"https://dl.acm.org/citation.cfm?id=3148189\">Implementing implicit OpenMP data sharing on GPUs</a>"},{"type":"list_item","depth":5,"payload":{"lines":[129,130]},"content":"<strong>CGO'16</strong>-<a href=\"http://dl.acm.org/citation.cfm?id=2854041\">gpucc: An Open-Source GPGPU Compiler</a>"},{"type":"list_item","depth":5,"payload":{"lines":[130,131]},"content":"<strong>LLVM'16</strong>-<a href=\"https://dl.acm.org/citation.cfm?id=3018870\">Offloading Support for OpenMP in Clang and LLVM</a>"},{"type":"list_item","depth":5,"payload":{"lines":[131,132]},"content":"<strong>PMBS'15</strong>-<a href=\"https://dl.acm.org/citation.cfm?id=2832089\">Performance Analysis of OpenMP on a GPU using a CORAL Proxy Application</a>"},{"type":"list_item","depth":5,"payload":{"lines":[132,133]},"content":"<strong>LLVM'15</strong>-<a href=\"https://dl.acm.org/citation.cfm?id=2833161\">Integrating GPU Support for OpenMP Ofﬂoading Directives into Clang</a>"},{"type":"list_item","depth":5,"payload":{"lines":[133,134]},"content":"<strong>LLVM'14</strong>-<a href=\"https://dl.acm.org/citation.cfm?id=2688364\">Coordinating GPU Threads for OpenMP 4.0 in LLVM</a>"}]},{"type":"heading","depth":3,"payload":{"lines":[135,136]},"content":"Programming Models","children":[{"type":"list_item","depth":5,"payload":{"lines":[137,138]},"content":"<strong>CGO'21</strong>-<a href=\"https://dl.acm.org/doi/abs/10.1109/CGO51591.2021.9370324\">C-for-metal: high performance SIMD programming on intel GPUs</a>"},{"type":"list_item","depth":5,"payload":{"lines":[138,139]},"content":"<strong>ECRTS'19</strong>-<a href=\"https://drops.dagstuhl.de/opus/volltexte/2019/10759/\">Novel Methodologies for Predictable CPU-To-GPU Command Offloading</a>"},{"type":"list_item","depth":5,"payload":{"lines":[139,140]},"content":"<strong>ASPLOS'14</strong>-<a href=\"https://dl.acm.org/citation.cfm?id=2541948\">Paraprox: Pattern-Based Approximation for Data Parallel Applications</a>"}]},{"type":"heading","depth":3,"payload":{"lines":[141,142]},"content":"Profile Guided Optimization","children":[{"type":"list_item","depth":5,"payload":{"lines":[143,144]},"content":"<strong>Geometry and Optimization'21</strong>-<a href=\"https://doi.org/10.1111/cgf.14382\">Cooperative Profile Guided Optimizations</a>"},{"type":"list_item","depth":5,"payload":{"lines":[144,145]},"content":"<strong>IPDPS'13</strong>-<a href=\"https://ieeexplore.ieee.org/document/6569883\">Kernel Specialization for Improved Adaptability and Performance on Graphics Processing Units (GPUs)</a>"}]},{"type":"heading","depth":3,"payload":{"lines":[146,147]},"content":"Binaries","children":[{"type":"list_item","depth":5,"payload":{"lines":[148,149]},"content":"<strong>CGO'19</strong>-<a href=\"https://dl.acm.org/citation.cfm?id=3314900\">Decoding CUDA binary</a>"},{"type":"list_item","depth":5,"payload":{"lines":[149,150]},"content":"<strong>ISCA'15</strong>-<a href=\"http://ieeexplore.ieee.org/document/7284065/\">Flexible software profiling of GPU architectures</a>"}]}]}]},null)</script>
</body>
</html>

## 硬件厂家共享方案

### NVIDIA GPU 共享方案

[GPU 的算力](https://developer.nvidia.com/cuda-gpus)很强，GPU 硬件很贵，为了节省固定资产的投入，需要将多个推理服务部署在同一张 GPU 卡上，在保证服务质量的前提下通过 GPU 共享来提升 GPU 的利用率。目前英伟达官方的 GPU 共享技术主要有两种方案：

* MIG
* vGPU
* MPS

#### NVIDIA MIG 方案

[VIDIA MIG 方案](https://www.nvidia.cn/technologies/multi-instance-gpu/)，多实例 GPU (MIG) 扩展了每个 NVIDIA H100、A100 及 A30 Tensor Core GPU 的性能和价值。MIG 可将 GPU 划分为多达七个实例，每个实例均完全独立于各自的高带宽显存、缓存和计算核心。如此一来，管理员便能支持所有大小的工作负载，且服务质量 (QoS) 稳定可靠，让每位用户都能享用加速计算资源。

![mig](/images/posts/mig.png)

MIG，即是一种 Hardware Partition。硬件资源隔离、故障隔离都是硬件实现的 —— 这是无可争议的隔离性最好的方案。它的问题是不灵活: **只有高端 GPU 支持；只支持 CUDA 计算；只支持 7 个 MIG 实例**。

#### NVIDIA vGPU 方案

[NVIDIA vGPU 方案](https://www.nvidia.cn/data-center/virtual-pc-apps/)采用虚拟化的技术，基于 SR-IOV 进行 GPU 设备虚拟化管理，在驱动层提供了时间分片执行的逻辑，并做了一定的显存隔离，这样在对显卡进行初始化设置的时候就可以根据需求将显卡进行划分。其中时间分片调度的逻辑可以是按实例均分，或者是自定义比例，显卡的显存需要按照预设的比例进行划分。Nvdia的vGPU方案在实施中有下面两点限制：

* vGPU划分完成之后，如果要改变这种预定义的划分，需要重启显卡才能生效，无法做到不重启更改配置。
* 其方案基于虚机，需要先对 GPU 物理机进行虚拟化之后，再在虚拟机内部署容器，无法直接基于物理机进行容器化的调度，另外 vGPU 方案需要收取 license 费用，增加了使用成本。

#### NVIDIA MPS 方案

[NVIDIA MPS 方案](https://docs.nvidia.com/deploy/pdf/CUDA_Multi_Process_Service_Overview.pdf)是一种算力分割的软件虚拟化方案。它通过将多个进程的 CUDA Context，合并到一个 CUDA Context 中，省去了 Context Switch 的开销，也在 Context 内部实现了算力隔离。如前所述，MPS 的致命缺陷，是 **把许多进程的 CUDA Context 合并成一个，从而导致了额外的故障传播**。所以尽管它的算力隔离效果极好，但长期以来工业界使用不多，多租户场景尤其如此。

### 寒武纪 GPU 共享方案

#### 寒武纪 SR-IOV 方案

SR-IOV 是 PCI-SIG 在 2007 年推出的规范，目的就是 PCIe 设备的虚拟化。SR-IOV 的本质是什么？考虑我们说过的 2 种资源和 2 种能力，来看看一个 VF 有什么:

* 配置空间是虚拟的（特权资源）
* MMIO 是物理的
* 中断和 DMA，因为 VF 有自己的 PCIe 协议层的标识（Routing ID，就是 BDF），从而拥有独立的地址空间。

那么，什么设备适合实现 SR-IOV？其实无非是要满足两点:

* 硬件资源要容易 partition
* 无状态（至少要接近无状态）

常见 PCIe 设备中，最适合 SR-IOV 的就是网卡了: 一或多对 TX/RX queue + 一或多个中断，结合上一个 Routing ID，就可以抽象为一个 VF。而且它是近乎无状态的。

试考虑 NVMe 设备，它的资源也很容易 partition，但是它有存储数据，因此在实现 SR-IOV 方面，就会有更多的顾虑。

回到 GPU 虚拟化：为什么 2007 年就出现 SR-IOV 规范、直到 2015 业界才出现第一个「表面上的」SRIOV-capable GPU ？这是因为，虽然 GPU 基本也是无状态的，但是它的硬件复杂度极高，远远超出 NIC、NVMe 这些，导致硬件资源的 partition 很难实现。

寒武纪虚拟化技术——vMLU，该虚拟化技术允许多个操作系统和应用程序共存于一个物理计算平台上，共享同一个芯片的计算资源。它为用户提供良好的安全性和隔离性，还支持如热迁移等高灵活特性。vMLU 帮助提高云计算密度，也使数据中心的 IT 资产管理更灵活。

除了虚拟化基本的资源共享特性，思元 270 首推的 SR-IOV 虚拟化技术，支持运行在云服务器上的多个实例直接共享智能芯片的硬件资源。传统虚拟化系统中大量的资源和时间损耗在 Hypervisor 或 VMM 软件层面，PCIe 设备的性能优势无法彻底发挥。而 SR-IOV 的价值在于消除这一软件瓶颈，助力多个虚拟机实现高效物理资源共享。

![sriov](/images//posts/sriov.jpeg)

与传统图形加速卡的 vGPU 所采用的虚拟化技术不同，思元 270 采用「非基于时间片的共享」方式，因为其没有因时间片切换上下文带来的性能损失，能充分保证各 VF 独立的服务质量，彼此完全独立运行互不影响。

另外，SR-IOV 还可以避免因分时复用切换应用带来的性能开销。vMLU 搭配 Docker 或 VM 运行时，单个 VF 业务性能保持在硬件性能的 91% 以上。这使得用户在多模型并行时，对各 VF 可以做出更准确的服务质量 (QoS) 预期，而不必考虑多模型时的拥塞或切换带来的性能开销。

基于 SR-IOV 的 vMLU：更好的租户隔离性

虚拟化技术被数据中心广泛采用，除了因为其提供了对资源共享的能力（提供了更好的密度性能），也因为相对于其它技术 (如 docker), 虚拟化提供了更好的隔离性和安全性。寒武纪 vMLU 基于 SR-IOV 的虚拟化技术可以帮助云用户实现更好的隔离特性，具体优势如下：

* 资源独立，互不干扰，能确保服务质量（QoS）；
* 多任务时，没有无队列阻塞的烦恼；
* 其具备独立内存资源，各 VF 之间互不可见；
* 它的部署相对简单，不需要对开源软件成分进行修改。

## 软件厂家共享方案

### 方案评估点

* 不会使用超过其被分配的算力大小
* 隔离本身不应该对于 GPU 算力有过多损耗
* 多个进程同时共享的时候，与其单独运行时相比，不应有太大的性能偏差，即共享可以有效避免进程之间的干扰。

### 截获 CUDA API 实现显存及算力隔离

#### 显存隔离

对于深度学习应用来说，对于显存的需求来自于三个方面。

* 第一是模型的 CUDA kernel context，可类比于 CPU 程序中的 text 段，提供给 CUDA kernel 执行的环境，这是一项刚需，没有充足的显存，kernel 将无法启动，且 context 的大小随着 kernel 的复杂程度有增长，但在整体模型显存需求中是最小的一部分。

* 第二部分来自于模型训练得出的一些参数，如卷积中的 weight 和 bias。

* 第三部分来自于模型在推理过程中的临时存储，用于储存中间的计算结果。

对于一般的模型来说，基本都不需要占用整个GPU的显存。但是这里有一个例外，Tensorflow 框架默认分配所有 GPU 的显存来进行自己的显存管理。当然 Tensorflow 框架有相应的选项可以屏蔽该行为，但是对于平台来说，要让每个用户修改 TF 的配置为屏蔽该行为，就不太可行。

为应对这一问题，一个巧妙的方法可以在不需要应用开发者参与的情况下，让 Tensorflow 的部署应用只分配它所需的显存大小而不出现问题。该方法即 API 动态拦截。Tensorflow 之所以可以知道当前 GPU 的剩余显存，是通过 cuDeviceTotalMem/cuMemGetInfo 这两个 CUDA library API。通过 LD_PRELOAD 的方式，在钩子 so 中实现这两个 API，那么 Tensorflow 执行的时候，link 首先会调用的是的 API 实现，而不是 CUDA 的，这样就可以动态的修改这两个 API 的返回结果，如这里想做的，将特定 Tensorflow 应用的显存配额限制在其申请数值。

在系统实现的过程中，还对 cuMemAlloc/cuMemFree 做了同样的拦截，目的是为了能够对同容器中的多个 GPU 进程进程统一管理。当多个 GPU 进程分配显存之和超过其配额时，可以通过 cuMalloc 来返回显存不足的错误。容器内显存配额管理是通过 share mem 来做的。

相关实现方式可以参考：[vcuMemGetInfo](https://github.com/yu3peng/vcuMemGetInfo)

#### 算力隔离

GPU程序的执行，是通过kernel的片段来具体实施，在CPU侧launch了 kernel之后，具体的kernel及其调用参数随即交由GPU的硬件调度器来在某个未来的时间点真正运行起来。在默认的情况下，kernel是被派发给GPU上所有的SM，且执行过程中不能被中断。

![RC](/images/posts/R-C.png)

CUDA中用来区分thread，来判断代码应该处理数据的偏移量的方法，是通过CUDA中的blockIdx/threadIdx这两个内嵌变量。这两个变量在机器码上是只读的，在thread由硬件调度器派发的时候所指定。通过硬件调度器，就完成了抽象的blockIdx/threadIdx和具体的SM/SP的绑定。

为了能够精确的控制算力，我们就不能再依赖硬件调度器来控制内核启动。在这里用了一个取巧的方法，就是让内核启动之后被“困”在固定数目的SM上面，这个数目的值和GPU整体SM个数的比例就是给这个内核算力配比。

为了形象化来阐述思路，这里我们对GPU做了一个抽象化的改动，SM的个数被定义为10个。然后有一个启动参数为<<<15,1>>>的内核，即CUDA block size为15，thread size为1。它正常启动的时候，硬件调度器会给每一个SM上分配一个内核的副本。这样在第一时间就消耗了10个block的副本，随后每个SM上内核执行完毕之后会退出，硬件调度器会进一步分配剩下的5个block副本，在这个也执行完毕之后就完成了整个内核的执行。

算力切分之后，我们会在内核启动时，动态的修改其启动参数，将其CUDA block size从15变为5。这样硬件调度器就会将内核副本分配到GPU上一半数目的SM上，空闲的一半可以为其他内核所使用。

我们虽然通过动态修改启动参数的方法，避免了内核占满全部SM资源，但此时还没完成“困”这一动作。所以此时的内核行为是其完成预定逻辑之后，会退出，导致此时内核不能覆盖block size为15时的数据空间。为了将其“困“住，我们在内核的汇编EXIT处，替换成了BRANCH操作。这样内核完成本身的逻辑之后，会跳转到我们预设的一段逻辑中。这个逻辑完成虚拟blockIdx/threadIdx的自增操作，随后再跳转到内核开始位置，来基于更新的blockIdx/threadIdx来进行新一轮计算。

这次需要指出的是blockIdx/threadIdx为只读寄存器，所以没办法直接更改它的值。作为一个替代的解决方案时，将内核中的blockIdx/threadIdx进行整体替换为可写的寄存器，这样我们就可以在预设的跳转逻辑中做更改操作。

## 参考资料

1. [怎样节省 2/3 的 GPU？爱奇艺 vGPU 的探索与实践](https://baijiahao.baidu.com/s?id=1701253997258904666&wfr=spider&for=pc)

2. [寒武纪vMLU技术面世，首推SR-IOV虚拟化功能](https://baijiahao.baidu.com/s?id=1666091700362192814&wfr=spider&for=pc)

3. [GPU虚拟化，算力隔离，和qGPU](https://cloud.tencent.com/developer/article/1831090)